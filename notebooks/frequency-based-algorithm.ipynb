{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d5053bb3",
   "metadata": {},
   "source": [
    "# TEXT SUMMERIZATION - FREQUENCY BASED ALGORITHM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca4d17b4",
   "metadata": {},
   "source": [
    "**1. Preprocessing the texts**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f20dbc3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re # regular expressions\n",
    "import nltk # natural language toolkit\n",
    "import string # for string operations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bb6193a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "original_text = \"\"\"Artificial intelligence is human like intelligence.\n",
    "                   it is the study of intelligent artificial agents. \n",
    "                   Science and engieering to produce intelligent machines. \n",
    "                   Solve problems and have intelligence. \n",
    "                   Related to intelligent behavior. \n",
    "                   Developing of reasoning machines. \n",
    "                   Learn from mistakes and sucesses. \n",
    "                   Artifical intelligence is related to reasoning in everyday situations.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1b25e2dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Artificial intelligence is human like intelligence. it is the study of intelligent artificial agents. Science and engieering to produce intelligent machines. Solve problems and have intelligence. Related to intelligent behavior. Developing of reasoning machines. Learn from mistakes and sucesses. Artifical intelligence is related to reasoning in everyday situations.'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "original_text = re.sub(r'\\s+', ' ', original_text)  # remove extra spaces and newlines\n",
    "original_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3133c47a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/mohammadazimi/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /Users/mohammadazimi/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')  # download the punkt tokenizer models\n",
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6cda95a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/mohammadazimi/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download(\"stopwords\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "59782214",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a', 'about', 'above', 'after', 'again', 'against', 'ain', 'all', 'am', 'an', 'and', 'any', 'are', 'aren', \"aren't\", 'as', 'at', 'be', 'because', 'been', 'before', 'being', 'below', 'between', 'both', 'but', 'by', 'can', 'couldn', \"couldn't\", 'd', 'did', 'didn', \"didn't\", 'do', 'does', 'doesn', \"doesn't\", 'doing', 'don', \"don't\", 'down', 'during', 'each', 'few', 'for', 'from', 'further', 'had', 'hadn', \"hadn't\", 'has', 'hasn', \"hasn't\", 'have', 'haven', \"haven't\", 'having', 'he', \"he'd\", \"he'll\", 'her', 'here', 'hers', 'herself', \"he's\", 'him', 'himself', 'his', 'how', 'i', \"i'd\", 'if', \"i'll\", \"i'm\", 'in', 'into', 'is', 'isn', \"isn't\", 'it', \"it'd\", \"it'll\", \"it's\", 'its', 'itself', \"i've\", 'just', 'll', 'm', 'ma', 'me', 'mightn', \"mightn't\", 'more', 'most', 'mustn', \"mustn't\", 'my', 'myself', 'needn', \"needn't\", 'no', 'nor', 'not', 'now', 'o', 'of', 'off', 'on', 'once', 'only', 'or', 'other', 'our', 'ours', 'ourselves', 'out', 'over', 'own', 're', 's', 'same', 'shan', \"shan't\", 'she', \"she'd\", \"she'll\", \"she's\", 'should', 'shouldn', \"shouldn't\", \"should've\", 'so', 'some', 'such', 't', 'than', 'that', \"that'll\", 'the', 'their', 'theirs', 'them', 'themselves', 'then', 'there', 'these', 'they', \"they'd\", \"they'll\", \"they're\", \"they've\", 'this', 'those', 'through', 'to', 'too', 'under', 'until', 'up', 've', 'very', 'was', 'wasn', \"wasn't\", 'we', \"we'd\", \"we'll\", \"we're\", 'were', 'weren', \"weren't\", \"we've\", 'what', 'when', 'where', 'which', 'while', 'who', 'whom', 'why', 'will', 'with', 'won', \"won't\", 'wouldn', \"wouldn't\", 'y', 'you', \"you'd\", \"you'll\", 'your', \"you're\", 'yours', 'yourself', 'yourselves', \"you've\"]\n",
      "!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\n"
     ]
    }
   ],
   "source": [
    "stopwords = nltk.corpus.stopwords.words('english')  # get the list of stopwords in English\n",
    "print(stopwords)\n",
    "len(stopwords)  # number of stopwords\n",
    "print(string.punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "6fd3bfd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(text): \n",
    "    formatted_text = text.lower()\n",
    "    tokens = []\n",
    "    # tokenize the text using word tokenizer \n",
    "    for token in nltk.word_tokenize(formatted_text, language=\"english\", preserve_line=False): \n",
    "        tokens.append(token)\n",
    "    #print(tokens)\n",
    "    tokens = [word for word in tokens if word not in stopwords and word not in string.punctuation] # remove stopwords and punctuation from the text \n",
    "    formatted_text = \" \".join(element for element in tokens)  # join the tokens back to string\n",
    "    \n",
    "    return formatted_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a776ee92",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'artificial intelligence human like intelligence study intelligent artificial agents science engieering produce intelligent machines solve problems intelligence related intelligent behavior developing reasoning machines learn mistakes sucesses artifical intelligence related reasoning everyday situations'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "formatted_text = preprocess(original_text)\n",
    "formatted_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dd4f5fa",
   "metadata": {},
   "source": [
    "**2. Word frequency**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97f83eff",
   "metadata": {},
   "source": [
    "**6. Weighted word frequency**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce5b3ad8",
   "metadata": {},
   "source": [
    "**4. Sentence tokenization**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "957bd49a",
   "metadata": {},
   "source": [
    "**5. Score for the senteces**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "871b4b36",
   "metadata": {},
   "source": [
    "**6. Order the sentences**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ba7ac6a",
   "metadata": {},
   "source": [
    "**7. Generate the summary**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cbcc2de",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
