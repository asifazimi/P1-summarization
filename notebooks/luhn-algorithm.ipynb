{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "213d22ed",
   "metadata": {},
   "source": [
    "# TEXT SUMMERIZATION - LUHN ALGORITHM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1492c5e4",
   "metadata": {},
   "source": [
    "**1. Preprocessing the texts**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef1e20e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install matplotlib\n",
    "# pip install wordcloud\n",
    "# pip install spacy\n",
    "# !python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "583f8931",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re # regular expressions\n",
    "import nltk # natural language toolkit\n",
    "import string # for string operations\n",
    "import heapq # for finding n largest elements\n",
    "from IPython.core.display import HTML # for displaying HTML in Jupyter Notebook\n",
    "from goose3 import Goose # for extracting text from web pages\n",
    "from rouge_score import rouge_scorer\n",
    "import feedparser\n",
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "import json\n",
    "from wordcloud import WordCloud # for generating word clouds\n",
    "import matplotlib.pyplot as plt # for plotting graphs\n",
    "import spacy # for advanced NLP tasks\n",
    "from spacy import displacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ec369d89",
   "metadata": {},
   "outputs": [],
   "source": [
    "original_text = \"\"\"Artificial intelligence is human like intelligence machines.\n",
    "                   It is the study of intelligent artificial agents.\n",
    "                   Science and engineering to produce intelligent machines.\n",
    "                   Solve problems and have intelligence.\n",
    "                   Related to intelligent behavior machines.\n",
    "                   Developing of reasoning machines.\n",
    "                   Learn from mistakes and successes.\n",
    "                   Artificial intelligence is related to reasoning in everyday situations.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4602db19",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Artificial intelligence is human like intelligence machines. It is the study of intelligent artificial agents. Science and engineering to produce intelligent machines. Solve problems and have intelligence. Related to intelligent behavior machines. Developing of reasoning machines. Learn from mistakes and successes. Artificial intelligence is related to reasoning in everyday situations.'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "original_text = re.sub(r'\\s+', ' ', original_text)  # remove extra spaces and newlines\n",
    "original_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0403d482",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a', 'about', 'above', 'after', 'again', 'against', 'ain', 'all', 'am', 'an', 'and', 'any', 'are', 'aren', \"aren't\", 'as', 'at', 'be', 'because', 'been', 'before', 'being', 'below', 'between', 'both', 'but', 'by', 'can', 'couldn', \"couldn't\", 'd', 'did', 'didn', \"didn't\", 'do', 'does', 'doesn', \"doesn't\", 'doing', 'don', \"don't\", 'down', 'during', 'each', 'few', 'for', 'from', 'further', 'had', 'hadn', \"hadn't\", 'has', 'hasn', \"hasn't\", 'have', 'haven', \"haven't\", 'having', 'he', \"he'd\", \"he'll\", 'her', 'here', 'hers', 'herself', \"he's\", 'him', 'himself', 'his', 'how', 'i', \"i'd\", 'if', \"i'll\", \"i'm\", 'in', 'into', 'is', 'isn', \"isn't\", 'it', \"it'd\", \"it'll\", \"it's\", 'its', 'itself', \"i've\", 'just', 'll', 'm', 'ma', 'me', 'mightn', \"mightn't\", 'more', 'most', 'mustn', \"mustn't\", 'my', 'myself', 'needn', \"needn't\", 'no', 'nor', 'not', 'now', 'o', 'of', 'off', 'on', 'once', 'only', 'or', 'other', 'our', 'ours', 'ourselves', 'out', 'over', 'own', 're', 's', 'same', 'shan', \"shan't\", 'she', \"she'd\", \"she'll\", \"she's\", 'should', 'shouldn', \"shouldn't\", \"should've\", 'so', 'some', 'such', 't', 'than', 'that', \"that'll\", 'the', 'their', 'theirs', 'them', 'themselves', 'then', 'there', 'these', 'they', \"they'd\", \"they'll\", \"they're\", \"they've\", 'this', 'those', 'through', 'to', 'too', 'under', 'until', 'up', 've', 'very', 'was', 'wasn', \"wasn't\", 'we', \"we'd\", \"we'll\", \"we're\", 'were', 'weren', \"weren't\", \"we've\", 'what', 'when', 'where', 'which', 'while', 'who', 'whom', 'why', 'will', 'with', 'won', \"won't\", 'wouldn', \"wouldn't\", 'y', 'you', \"you'd\", \"you'll\", 'your', \"you're\", 'yours', 'yourself', 'yourselves', \"you've\"]\n",
      "!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\n"
     ]
    }
   ],
   "source": [
    "stopwords = nltk.corpus.stopwords.words('english')  # get the list of stopwords in English\n",
    "print(stopwords)\n",
    "len(stopwords)  # number of stopwords\n",
    "print(string.punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "cd22ffe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(text): \n",
    "    formatted_text = text.lower()\n",
    "    tokens = []\n",
    "    # tokenize the text using word tokenizer \n",
    "    for token in nltk.word_tokenize(formatted_text, language=\"english\", preserve_line=False): \n",
    "        tokens.append(token)\n",
    "    #print(tokens)\n",
    "    tokens = [word for word in tokens if word not in stopwords and word not in string.punctuation] # remove stopwords and punctuation from the text \n",
    "    formatted_text = \" \".join(element for element in tokens)  # join the tokens back to string\n",
    "    \n",
    "    return formatted_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e619a55d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'artificial intelligence human like intelligence machines study intelligent artificial agents science engineering produce intelligent machines solve problems intelligence related intelligent behavior machines developing reasoning machines learn mistakes successes artificial intelligence related reasoning everyday situations'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "formatted_text = preprocess(original_text)\n",
    "formatted_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcc680ad",
   "metadata": {},
   "source": [
    "**2. Function to calculate sentences score**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c067a6df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_sentences_score(sentences, important_words, distance): \n",
    "    scores = []\n",
    "    sentence_index = 0\n",
    "    \n",
    "    for sentence in [nltk.word_tokenize(sentence) for sentence in sentences]: # tokenize each sentence\n",
    "        # print(\"---------\")\n",
    "        # print(\"Words in each Sentence: \", sentence)\n",
    "        \n",
    "        word_index = []\n",
    "        for word in important_words: \n",
    "            # print(\"Important word: \", word)\n",
    "            try: \n",
    "                word_index.append(sentence.index(word))\n",
    "            except ValueError:\n",
    "                pass \n",
    "        word_index.sort()\n",
    "        # print(\"Word Indexes: \", word_index) # indexes of important words in the sentence\n",
    "        \n",
    "        if len(word_index) == 0:\n",
    "            continue\n",
    "        \n",
    "        # [0, 1, 2, 5, 8, 9]\n",
    "        group_list = [] # list of groups of important words\n",
    "        group = [word_index[0]] # current group of important words\n",
    "        \n",
    "        i = 1 # 3\n",
    "        while i < len(word_index): # 3\n",
    "            # first execution: 1 - 0\n",
    "            # second execution: 2 - 1\n",
    "            # third execution: 5 - 2\n",
    "            if word_index[i] - word_index[i-1] < distance: \n",
    "                group.append(word_index[i])\n",
    "                # print(\"Group after append: \", group)\n",
    "            else: \n",
    "                group_list.append(group[:])\n",
    "                group = [word_index[i]]\n",
    "                # print(\"New Group: \", group)\n",
    "            i += 1\n",
    "            \n",
    "        group_list.append(group) # append the last group\n",
    "        # print(\"Group List: \", group_list)\n",
    "            \n",
    "        # calculate the score for each group\n",
    "        max_group_score = 0\n",
    "        for g in group_list: \n",
    "            # print(g)\n",
    "            important_words_in_group = len(g)\n",
    "            total_words_in_group = g[-1] - g[0] + 1 # total words in the group  \n",
    "            score = 1.0 * (important_words_in_group ** 2) / total_words_in_group # multiplied by one in order to have the value in the same scale \n",
    "            # print(\"Score: \", score)\n",
    "            \n",
    "            if score > max_group_score:\n",
    "                max_group_score = score\n",
    "            \n",
    "        scores.append((max_group_score, sentence_index))\n",
    "        sentence_index += 1\n",
    "        \n",
    "    return scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73c10e46",
   "metadata": {},
   "source": [
    "**6. Function to summarize the texts**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "7b002ccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize(text, top_n_words, distance, number_of_sentences, percantage=0):\n",
    "    original_setences = [sentence for sentence in nltk.sent_tokenize(text)] # tokenize the text into sentences\n",
    "    # print(\"Before Preprocessing text: \", original_setences)\n",
    "    formatted_sentences = [preprocess(original_sentence) for original_sentence in original_setences] # preprocess each sentence\n",
    "    # print(\"After Preprocessing text: \", formatted_sentences)\n",
    "    words = [word for sentence in formatted_sentences for word in nltk.word_tokenize(sentence)] # tokenize the preprocessed sentences into words\n",
    "    # print(\"Tokenized words: \", words)\n",
    "    frequency = nltk.FreqDist(words) # get the frequency distribution of the words\n",
    "    # return frequency\n",
    "    top_n_words = [word[0] for word in frequency.most_common(top_n_words)] # get the top n words\n",
    "    # print(\"Top n words: \", top_n_words)\n",
    "    sentences_score = calculate_sentences_score(formatted_sentences, top_n_words, distance)\n",
    "    # print(\"Score in each sentence (score, sentence_index): \", sentences_score)\n",
    "    if percantage > 0:\n",
    "        best_sentences = heapq.nlargest(int(len(formatted_sentences) * percantage), sentences_score)\n",
    "    else: \n",
    "        best_sentences = heapq.nlargest(number_of_sentences, sentences_score)\n",
    "    # print(\"Best sentences (score, sentence_index): \", best_sentences)\n",
    "    best_sentences = [original_setences[i] for (score, i) in best_sentences]\n",
    "    # print(best_sentences)\n",
    "    return original_setences, best_sentences, sentences_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "dedc6036",
   "metadata": {},
   "outputs": [],
   "source": [
    "original_setences, best_sentences, sentences_score = summarize(original_text, 5, 2, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "39d7a03d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<h2>Summary</h2>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "  Artificial intelligence is human like intelligence machines. It is the study of intelligent artificial agents.<mark>Science and engineering to produce intelligent machines.</mark> Solve problems and have intelligence.<mark>Related to intelligent behavior machines.</mark> Developing of reasoning machines.<mark>Learn from mistakes and successes.</mark> Artificial intelligence is related to reasoning in everyday situations."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "text = \" \"\n",
    "display(HTML(f\"<h2>Summary</h2>\"))\n",
    "for sentence in original_setences:\n",
    "    if sentence in best_sentences: \n",
    "        text += sentence.replace(sentence, f\"<mark>{sentence}</mark>\")\n",
    "    else: \n",
    "        text += \" \" + sentence\n",
    "display(HTML(f\"\"\"{text}\"\"\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e870a351",
   "metadata": {},
   "source": [
    "**7. Extracting articles from RSS feeds**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d167dc1",
   "metadata": {},
   "source": [
    "AI feeds: https://blog.feedspot.com/ai_rss_feeds/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d4393af2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: feedparser in /Users/mohammadazimi/Desktop/P1-summarization/.venv/lib/python3.13/site-packages (6.0.12)\n",
      "Requirement already satisfied: sgmllib3k in /Users/mohammadazimi/Desktop/P1-summarization/.venv/lib/python3.13/site-packages (from feedparser) (1.0.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install feedparser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "48afdd17",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://aitrends.com/feed/\"\n",
    "feed = feedparser.parse(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "cc8b21dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'title': 'Best Practices for Building the AI Development Platform in Government',\n",
       " 'title_detail': {'type': 'text/plain',\n",
       "  'language': None,\n",
       "  'base': 'https://www.aitrends.com/feed/',\n",
       "  'value': 'Best Practices for Building the AI Development Platform in Government'},\n",
       " 'links': [{'rel': 'alternate',\n",
       "   'type': 'text/html',\n",
       "   'href': 'https://www.aitrends.com/ai-world-government/best-practices-for-building-the-ai-development-platform-in-government/'}],\n",
       " 'link': 'https://www.aitrends.com/ai-world-government/best-practices-for-building-the-ai-development-platform-in-government/',\n",
       " 'authors': [{'name': 'Allison Proffitt'}],\n",
       " 'author': 'Allison Proffitt',\n",
       " 'author_detail': {'name': 'Allison Proffitt'},\n",
       " 'published': 'Thu, 28 Oct 2021 23:20:17 +0000',\n",
       " 'published_parsed': time.struct_time(tm_year=2021, tm_mon=10, tm_mday=28, tm_hour=23, tm_min=20, tm_sec=17, tm_wday=3, tm_yday=301, tm_isdst=0),\n",
       " 'tags': [{'term': 'AI World Government', 'scheme': None, 'label': None},\n",
       "  {'term': 'Ethics and Social Issues', 'scheme': None, 'label': None},\n",
       "  {'term': 'Software Development', 'scheme': None, 'label': None},\n",
       "  {'term': 'ai education', 'scheme': None, 'label': None},\n",
       "  {'term': 'AI software testing', 'scheme': None, 'label': None},\n",
       "  {'term': 'ethics and social issues', 'scheme': None, 'label': None},\n",
       "  {'term': 'software development', 'scheme': None, 'label': None},\n",
       "  {'term': 'workforce', 'scheme': None, 'label': None}],\n",
       " 'id': 'https://www.aitrends.com/?p=21159',\n",
       " 'guidislink': False,\n",
       " 'summary': '<img alt=\"\" class=\"webfeedsFeaturedVisual wp-post-image\" height=\"70\" src=\"https://www.aitrends.com/wp-content/uploads/2021/10/10-29SoftwareDev-ArmedServices-2-100x70.jpeg\" style=\"float: left; margin-right: 5px;\" width=\"100\" />By John P. Desmond, AI Trends Editor\\xa0 The AI stack defined by Carnegie Mellon University is fundamental to the approach being taken by the US Army for its AI development platform efforts, according to Isaac Faber, Chief Data Scientist at the US Army AI Integration Center, speaking at the\\xa0AI World Government\\xa0event held in-person and virtually [&#8230;]',\n",
       " 'summary_detail': {'type': 'text/html',\n",
       "  'language': None,\n",
       "  'base': 'https://www.aitrends.com/feed/',\n",
       "  'value': '<img alt=\"\" class=\"webfeedsFeaturedVisual wp-post-image\" height=\"70\" src=\"https://www.aitrends.com/wp-content/uploads/2021/10/10-29SoftwareDev-ArmedServices-2-100x70.jpeg\" style=\"float: left; margin-right: 5px;\" width=\"100\" />By John P. Desmond, AI Trends Editor\\xa0 The AI stack defined by Carnegie Mellon University is fundamental to the approach being taken by the US Army for its AI development platform efforts, according to Isaac Faber, Chief Data Scientist at the US Army AI Integration Center, speaking at the\\xa0AI World Government\\xa0event held in-person and virtually [&#8230;]'},\n",
       " 'content': [{'type': 'text/html',\n",
       "   'language': None,\n",
       "   'base': 'https://www.aitrends.com/feed/',\n",
       "   'value': '<img alt=\"\" class=\"webfeedsFeaturedVisual wp-post-image\" height=\"70\" src=\"https://www.aitrends.com/wp-content/uploads/2021/10/10-29SoftwareDev-ArmedServices-2-100x70.jpeg\" style=\"float: left; margin-right: 5px;\" width=\"100\" /><p><i><span>By John P. Desmond, AI Trends Editor</span></i><span>\\xa0</span></p>\\n<p><span>The AI stack defined by Carnegie Mellon University is fundamental to the approach being taken by the US Army for its AI development platform efforts, according to Isaac Faber, Chief Data Scientist at the US Army AI Integration Center, speaking at the\\xa0</span><a href=\"https://www.aiworldgov.com/\"><span>AI World Government</span></a><span>\\xa0event held in-person and virtually from Alexandria, Va., last week.</span><span>\\xa0</span><span>\\xa0</span></p>\\n<figure class=\"wp-caption alignleft\" id=\"attachment_21162\" style=\"width: 168px;\"><img alt=\"\" class=\"size-full wp-image-21162\" height=\"289\" src=\"https://www.aitrends.com/wp-content/uploads/2021/10/10-29IsaacFaber-2.jpeg\" width=\"168\" /><figcaption class=\"wp-caption-text\" id=\"caption-attachment-21162\">Isaac Faber, Chief Data Scientist, US Army AI Integration Center</figcaption></figure>\\n<p><span>“If we want to move the Army from legacy systems through digital modernization, one of the biggest issues I have found is the difficulty in abstracting away the differences in applications,” he said. “The most important part of digital transformation is the middle layer, the platform that makes it easier to be on the cloud or on a local computer.” The desire is to be able to move your software platform to another platform, with the same ease\\xa0with\\xa0which a new smartphone carries over the user’s contacts and histories.</span><span>\\xa0</span><span>\\xa0</span></p>\\n<p><span>Ethics cuts across all layers of the AI application stack, which positions the planning stage at the top, followed by decision support, modeling, machine learning, massive data management and the device layer or platform at the bottom.</span><span>\\xa0</span><span>\\xa0</span></p>\\n<p><span>“I am advocating that we think of the stack as a core infrastructure and a way for applications to be deployed and not to be siloed in our approach,” he said. “We need to create a development environment for a globally-distributed workforce.”\\xa0</span><span>\\xa0</span><span>\\xa0</span></p>\\n<p><span>The Army has been working on a Common Operating Environment Software (Coes) platform, first announced in 2017, a design for DOD work that is scalable, agile, modular, portable and open. “It is suitable for a broad range of AI projects,” Faber said. For executing the effort, “The devil is in the details,” he said.\\xa0</span><span>\\xa0</span><span>\\xa0</span></p>\\n<p><span>The Army is working with CMU and private companies on a prototype platform, including with\\xa0</span><a href=\"https://visimo.ai/\"><span>Visimo</span></a><span>\\xa0of Coraopolis, Pa., which offers AI development services. Faber said he prefers to collaborate and coordinate with private industry rather than buying products off the shelf. “The problem with that is, you are stuck with the value you are being provided by that one vendor, which is usually not designed for the challenges of DOD networks,” he said.</span><span>\\xa0</span><span>\\xa0</span></p>\\n<p><b><span>Army Trains a Range of Tech Teams in AI</span></b><span>\\xa0</span></p>\\n<p><span>The Army engages in AI workforce development efforts for several teams, including:\\xa0 leadership, professionals with graduate degrees; technical staff, which is put through training to get certified; and AI users.\\xa0</span><span>\\xa0</span><span>\\xa0</span></p>\\n<p><span>Tech teams in the Army have different areas of focus include: general purpose software development, operational data science, deployment which includes analytics, and a machine learning operations team, such as a large team required to build a computer vision system. “As folks come through the workforce, they need a place to collaborate, build and share,” Faber said.\\xa0</span><span>\\xa0</span><span>\\xa0</span></p>\\n<p><span>Types of projects include diagnostic, which might be combining streams of historical data, predictive and prescriptive, which recommends a course of action based on a prediction. “At the far end is AI; you don’t start with that,” said Faber. The developer has to solve three problems: data engineering, the AI development platform, which he called “the green bubble,” and the deployment platform, which he called “the red bubble.”\\xa0</span><span>\\xa0</span><span>\\xa0</span></p>\\n<p><span>“These are mutually exclusive and all interconnected. Those teams of different people need to programmatically coordinate. Usually a good project team will have people from each of those bubble areas,” he said. “If you have not done this yet, do not try to solve the green bubble problem. It makes no sense to pursue AI until you have an operational need.”\\xa0</span><span>\\xa0</span><span>\\xa0</span></p>\\n<p><span>Asked by a participant which group is the most difficult to reach and train, Faber said without hesitation, “The hardest to reach are the executives. They need to learn what the value is to be provided by the AI ecosystem. The biggest challenge is how to communicate that value,” he said.\\xa0</span><span>\\xa0</span><span>\\xa0</span></p>\\n<p><b><span>Panel Discusses AI Use Cases with the Most Potential</span></b><span>\\xa0</span><span>\\xa0</span></p>\\n<p><span>In a panel on Foundations of Emerging AI, moderator Curt Savoie, program director, Global Smart Cities Strategies for IDC, the market research firm, asked what emerging AI use case has the most potential.\\xa0</span><span>\\xa0</span></p>\\n<p><span>Jean-Charles Lede, autonomy tech advisor for the US Air Force, Office of Scientific Research, said,” I would point to decision advantages at the edge, supporting pilots and operators, and decisions at the back, for mission and resource planning.”\\xa0</span><span>\\xa0</span><span>\\xa0</span></p>\\n<figure class=\"wp-caption alignleft\" id=\"attachment_21161\" style=\"width: 215px;\"><img alt=\"\" class=\"size-full wp-image-21161\" height=\"269\" src=\"https://www.aitrends.com/wp-content/uploads/2021/10/10-29KristaKinnard-2.jpeg\" width=\"215\" /><figcaption class=\"wp-caption-text\" id=\"caption-attachment-21161\">Krista Kinnard, Chief of Emerging Technology for the Department of Labor</figcaption></figure>\\n<p><span>Krista Kinnard, Chief of Emerging Technology for the Department of Labor, said, “Natural language processing is an opportunity to open the doors to AI in the Department of Labor,” she said. “Ultimately, we are dealing with data on people, programs,\\xa0and organizations.”\\xa0\\xa0</span><span>\\xa0</span><span>\\xa0</span></p>\\n<p><span>Savoie asked what are the big risks and dangers the panelists see when implementing AI.\\xa0</span><span>\\xa0</span><span>\\xa0</span></p>\\n<p><span>Anil Chaudhry, Director of Federal AI Implementations for the General Services Administration (GSA), said in a typical IT organization using traditional software development, the impact of a decision by a developer only goes so far. With AI, “You have to consider the impact on a whole class of people, constituents,\\xa0and stakeholders. With a simple change in algorithms, you could be delaying benefits to millions of people or making incorrect inferences at scale. That’s the most important risk,” he said.</span><span>\\xa0</span><span>\\xa0</span></p>\\n<p><span>He said he asks his contract partners to have “humans in the loop and humans on the loop.”\\xa0</span><span>\\xa0</span><span>\\xa0</span></p>\\n<p><span>Kinnard seconded this, saying, “We have no intention of removing humans from the loop. It’s really about empowering people to make better decisions.”\\xa0</span><span>\\xa0</span><span>\\xa0</span></p>\\n<p><span>She emphasized the importance of monitoring the AI models after they are deployed. “Models can drift as the data underlying the changes,” she said. “So you need a level of critical thinking to not only do the task, but to assess whether what the AI model is doing is acceptable.”\\xa0</span><span>\\xa0</span><span>\\xa0</span></p>\\n<p><span>She added, “We have built out use cases and partnerships across the government to make sure we’re implementing responsible AI. We will never replace people with algorithms.”</span><span>\\xa0</span><span>\\xa0</span></p>\\n<p><span>Lede of the Air Force said, “We often have use cases where the data does not exist. We cannot explore 50 years of war data, so we use simulation. The risk is in teaching an algorithm that you have a ‘simulation to real gap’ that is a real risk. You are not sure how the algorithms will map to the real world.”</span><span>\\xa0</span><span>\\xa0</span></p>\\n<p><span>Chaudhry emphasized the importance of a testing strategy for AI systems. He warned of developers “who get enamored with a tool and forget the purpose of the exercise.” He recommended the development manager design in independent verification and validation strategy. “Your testing, that is where you have to focus your energy as a leader. The leader needs an idea in mind, before committing resources, on how they will justify whether the investment was a success.”\\xa0</span><span>\\xa0</span><span>\\xa0</span></p>\\n<p><span>Lede of the Air Force talked about the importance of\\xa0explainability. “I am a technologist. I don’t\\xa0do laws. The ability for the AI function to explain in a way a human can interact with, is important. The AI is a partner that we have a dialogue with, instead of the AI coming up with a conclusion that we have no way of verifying,” he said.\\xa0</span><span>\\xa0</span></p>\\n<p><span>Learn more at\\xa0</span><a href=\"https://www.aiworldgov.com/\"><span>AI World Government.</span></a><span>\\xa0</span></p>'}]}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feed.entries[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "cbf846b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Practices for Building the AI Development Platform in Government\n",
      "<img alt=\"\" class=\"webfeedsFeaturedVisual wp-post-image\" height=\"70\" src=\"https://www.aitrends.com/wp-content/uploads/2021/10/10-29SoftwareDev-ArmedServices-2-100x70.jpeg\" style=\"float: left; margin-right: 5px;\" width=\"100\" /><p><i><span>By John P. Desmond, AI Trends Editor</span></i><span> </span></p>\n",
      "<p><span>The AI stack defined by Carnegie Mellon University is fundamental to the approach being taken by the US Army for its AI development platform efforts, according to Isaac Faber, Chief Data Scientist at the US Army AI Integration Center, speaking at the </span><a href=\"https://www.aiworldgov.com/\"><span>AI World Government</span></a><span> event held in-person and virtually from Alexandria, Va., last week.</span><span> </span><span> </span></p>\n",
      "<figure class=\"wp-caption alignleft\" id=\"attachment_21162\" style=\"width: 168px;\"><img alt=\"\" class=\"size-full wp-image-21162\" height=\"289\" src=\"https://www.aitrends.com/wp-content/uploads/2021/10/10-29IsaacFaber-2.jpeg\" width=\"168\" /><figcaption class=\"wp-caption-text\" id=\"caption-attachment-21162\">Isaac Faber, Chief Data Scientist, US Army AI Integration Center</figcaption></figure>\n",
      "<p><span>“If we want to move the Army from legacy systems through digital modernization, one of the biggest issues I have found is the difficulty in abstracting away the differences in applications,” he said. “The most important part of digital transformation is the middle layer, the platform that makes it easier to be on the cloud or on a local computer.” The desire is to be able to move your software platform to another platform, with the same ease with which a new smartphone carries over the user’s contacts and histories.</span><span> </span><span> </span></p>\n",
      "<p><span>Ethics cuts across all layers of the AI application stack, which positions the planning stage at the top, followed by decision support, modeling, machine learning, massive data management and the device layer or platform at the bottom.</span><span> </span><span> </span></p>\n",
      "<p><span>“I am advocating that we think of the stack as a core infrastructure and a way for applications to be deployed and not to be siloed in our approach,” he said. “We need to create a development environment for a globally-distributed workforce.” </span><span> </span><span> </span></p>\n",
      "<p><span>The Army has been working on a Common Operating Environment Software (Coes) platform, first announced in 2017, a design for DOD work that is scalable, agile, modular, portable and open. “It is suitable for a broad range of AI projects,” Faber said. For executing the effort, “The devil is in the details,” he said. </span><span> </span><span> </span></p>\n",
      "<p><span>The Army is working with CMU and private companies on a prototype platform, including with </span><a href=\"https://visimo.ai/\"><span>Visimo</span></a><span> of Coraopolis, Pa., which offers AI development services. Faber said he prefers to collaborate and coordinate with private industry rather than buying products off the shelf. “The problem with that is, you are stuck with the value you are being provided by that one vendor, which is usually not designed for the challenges of DOD networks,” he said.</span><span> </span><span> </span></p>\n",
      "<p><b><span>Army Trains a Range of Tech Teams in AI</span></b><span> </span></p>\n",
      "<p><span>The Army engages in AI workforce development efforts for several teams, including:  leadership, professionals with graduate degrees; technical staff, which is put through training to get certified; and AI users. </span><span> </span><span> </span></p>\n",
      "<p><span>Tech teams in the Army have different areas of focus include: general purpose software development, operational data science, deployment which includes analytics, and a machine learning operations team, such as a large team required to build a computer vision system. “As folks come through the workforce, they need a place to collaborate, build and share,” Faber said. </span><span> </span><span> </span></p>\n",
      "<p><span>Types of projects include diagnostic, which might be combining streams of historical data, predictive and prescriptive, which recommends a course of action based on a prediction. “At the far end is AI; you don’t start with that,” said Faber. The developer has to solve three problems: data engineering, the AI development platform, which he called “the green bubble,” and the deployment platform, which he called “the red bubble.” </span><span> </span><span> </span></p>\n",
      "<p><span>“These are mutually exclusive and all interconnected. Those teams of different people need to programmatically coordinate. Usually a good project team will have people from each of those bubble areas,” he said. “If you have not done this yet, do not try to solve the green bubble problem. It makes no sense to pursue AI until you have an operational need.” </span><span> </span><span> </span></p>\n",
      "<p><span>Asked by a participant which group is the most difficult to reach and train, Faber said without hesitation, “The hardest to reach are the executives. They need to learn what the value is to be provided by the AI ecosystem. The biggest challenge is how to communicate that value,” he said. </span><span> </span><span> </span></p>\n",
      "<p><b><span>Panel Discusses AI Use Cases with the Most Potential</span></b><span> </span><span> </span></p>\n",
      "<p><span>In a panel on Foundations of Emerging AI, moderator Curt Savoie, program director, Global Smart Cities Strategies for IDC, the market research firm, asked what emerging AI use case has the most potential. </span><span> </span></p>\n",
      "<p><span>Jean-Charles Lede, autonomy tech advisor for the US Air Force, Office of Scientific Research, said,” I would point to decision advantages at the edge, supporting pilots and operators, and decisions at the back, for mission and resource planning.” </span><span> </span><span> </span></p>\n",
      "<figure class=\"wp-caption alignleft\" id=\"attachment_21161\" style=\"width: 215px;\"><img alt=\"\" class=\"size-full wp-image-21161\" height=\"269\" src=\"https://www.aitrends.com/wp-content/uploads/2021/10/10-29KristaKinnard-2.jpeg\" width=\"215\" /><figcaption class=\"wp-caption-text\" id=\"caption-attachment-21161\">Krista Kinnard, Chief of Emerging Technology for the Department of Labor</figcaption></figure>\n",
      "<p><span>Krista Kinnard, Chief of Emerging Technology for the Department of Labor, said, “Natural language processing is an opportunity to open the doors to AI in the Department of Labor,” she said. “Ultimately, we are dealing with data on people, programs, and organizations.”  </span><span> </span><span> </span></p>\n",
      "<p><span>Savoie asked what are the big risks and dangers the panelists see when implementing AI. </span><span> </span><span> </span></p>\n",
      "<p><span>Anil Chaudhry, Director of Federal AI Implementations for the General Services Administration (GSA), said in a typical IT organization using traditional software development, the impact of a decision by a developer only goes so far. With AI, “You have to consider the impact on a whole class of people, constituents, and stakeholders. With a simple change in algorithms, you could be delaying benefits to millions of people or making incorrect inferences at scale. That’s the most important risk,” he said.</span><span> </span><span> </span></p>\n",
      "<p><span>He said he asks his contract partners to have “humans in the loop and humans on the loop.” </span><span> </span><span> </span></p>\n",
      "<p><span>Kinnard seconded this, saying, “We have no intention of removing humans from the loop. It’s really about empowering people to make better decisions.” </span><span> </span><span> </span></p>\n",
      "<p><span>She emphasized the importance of monitoring the AI models after they are deployed. “Models can drift as the data underlying the changes,” she said. “So you need a level of critical thinking to not only do the task, but to assess whether what the AI model is doing is acceptable.” </span><span> </span><span> </span></p>\n",
      "<p><span>She added, “We have built out use cases and partnerships across the government to make sure we’re implementing responsible AI. We will never replace people with algorithms.”</span><span> </span><span> </span></p>\n",
      "<p><span>Lede of the Air Force said, “We often have use cases where the data does not exist. We cannot explore 50 years of war data, so we use simulation. The risk is in teaching an algorithm that you have a ‘simulation to real gap’ that is a real risk. You are not sure how the algorithms will map to the real world.”</span><span> </span><span> </span></p>\n",
      "<p><span>Chaudhry emphasized the importance of a testing strategy for AI systems. He warned of developers “who get enamored with a tool and forget the purpose of the exercise.” He recommended the development manager design in independent verification and validation strategy. “Your testing, that is where you have to focus your energy as a leader. The leader needs an idea in mind, before committing resources, on how they will justify whether the investment was a success.” </span><span> </span><span> </span></p>\n",
      "<p><span>Lede of the Air Force talked about the importance of explainability. “I am a technologist. I don’t do laws. The ability for the AI function to explain in a way a human can interact with, is important. The AI is a partner that we have a dialogue with, instead of the AI coming up with a conclusion that we have no way of verifying,” he said. </span><span> </span></p>\n",
      "<p><span>Learn more at </span><a href=\"https://www.aiworldgov.com/\"><span>AI World Government.</span></a><span> </span></p>\n",
      "Advance Trustworthy AI and ML, and Identify Best Practices for Scaling AI\n",
      "<img alt=\"\" class=\"webfeedsFeaturedVisual wp-post-image\" height=\"70\" src=\"https://www.aitrends.com/wp-content/uploads/2021/10/10-29GSAOffice-2-100x70.jpeg\" style=\"float: left; margin-right: 5px;\" width=\"100\" /><p><i><span>By John P. Desmond, AI Trends Editor</span></i><span> </span><span> </span></p>\n",
      "<p><span>Advancing trustworthy AI and machine learning to mitigate agency risk is a priority for the US Department of Energy (DOE), and identifying best practices for implementing AI at scale is a priority for the US General Services Administration (GSA).</span><span> </span><span> </span></p>\n",
      "<p><span>That’s what attendees learned in two sessions at the </span><a href=\"https://www.aiworldgov.com/\"><span>AI World Government</span></a><span> live and virtual event held in Alexandria, Va. last week. </span><span> </span><span> </span></p>\n",
      "<figure class=\"wp-caption alignleft\" id=\"attachment_21165\" style=\"width: 200px;\"><img alt=\"\" class=\"size-full wp-image-21165\" height=\"295\" src=\"https://www.aitrends.com/wp-content/uploads/2021/10/10-29PamIsom-2.jpeg\" width=\"200\" /><figcaption class=\"wp-caption-text\" id=\"caption-attachment-21165\">Pamela Isom, Director of the AI and Technology Office, DOE</figcaption></figure>\n",
      "<p><span>Pamela Isom, Director of the AI and Technology Office at the DOE, who spoke on Advancing Trustworthy AI and ML Techniques for Mitigating Agency Risks, has been involved in proliferating the use of AI across the agency for several years. With an emphasis on applied AI and data science, she oversees risk mitigation policies and standards and has been involved with applying AI to save lives, fight fraud, and strengthen the cybersecurity infrastructure.</span><span> </span><span> </span></p>\n",
      "<p><span>She emphasized the need for the AI project effort to be part of a strategic portfolio. “My office is there to drive a holistic view on AI and to mitigate risk by bringing us together to address challenges,” she said. The effort is assisted by the DOE’s AI and Technology Office, which is focused on transforming the DOE into a world-leading AI enterprise by accelerating research, development, delivery and the adoption of AI.</span><span> </span><span> </span></p>\n",
      "<p><span>“I am telling my organization to be mindful of the fact that you can have tons and tons of data, but it might not be representative,” she said. Her team looks at examples from international partners, industry, academia and other agencies for outcomes “we can trust” from systems incorporating AI.</span><span> </span><span> </span></p>\n",
      "<p><span>“We know that AI is disruptive, in trying to do what humans do and do it better,” she said. “It is beyond human capability; it goes beyond data in spreadsheets; it can tell me what I’m going to do next before I contemplate it myself. It’s that powerful,” she said.</span><span> </span><span> </span></p>\n",
      "<p><span>As a result, close attention must be paid to data sources. “AI is vital to the economy and our national security. We need precision; we need algorithms we can trust; we need accuracy. We don’t need biases,” Isom said, adding, “And don’t forget that you need to monitor the output of the models long after they have been deployed.” </span><span> </span><span> </span></p>\n",
      "<p><b><span>Executive Orders Guide GSA AI Work</span></b><span> </span></p>\n",
      "<p><span>Executive Order 14028, a detailed set of actions to address the cybersecurity of government agencies, issued in May of this year, and Executive Order 13960, promoting the use of trustworthy AI in the Federal government, issued in December 2020, provide valuable guides to her work. </span><span> </span><span> </span></p>\n",
      "<p><span>To help manage the risk of AI development and deployment, Isom has produced the AI Risk Management Playbook, which provides guidance around system features and mitigation techniques. It also has a filter for ethical and trustworthy principles which are considered throughout AI lifecycle stages and risk types. Plus, the playbook ties to relevant Executive Orders.</span><span> </span><span> </span></p>\n",
      "<p><span>And it provides examples, such as your results came in at 80% accuracy, but you wanted 90%. “Something is wrong there,” Isom said, adding, “The playbook helps you look at these types of problems and what you can do to mitigate risk, and what factors you should weigh as you design and build your project.”</span><span> </span><span> </span></p>\n",
      "<p><span>While internal to DOE at present, the agency is looking into next steps for an external version. “We will share it with other federal agencies soon,” she said. </span><span> </span><span> </span></p>\n",
      "<p><b><span>GSA Best Practices for Scaling AI Projects Outlined</span></b><span> </span><span> </span></p>\n",
      "<figure class=\"wp-caption alignleft\" id=\"attachment_21167\" style=\"width: 200px;\"><img alt=\"\" class=\"size-full wp-image-21167\" height=\"200\" src=\"https://www.aitrends.com/wp-content/uploads/2021/10/10-29AnilChaudhry-2.jpeg\" width=\"200\" /><figcaption class=\"wp-caption-text\" id=\"caption-attachment-21167\">Anil Chaudhry, Director of Federal AI Implementations, AI Center of Excellence (CoE), GSA</figcaption></figure>\n",
      "<p><span>Anil Chaudhry, Director of Federal AI Implementations for the AI Center of Excellence (CoE) of the GSA, who spoke on Best Practices for Implementing AI at Scale, has over 20 years of experience in technology delivery, operations and program management in the defense, intelligence and national security sectors. </span><span> </span><span> </span></p>\n",
      "<p><span>The mission of the CoE is to accelerate technology modernization across the government, improve the public experience and increase operational efficiency. “Our business model is to partner with industry subject matter experts to solve problems,” Chaudhry said, adding, “We are not in the business of recreating industry solutions and duplicating them.” </span><span> </span><span> </span></p>\n",
      "<p><span>The CoE is providing recommendations to partner agencies and working with them to implement AI systems as the federal government engages heavily in AI development. “For AI, the government landscape is vast. Every federal agency has some sort of AI project going on right now,” he said, and the maturity of AI experience varies widely across agencies.</span><span> </span><span> </span></p>\n",
      "<p><span>Typical use cases he is seeing include having AI focus on increasing speed and efficiency, on cost savings and cost avoidance, on improved response time and increased quality and compliance. As one best practice, he recommended the agencies </span><b><span>vet their commercial experience </span></b><span>with the large datasets they will encounter in government. </span><span> </span><span> </span></p>\n",
      "<p><span>“We’re talking petabytes and exabytes here, of structured and unstructured data,” Chaudhry said. </span><i><span>[Ed. Note: A petabyte is 1,000 terabytes.] </span></i><span>“Also ask industry partners about their strategies and processes on how they do macro and micro trend analysis, and what their experience has been in the deployment of bots such as in Robotic Process Automation, and how they demonstrate sustainability as a result of drift of data.” </span><span> </span><span> </span></p>\n",
      "<p><span>He also asks potential industry partners to</span><b><span> describe the AI talent on their team</span></b><span> or what talent they can access. If the company is weak on AI talent, Chaudhry would ask, “If you buy something, how will you know you got what you wanted when you have no way of evaluating it?”</span><span> </span><span> </span></p>\n",
      "<p><span>He added, “A best practice in implementing AI is defining how you train your workforce to leverage AI tools, techniques and practices, and to define how you grow and mature your workforce. Access to talent leads to either success or failure in AI projects, especially when it comes to scaling a pilot up to a fully deployed system.”</span><span> </span><span> </span></p>\n",
      "<p><span>In another best practice, Chaudhry recommended examining the industry partner’s </span><b><span>access to financial capital.</span></b><span> “AI is a field where the flow of capital is highly volatile. “You cannot predict or project that you will spend X amount of dollars this year to get where you want to be,” he said, because an AI development team may need to explore another hypothesis, or clean up some data that may not be transparent or is potentially biased. “If you don’t have access to funding, it is a risk your project will fail,” he said.</span><span> </span><span> </span></p>\n",
      "<p><span>Another best practice is </span><b><span>access to logistical capital</span></b><span>, such as the data  that sensors collect for an AI IoT system. “AI requires an enormous amount of data that is authoritative and timely. Direct access to that data is critical,” Chaudhry said. He recommended that data sharing agreements  be in place with organizations relevant to the AI system. “You might not need it right away, but having access to the data, so you could immediately use it and to have thought through the privacy issues before you need the data, is a good practice for scaling AI programs,” he said. </span><span> </span><span> </span></p>\n",
      "<p><span>A final best practice is planning of </span><b><span>physical infrastructure, </span></b><span>such as data center space. “When you are in a pilot, you need to know how much capacity you need to reserve at your data center, and how many end points you need to manage” when the application scales up, Chaudhry said, adding, “This all ties back to access to capital and all the other best practices.“</span><span> </span></p>\n",
      "<p><span>Learn more at </span><a href=\"https://www.aiworldgov.com/\"><span>AI World Government</span></a><span>.</span><span> </span></p>\n",
      "Promise and Perils of Using AI for Hiring: Guard Against Data Bias\n",
      "<img alt=\"\" class=\"webfeedsFeaturedVisual wp-post-image\" height=\"70\" src=\"https://www.aitrends.com/wp-content/uploads/2021/10/10-29EEOCLogo-1-100x70.png\" style=\"float: left; margin-right: 5px;\" width=\"100\" /><p><i><span>By AI Trends Staff</span></i><span> </span><span> </span></p>\n",
      "<p><span>While AI in hiring is now widely used for writing job descriptions, screening candidates, and automating interviews, it poses a risk of wide discrimination if not implemented carefully.</span><span> </span></p>\n",
      "<figure class=\"wp-caption alignleft\" id=\"attachment_21170\" style=\"width: 200px;\"><img alt=\"\" class=\"size-full wp-image-21170\" height=\"200\" src=\"https://www.aitrends.com/wp-content/uploads/2021/10/10-29KeithSonderling-1.jpeg\" width=\"200\" /><figcaption class=\"wp-caption-text\" id=\"caption-attachment-21170\">Keith Sonderling, Commissioner, US Equal Opportunity Commission</figcaption></figure>\n",
      "<p><span>That was the message from Keith Sonderling, Commissioner with the US Equal Opportunity Commision, speaking at the </span><a href=\"https://www.aiworldgov.com/\"><span>AI World Government </span></a><span>event held live and virtually in Alexandria, Va., last week. Sonderling is responsible for enforcing federal laws that prohibit discrimination against job applicants because of race, color, religion, sex, national origin, age or disability. </span><span> </span><span> </span></p>\n",
      "<p><span>“The thought that AI would become mainstream in HR departments was closer to science fiction two year ago, but the pandemic has accelerated the rate at which AI is being used by employers,” he said. “Virtual recruiting is now here to stay.”</span><span> </span><span> </span></p>\n",
      "<p><span>It’s a busy time for HR professionals. “The great resignation is leading to the great rehiring, and AI will play a role in that like we have not seen before,” Sonderling said. </span><span> </span></p>\n",
      "<p><span>AI has been employed for years in hiring—“It did not happen overnight.”—for tasks including chatting with applications, predicting whether a candidate would take the job, projecting what type of employee they would be and mapping out upskilling and reskilling opportunities. “In short, AI is now making all the decisions once made by HR personnel,” which he did not characterize as good or bad. </span><span> </span><span> </span></p>\n",
      "<p><span>“Carefully designed and properly used, AI has the potential to make the workplace more fair,” Sonderling said. “But carelessly implemented, AI could discriminate on a scale we have never seen before by an HR professional.”</span><span> </span><span> </span></p>\n",
      "<p><b><span>Training Datasets for AI Models Used for Hiring Need to Reflect Diversity</span></b><span> </span><span> </span></p>\n",
      "<p><span>This is because AI models rely on training data. If the company’s current workforce is used as the basis for training, “It will replicate the status quo. If it’s one gender or one race primarily, it will replicate that,” he said. Conversely, AI can help mitigate risks of hiring bias by race, ethnic background, or disability status. “I want to see AI improve on workplace discrimination,” he said.</span><span> </span><span> </span></p>\n",
      "<p><span>Amazon began building a hiring application in 2014, and found over time that it discriminated against women in its recommendations, because the AI model was trained on a dataset of the company’s own hiring record for the previous 10 years, which was primarily of males. Amazon developers tried to correct it but ultimately scrapped the system in 2017. </span><span> </span><span> </span></p>\n",
      "<p><span>Facebook has recently agreed to pay $14.25 million to settle civil claims by the US government that the social media company discriminated against American workers and violated federal recruitment rules, according to an account from </span><a href=\"https://www.reuters.com/legal/litigation/facebook-pay-up-1425-mln-settle-us-employment-discrimination-claims-2021-10-19/\"><span>Reuters</span></a><span>. The case centered on Facebook’s use of what it called its PERM program for labor certification. The government found that Facebook refused to hire American workers for jobs that had been reserved for temporary visa holders under the PERM program. </span><span> </span><span> </span></p>\n",
      "<p><span>“Excluding people from the hiring pool is a violation,” Sonderling said.  If the AI program “withholds the existence of the job opportunity to that class, so they cannot exercise their rights, or if it downgrades a protected class, it is within our domain,” he said. </span><span> </span><span> </span></p>\n",
      "<p><span>Employment assessments, which became more common after World War II, have provided  high value to HR managers and with help from AI they have the potential to minimize bias in hiring. “At the same time, they are vulnerable to claims of discrimination, so employers need to be careful and cannot take a hands-off approach,” Sonderling said. “Inaccurate data will amplify bias in decision-making. Employers must be vigilant against discriminatory outcomes.”</span><span> </span><span> </span></p>\n",
      "<p><span>He recommended researching solutions from vendors who vet data for risks of bias on the basis of race, sex, and other factors. </span><span> </span><span> </span></p>\n",
      "<p><span>One example is from </span><a href=\"https://www.hirevue.com/\"><span>HireVue </span></a><span>of South Jordan, Utah, which has built a hiring platform predicated on the US Equal Opportunity Commission’s Uniform Guidelines, designed specifically to mitigate unfair hiring practices, according to an account from </span><a href=\"https://allwork.space/2021/10/ai-streamlines-the-hiring-process-for-many-companies-but-is-that-a-good-thing/\"><span>allWork</span></a><span>. </span><span> </span><span> </span></p>\n",
      "<p><span>A post on AI ethical principles on its website states in part, “Because HireVue uses AI technology in our products, we actively work to prevent the introduction or propagation of bias against any group or individual. We will continue to carefully review the datasets we use in our work and ensure that they are as accurate and diverse as possible. We also continue to advance our abilities to monitor, detect, and mitigate bias. We strive to build teams from diverse backgrounds with diverse knowledge, experiences, and perspectives to best represent the people our systems serve.”</span><span> </span><span> </span></p>\n",
      "<p><span>Also, “Our data scientists and IO psychologists build HireVue Assessment algorithms in a way that removes data from consideration by the algorithm that contributes to adverse impact without significantly impacting the assessment’s predictive accuracy. The result is a highly valid, bias-mitigated assessment that helps to enhance human decision making while actively promoting diversity and equal opportunity regardless of gender, ethnicity, age, or disability status.”</span><span> </span><span> </span></p>\n",
      "<figure class=\"wp-caption alignleft\" id=\"attachment_21172\" style=\"width: 200px;\"><img alt=\"\" class=\"size-full wp-image-21172\" height=\"200\" src=\"https://www.aitrends.com/wp-content/uploads/2021/10/10-29EdIkeguchi-1.jpeg\" width=\"200\" /><figcaption class=\"wp-caption-text\" id=\"caption-attachment-21172\">Dr. Ed Ikeguchi, CEO, AiCure</figcaption></figure>\n",
      "<p><span>The issue of bias in datasets used to train AI models is not confined to hiring. Dr. Ed Ikeguchi, CEO of AiCure, an AI analytics company working in the life sciences industry, stated in a recent account in </span><a href=\"https://www.healthcareitnews.com/news/how-one-ai-company-works-reduce-algorithmic-bias\"><span>HealthcareITNews</span></a><span>, “</span><span>AI is only as strong as the data it&#8217;s fed, and lately that data backbone&#8217;s credibility is being increasingly called into question. Today&#8217;s AI developers lack access to large, diverse data sets on which to train and validate new tools.”</span><span> </span><span> </span></p>\n",
      "<p><span>He added, “They often need to leverage open-source datasets, but many of these were trained using computer programmer volunteers, which is a predominantly white population. Because algorithms are often trained on single-origin data samples with limited diversity, when applied in real-world scenarios to a broader population of different races, genders, ages, and more, tech that appeared highly accurate in research may prove unreliable.”</span><span> </span></p>\n",
      "<p><span>Also, “There needs to be an element of governance and peer review for all algorithms, as even the most solid and tested algorithm is bound to have unexpected results arise. An algorithm is never done learning</span><span>—</span><span>it must be constantly developed and fed more data to improve.”</span><span> </span></p>\n",
      "<p><span>And, “As an industry, we need to become more skeptical of AI&#8217;s conclusions and encourage transparency in the industry. Companies should readily answer basic questions, such as &#8216;How was the algorithm trained? On what basis did it draw this conclusion?”</span><span> </span></p>\n",
      "<p><span>Read the source articles and information at </span><a href=\"https://www.aiworldgov.com/\"><span>AI World Government</span></a><span>, f</span><span>rom </span><a href=\"https://www.reuters.com/legal/litigation/facebook-pay-up-1425-mln-settle-us-employment-discrimination-claims-2021-10-19/\"><span>Reuters</span></a><span> and from </span><a href=\"https://www.healthcareitnews.com/news/how-one-ai-company-works-reduce-algorithmic-bias\"><span>HealthcareITNews</span></a><span>.</span><span> </span></p>\n",
      "Predictive Maintenance Proving Out as Successful AI Use Case\n",
      "<img alt=\"\" class=\"webfeedsFeaturedVisual wp-post-image\" height=\"70\" src=\"https://www.aitrends.com/wp-content/uploads/2021/10/10-29PredictiveMaintenance-2-100x70.jpeg\" style=\"float: left; margin-right: 5px;\" width=\"100\" /><p><i><span>By John P. Desmond, AI Trends Editor</span></i><span> </span><span> </span></p>\n",
      "<p><span>More companies are successfully exploiting predictive maintenance systems that combine AI and IoT sensors to collect data that anticipates breakdowns and recommends preventive action before break or machines fail, in a demonstration of an AI use case with proven value. </span><span> </span></p>\n",
      "<p><span>This growth is reflected in optimistic market forecasts. The predictive maintenance market is sized at $6.9 billion today and is projected to grow to $28.2 billion by 2026, according to a report from </span><a href=\"https://iot-analytics.com/predictive-maintenance-market-evolution-from-niche-topic-to-high-roi-application/\"><span>IoT Analytics</span></a><span> of Hamburg, Germany. The firm counts over 280 vendors offering solutions in the market today, projected to grow to over 500 by 2026.</span><span> </span><span> </span></p>\n",
      "<figure class=\"wp-caption alignleft\" id=\"attachment_21157\" style=\"width: 200px;\"><img alt=\"\" class=\"size-full wp-image-21157\" height=\"200\" src=\"https://www.aitrends.com/wp-content/uploads/2021/10/10-29FernandoB-2.jpeg\" width=\"200\" /><figcaption class=\"wp-caption-text\" id=\"caption-attachment-21157\">Fernando Bruegge, Analyst, IoT Analytics, Hamburg, Germany</figcaption></figure>\n",
      "<p><span>“This research is a wake-up call to those that claim IoT is failing,” stated analyst Fernando Bruegge, author of the report, adding, “For companies that own industrial assets or sell equipment, now is the time to invest in predictive maintenance-type solutions.” And, “Enterprise technology firms need to prepare to integrate predictive maintenance solutions into their offerings,” Bruegge suggested.</span><span> </span><span> </span></p>\n",
      "<p><span>Here is a review of some specific experience with predictive maintenance systems that combine AI and IoT sensors.</span><span> </span></p>\n",
      "<p><span>Aircraft engine manufacturer</span><b><span> </span></b><a href=\"https://www.rolls-royce.com/\"><b><span>Rolls-Royce</span></b></a><span> is </span><a href=\"https://www.cio.com/article/3620993/rolls-royce-turns-to-digital-twins-to-improve-jet-engine-efficiency.html\"><span>deploying predictive analytics</span></a><span> to help reduce the amount of carbon its engines produce, while also optimizing maintenance to help customers keep planes in the air longer, according to a recent account in </span><a href=\"https://www.cio.com/article/3624014/predictive-analytics-4-success-stories.html?upd=1635362935537\"><span>CIO</span></a><span>.</span><span> </span></p>\n",
      "<p><span>Rolls-Royce built an Intelligent Engine platform to monitor engine flight, gathering data on weather conditions and how pilots are flying. Machine learning is applied to the data to customize maintenance regimes for individual engines.</span><span> </span></p>\n",
      "<figure class=\"wp-caption alignleft\" id=\"attachment_21155\" style=\"width: 200px;\"><img alt=\"\" class=\"size-full wp-image-21155\" height=\"199\" src=\"https://www.aitrends.com/wp-content/uploads/2021/10/10-29StuartHughes-2.jpeg\" width=\"200\" /><figcaption class=\"wp-caption-text\" id=\"caption-attachment-21155\">Stuart Hughes, chief information and digital officer, Rolls-Royce</figcaption></figure>\n",
      "<p><span>“We’re tailoring our maintenance regimes to make sure that we’re optimizing for the life an engine has, not the life the manual says it should have,” stated Stuart Hughes, chief information and digital officer at Rolls-Royce. “It’s truly variable service, looking at each engine as an individual engine.”</span><span> </span></p>\n",
      "<p><span>Customers are seeing less service interruption. “Rolls-Royce has been monitoring engines and charging per hour for at least 20 years,” Hughes stated. “That part of the business isn’t new. But as we’ve evolved, we’ve begun to treat the engine as a singular engine. It’s much more about the personalization of that engine.”</span><span> </span><span> </span></p>\n",
      "<p><span>Predictive analytics is being applied in healthcare as well as in the manufacturing industry. Kaiser Permanente, the integrated managed care consortium based in Oakland, Calif. Is using predictive analytics to identify non-intensive care unit (ICU) patients at risk of rapid deterioration. </span><span> </span><span> </span></p>\n",
      "<p><span>While non-ICU patients that require unexpected transfers to the ICU constitute less than 4% of the total hospital population, they account for 20% of all hospital deaths, according to Dr. Gabriel Escobar, research scientist, Division of Research, and regional director, Hospital Operations Research, Kaiser Permanente Northern California.</span><span> </span></p>\n",
      "<p><b><span>Kaiser Permanente Practicing Predictive Maintenance in Healthcare</span></b><span> </span></p>\n",
      "<p><span>Kaiser Permanente developed the Advanced Alert Monitor (AAM) system, leveraging three predictive analytic models to analyze more than 70 factors in a given patient’s electronic health record to generate a composite risk score.</span><span> </span></p>\n",
      "<p><span>“The AAM system synthesizes and analyzes vital statistics, lab results, and other variables to generate hourly deterioration risk scores for adult hospital patients in the medical-surgical and transitional care units,” stated Dick Daniels, executive vice president and CIO of Kaiser Permanente in the CIO account. “Remote hospital teams evaluate the risk scores every hour and notify rapid response teams in the hospital when potential deterioration is detected. The rapid response team conducts bedside evaluation of the patient and calibrates the course treatment with the hospitalist.”</span><span> </span></p>\n",
      "<p><span>In advice to other practitioners, Daniels recommended a focus on how the tool will be fit into the workflow of health care teams. “It took us about five years to perform the initial mapping of the electronic medical record backend and develop the predictive models,” Daniels stated. “It then took us another two to three years to transition these models into a live web services application that could be used operationally.”</span><span> </span></p>\n",
      "<p><span>In an example from the food industry, a PepsiCo Frito-Lay plant in Fayetteville, Tenn. is using predictive maintenance successfully, with year-to-date equipment downtime at 0.75% and unplanned downtime at 2.88%, according to Carlos Calloway, the site’s reliability engineering manager, in an account in </span><a href=\"https://www.plantservices.com/articles/2021/push-the-needle-how-6-companies-are-achieving-predictive-maintenance-success/\"><span>PlantServices</span></a><span>.</span><span> </span></p>\n",
      "<p><span>Examples of monitoring include: vibration readings confirmed by ultrasound helped to prevent a PC combustion blower motor from failing and shutting down the whole potato chip department; infrared analysis of the main pole for the plant’s GES automated warehouse detected a hot fuse holder, which helped to avoid a shutdown of the entire warehouse; and increased acid levels were detected in oil samples from a baked extruder gearbox, indicating oil degradation, which enabled prevention of a shutdown of Cheetos Puffs production.</span><span> </span></p>\n",
      "<p><span>The Frito-Lay plant produces more than 150 million pounds of product per year, including Lays, Ruffles, Cheetos, Doritos, Fritos, and Tostitos.</span><span> </span><span> </span></p>\n",
      "<p><span>The types of monitoring include vibration analysis, used on mechanical applications, which is processed with the help of a third-party company which sends alerts to the plant for investigation and resolution. Another service partner performs quarterly vibration monitoring on selected equipment. All motor control center rooms and electrical panels are monitored with quarterly infrared analysis, which is also used on electrical equipment, some rotating equipment, and heat exchangers. In addition, the plant has done ultrasonic monitoring for more than 15 years, and it is “kind of like the pride and joy of our site from a predictive standpoint,” stated Calloway.</span><span> </span><span> </span></p>\n",
      "<p><span>The plan has a number of products in place from UE Systems of Elmsford, NY, supplier of ultrasonic instruments, hardware and software, and training for predictive maintenance. </span><span> </span><span> </span></p>\n",
      "<p><b><span>Louisiana Alumina Plant Automating Bearing Maintenance </span></b><span> </span><span> </span></p>\n",
      "<p><span>Bearings, which wear over time under varying conditions of weather and temperature in the case of automobiles, are a leading candidate for IoT monitoring and predictive maintenance with AI. The </span><a href=\"http://www.newdayal.com/noranda-alumina\"><span>Noranda Alumina</span></a><span> plant in Gramercy, La. is finding a big payoff from its investment in a system to improve the lubrication of bearings in its production equipment. </span><span> </span></p>\n",
      "<p><span>The system has resulted in a 60% decline in bearing changes in the second year of using the new lubrication system, translating to some $900,000 in savings on bearings that did not need to be replaced and avoided downtime. </span><span> </span></p>\n",
      "<p><span>“Four hours of downtime is about $1 million dollars’ worth of lost production,” stated Russell Goodwin, a reliability engineer and millwright instructor at Noranda Alumina, in the PlantServices account, which was based on presentations at the Leading Reliability 2021 event.</span><span> </span></p>\n",
      "<p><span>The Noranda Alumina plant is the only alumina plant operating in the US. “If we shut down, you’ll need to import it,” stated Goodwin. The plant experiences pervasive dust, dirt, and caustic substances, which complicate efforts at improved reliability and maintenance practices.</span><span> </span><span> </span></p>\n",
      "<p><span>Noranda Alumina tracks all motors and gearboxes at 1,500 rpm and higher with vibration readings, and most below 1,500 with ultrasound. Ultrasonic monitoring, of sound in ranges beyond human hearing, was introduced to the plant after Goodwin joined the company in 2019. At the time, grease monitoring had room for improvement. “If grease was not visibly coming out of the seal, the mechanical supervisor did not count the round as complete,” stated Goodwin.</span><span> </span><span> </span></p>\n",
      "<p><span>After introducing automation, the greasing system has improved dramatically, he stated. The system was also able to detect bearings in a belt whose bearings were wearing out too quickly due to contamination. “Tool-enabled tracking helped to prove that it wasn’t improper greasing, but rather the bearing was made improperly,” stated Goodwin.</span><span> </span><span> </span></p>\n",
      "<p><span>Read the source articles and information in </span><span> </span><a href=\"https://iot-analytics.com/predictive-maintenance-market-evolution-from-niche-topic-to-high-roi-application/\"><span>IoT Analytics</span></a><span>, </span><span>in </span><a href=\"https://www.cio.com/article/3624014/predictive-analytics-4-success-stories.html?upd=1635362935537\"><span>CIO</span></a><span> and in </span><a href=\"https://www.plantservices.com/articles/2021/push-the-needle-how-6-companies-are-achieving-predictive-maintenance-success/\"><span>PlantServices</span></a><span>.</span><span> </span></p>\n",
      "Novelty In The Game Of Go Provides Bright Insights For AI And Autonomous Vehicles\n",
      "<img alt=\"\" class=\"webfeedsFeaturedVisual wp-post-image\" height=\"70\" src=\"https://www.aitrends.com/wp-content/uploads/2021/10/10-29GameofGo-2-100x70.jpeg\" style=\"float: left; margin-right: 5px;\" width=\"100\" /><p><i><span>By Lance Eliot, the AI Trends Insider</span></i><span> </span><span> </span></p>\n",
      "<p><span>We already expect that humans to exhibit flashes of brilliance. It might not happen all the time, but the act itself is welcomed and not altogether disturbing when it occurs.</span><span> </span><span> </span><span> </span></p>\n",
      "<p><span>What about when Artificial Intelligence (AI) seems to display an act of novelty? Any such instance is bound to get our attention; questions arise right away.</span><span> </span><span> </span><span> </span></p>\n",
      "<p><span>How did the AI come up with the apparent out-of-the-blue insight or novel indication? Was it a mistake, or did it fit within the parameters of what the AI was expected to produce? There is also the immediate consideration of whether the AI somehow is slipping toward the precipice of becoming sentient.</span><span> </span><span> </span><span> </span></p>\n",
      "<p><span>Please be aware that no AI system in existence is anywhere close to reaching sentience, despite the claims and falsehoods tossed around in the media. As such, if today’s AI seems to do something that appears to be a novel act, you should not leap to the conclusion that this is a sign of human insight within technology or the emergence of human ingenuity among AI.</span><span> </span><span> </span><span> </span></p>\n",
      "<p><span>That’s an anthropomorphic bridge too far.</span><span> </span><span> </span><span> </span></p>\n",
      "<p><span>The reality is that any such AI “insightful” novelties are based on various concrete computational algorithms and tangible data-based pattern matching.</span><span> </span><span> </span><span> </span></p>\n",
      "<p><span>In today’s column, we’ll be taking a close look at an example of an AI-powered novel act, illustrated via the game of Go, and relate these facets to the advent of AI-based true self-driving cars as a means of understanding the AI-versus-human related ramifications.</span><span> </span></p>\n",
      "<p><span>Realize that the capacity to spot or suggest a novelty is being done methodically by an AI system, while, in contrast, no one can say for sure how humans can devise novel thoughts or intuitions.</span><span> </span></p>\n",
      "<p><span>Perhaps we too are bound by some internal mechanistic-like facets, or maybe there is something else going on. Someday, hopefully, we will crack open the secret inner workings of the mind and finally know how we think. I suppose it might undercut the mystery and magical aura that oftentimes goes along with those of us that have moments of outside-the-box visions, though I’d trade that enigma to know how the cups-and-balls trickery truly functions (going behind the curtain, as it were).</span><span> </span><span> </span><span> </span></p>\n",
      "<p><span>Speaking of novelty, a famous game match involving the playing of Go can provide useful illumination on this overall topic.</span><span> </span><span> </span><span> </span></p>\n",
      "<p><span>Go is a popular board game in the same complexity category as chess. Arguments are made about which is tougher, chess or Go, but I’m not going to get mired into that morass. For the sake of civil discussion, the key point is that Go is highly complex and requires intense mental concentration especially at the tournament level.</span><span> </span><span> </span><span> </span></p>\n",
      "<p><span>Generally, Go consists of trying to capture territory on a standard Go board, consisting of a 19 by 19 grid of intersecting lines. For those of you that have never tried playing Go, the closest similar kind of game might be the connect-the-dots that you played in childhood, which involves grabbing up territory, though Go is magnitudes more involved. </span><span> </span><span> </span><span> </span></p>\n",
      "<p><span>There is no need for you to know anything in particular about Go to get the gist of what will be discussed next regarding the act of human novelty and the act of AI novelty.</span><span> </span><span> </span><span> </span></p>\n",
      "<p><span>A famous Go competition took place about four years ago that pitted one of the world’s top professional Go players, Lee Sedol, against an AI program that had been crafted to play Go, coined as AlphaGo. There is a riveting documentary about the contest and plenty of write-ups and online videos that have in detail covered the match, including post-game analysis.</span><span> </span><span> </span><span> </span></p>\n",
      "<p><span>Put yourself back in time to 2016 and relive what happened.</span><span> </span></p>\n",
      "<p><span>Most AI developers did not anticipate that the AI of that time would be proficient enough to beat a top Go player. Sure, AI had already been able to best some top chess players, and thus offered a glimmer of expectation that Go would eventually be equally undertaken, but there weren’t any Go programs that had been able to compete at the pinnacle levels of human Go players. Most expected that it would probably be around the year 2020 or so before the capabilities of AI would be sufficient to compete in world-class Go tournaments.</span><span> </span><span> </span></p>\n",
      "<p><b><span>DeepMind Created AlphaGo Using Deep Learning, Machine Learning</span></b><span> </span><span> </span><span> </span></p>\n",
      "<p><span>A small-sized tech company named DeepMind Technologies devised the AlphaGo AI playing system (the firm was later acquired by Google). Using techniques from Machine Learning and Deep Learning, the AlphaGo program was being revamped and adjusted right up to the actual tournament, a typical kind of last-ditch developer contortions that many of us have done when trying to get the last bit of added edge into something that is about to be demonstrated.</span><span> </span><span> </span><span> </span></p>\n",
      "<p><span>This was a monumental competition that had garnered global interest.</span><span> </span><span> </span><span> </span></p>\n",
      "<p><span>Human players of Go were doubtful that the AlphaGo program would win. Many AI techies were doubtful that AlphaGo would win. Even the AlphaGo developers were unsure of how well the program would do, including the stay-awake-at-night fears that the AlphaGo program would hit a bug or go into a kind of delusional mode and make outright mistakes and play foolishly.</span><span> </span><span> </span><span> </span></p>\n",
      "<p><span>A million dollars in prize money was put into the pot for the competition. There would be five Go games played, one per day, along with associated rules about taking breaks, etc. Some predicted that Sedol would handily win all five games, doing so without cracking a sweat. AI pundits were clinging to the hope that AlphaGo would win at least one of the five games, and otherwise, present itself as a respectable level of Go player throughout the contest.</span><span> </span></p>\n",
      "<p><span>In the first match, AlphaGo won.</span><span> </span><span> </span><span> </span></p>\n",
      "<p><span>This was pretty much a worldwide shocker. Sedol was taken aback. Lots of Go players were surprised that a computer program could compete and beat someone at Sedol’s level of play. Everyone began to give some street cred to the AlphaGo program and the efforts by the AI developers.</span><span> </span><span> </span><span> </span></p>\n",
      "<p><span>Tension grew for the next match.</span><span> </span><span> </span><span> </span></p>\n",
      "<p><span>For the second game, it was anticipated that Sedol might significantly change his approach to the contest. Perhaps he had been overconfident coming into the competition, some harshly asserted, and the loss of the first game would awaken him to the importance of putting all his concentration into the tournament. Or, possibly he had played as though he was competing with a lesser capable player and thus was not pulling out all the stops to try and win the match.</span><span> </span><span> </span><span> </span></p>\n",
      "<p><span>What happened in the second game?</span><span> </span></p>\n",
      "<p><span>Turns out that AlphaGo prevailed, again, and also did something that was seemingly remarkable for those that avidly play Go. On the 37</span><span>th</span><span> move of the match, the AlphaGo program opted to make placement onto the Go board in a spot that nobody especially anticipated. It was a surprise move, coming partway through a match that otherwise was relatively conventional in the nature of the moves being made by both Sedol and AlphaGo.</span><span> </span><span> </span><span> </span></p>\n",
      "<p><span>At the time, in real-time, rampant speculation was that the move was an utter gaffe on the part of the AlphaGo program.</span><span> </span><span> </span><span> </span></p>\n",
      "<p><span>Instead, it became famous as a novel move, known now as “Move 37” and heralded in Go and used colloquially overall to suggest any instance when AI does something of a novel or unexpected manner.</span><span> </span><span> </span><span> </span></p>\n",
      "<p><span>In the third match, AlphaGo won again, now having successfully beaten Sedol in a 3-out-of-5 winner competition. They continued though to play a fourth and a fifth game.</span><span> </span><span> </span><span> </span></p>\n",
      "<p><span>During the fourth game, things were tight as usual and the match play was going head-to-head (well, head versus AI). Put yourself into the shoes of Sedol. In one sense, he wasn’t just a Go player, he was somehow representing all of humanity (an unfair and misguided viewpoint, but pervasive anyway), and the pressure was on him to win at least one game. Just even one game would be something to hang your hat on, and bolster faith in mankind (again, a nonsensical way to look at it).</span><span> </span><span> </span><span> </span></p>\n",
      "<p><span>At the seventy-eighth move of the fourth game, Sedol made a so-called “wedge” play that was not conventional and surprised onlookers. The next move by AlphaGo was rotten and diminished the likelihood of a win by the AI system. After additional play, ultimately AlphaGo tossed in the towel and resigned from the match, thus Sedol finally had a win against the AI in his belt. He ended-up losing the fifth game, so AlphaGo won four games, Sedol won one). His move also became famous, generally known as “Move 78” in the lore of Go playing.</span><span> </span></p>\n",
      "<p><span>Something else that is worthwhile to know about involves the overarching strategy that AlphaGo was crafted to utilize.</span><span> </span><span> </span><span> </span></p>\n",
      "<p><span>When you play a game, let’s say connect-the-dots, you can aim to grab as many squares at each moment of play, doing so under the belief that inevitably you will then win by the accumulation of those tactically-oriented successes. Human players of Go are often apt to play that way, as it can be said too of chess players, and nearly any kind of game playing altogether.</span><span> </span><span> </span><span> </span></p>\n",
      "<p><span>Another approach involves playing to win, even if only by the thinnest of margins, as long as you win. In that case, you might not be motivated for each tactical move to gain near-term territory or score immediate points, and be willing instead to play a larger scope game per se. The proverbial mantra is that if you are shortsighted, you might win some of the battles, but could eventually lose the war. Therefore, it might be a better strategy to keep your eye on the prize, winning the war, albeit if it means that there are battles and skirmishes to be lost along the way.</span><span> </span><span> </span><span> </span></p>\n",
      "<p><span>The AI developers devised AlphaGo with that kind of macro-perspective underlying how the AI system functioned.</span><span> </span><span> </span><span> </span></p>\n",
      "<p><span>Humans can have an especially hard time choosing at the moment to make a move that might look bad or ill-advised, such as giving up territory, finding themselves to be unable to grit their teeth, and taking a lump or two during play. The embarrassment at the instant is difficult to offset by betting that it is going to ultimately be okay, and you will prevail in the end.</span><span> </span><span> </span><span> </span></p>\n",
      "<p><span>For an AI system, there is no semblance of that kind of sentiment involved, and it is all about calculated odds and probabilities.</span><span> </span><span> </span><span> </span></p>\n",
      "<p><span>Now that we’ve covered the legendary Go match, let’s consider some lessons learned about novelty.</span><span> </span><span> </span><span> </span></p>\n",
      "<p><span>The “Move 38” made by the AI system was not magical. It was an interesting move, for sure, and the AI developers later indicated that the move was one that the AI had calculated would rarely be undertaken by a human player.</span><span> </span><span> </span><span> </span></p>\n",
      "<p><span>This can be interpreted in two ways (at least).</span><span> </span><span> </span><span> </span></p>\n",
      "<p><span>One interpretation is that a human player would not make that move because humans are right and know that it would be a lousy move.</span><span> </span><span> </span><span> </span></p>\n",
      "<p><span>Another interpretation is that humans would not make that move due to a belief that the move is unwise, but this could be a result of the humans insufficiently assessing the ultimate value of the move, in the long-run, and getting caught up in a shorter time frame semblance of play.</span><span> </span></p>\n",
      "<p><span>In this instance, it turned out to be a good move—maybe a brilliant move—and turned the course of the game to the advantage of the AI. Thus, what looked like brilliance was in fact a calculated move that few humans would have imagined as valuable and for which jostled humans to rethink how they think about such matters.</span><span> </span><span> </span><span> </span></p>\n",
      "<p><span>Some useful recap lessons:</span><span> </span><span> </span><span> </span></p>\n",
      "<p><b><span>Showcasing Human Self-Limited Insight.</span></b><span> When the AI does something seemingly novel, it might be viewed as novel simply because humans have already predetermined what is customary and anything beyond that is blunted by the assumption that it is unworthy or mistaken. You could say that we are mentally trapped by our own drawing of the lines of what is considered as inside versus outside the box.</span><span> </span><span> </span><span> </span></p>\n",
      "<p><b><span>Humans Exploiting AI For Added Insight</span></b><span>. Humans can gainfully assess an AI-powered novelty to potentially re-calibrate human thinking on a given topic, enlarging our understanding via leveraging something that the AI, via its vast calculative capacity, might detect or spot that we have not yet so ascertained. Thus, besides admiring the novelty, we ought to seek to improve our mental prowess by whatever source shines brightly including an AI system.</span><span> </span><span> </span><span> </span></p>\n",
      "<p><b><span>AI Novelty Is A Dual-Edged Sword.</span></b><span> We need to be mindful of all AI systems and their possibility of acting in a novel way, which could be good or could be bad. In the Go game, it worked out well. In other circumstances, the AI exploiting the novelty route might go off the tracks, as it were.</span><span> </span><span> </span><span> </span></p>\n",
      "<p><span>Let’s see how this can be made tangible via exploring the advent of AI-based true self-driving cars.</span><span> </span><span> </span><span> </span></p>\n",
      "<p><span>For my framework about AI autonomous cars, see the link here: </span><a href=\"https://aitrends.com/ai-insider/framework-ai-self-driving-driverless-cars-big-picture/\"><span>https://aitrends.com/ai-insider/framework-ai-self-driving-driverless-cars-big-picture/</span></a><span> </span><span> </span><span> </span></p>\n",
      "<p><span>Why this is a moonshot effort, see my explanation here: </span><a href=\"https://aitrends.com/ai-insider/self-driving-car-mother-ai-projects-moonshot/\"><span>https://aitrends.com/ai-insider/self-driving-car-mother-ai-projects-moonshot/</span></a><span> </span><span> </span><span> </span></p>\n",
      "<p><span>For more about the levels as a type of Richter scale, see my discussion here: </span><a href=\"https://aitrends.com/ai-insider/richter-scale-levels-self-driving-cars/\"><span>https://aitrends.com/ai-insider/richter-scale-levels-self-driving-cars/</span></a><span> </span><span> </span><span> </span></p>\n",
      "<p><span>For the argument about bifurcating the levels, see my explanation here: </span><a href=\"https://aitrends.com/ai-insider/reframing-ai-levels-for-self-driving-cars-bifurcation-of-autonomy/\"><span>https://aitrends.com/ai-insider/reframing-ai-levels-for-self-driving-cars-bifurcation-of-autonomy/</span></a><span> </span><span> </span><span> </span></p>\n",
      "<p><b><span>Understanding The Levels Of Self-Driving Cars</span></b><span> </span></p>\n",
      "<p><span>As a clarification, true self-driving cars are ones where the AI drives the car entirely on its own and there isn’t any human assistance during the driving task.</span><span> </span></p>\n",
      "<p><span>These driverless vehicles are considered a Level 4 and Level 5, while a car that requires a human driver to co-share the driving effort is usually considered at a Level 2 or Level 3. The cars that co-share the driving task are described as being semi-autonomous, and typically contain a variety of automated add-on’s that are referred to as ADAS (Advanced Driver-Assistance Systems).</span><span> </span><span> </span><span> </span></p>\n",
      "<p><span>There is not yet a true self-driving car at Level 5, which we don’t yet even know if this will be possible to achieve, and nor how long it will take to get there.</span><span> </span><span> </span><span> </span></p>\n",
      "<p><span>Meanwhile, the Level 4 efforts are gradually trying to get some traction by undergoing very narrow and selective public roadway trials, though there is controversy over whether this testing should be allowed per se (we are all life-or-death guinea pigs in an experiment taking place on our highways and byways, some contend).</span><span> </span><span> </span><span> </span></p>\n",
      "<p><span>For why remote piloting or operating of self-driving cars is generally eschewed, see my explanation here: </span><a href=\"https://aitrends.com/ai-insider/remote-piloting-is-a-self-driving-car-crutch/\"><span>https://aitrends.com/ai-insider/remote-piloting-is-a-self-driving-car-crutch/</span></a><span> </span><span> </span><span> </span></p>\n",
      "<p><span>To be wary of fake news about self-driving cars, see my tips here: </span><a href=\"https://aitrends.com/ai-insider/ai-fake-news-about-self-driving-cars/\"><span>https://aitrends.com/ai-insider/ai-fake-news-about-self-driving-cars/</span></a><span> </span></p>\n",
      "<p><span>The ethical implications of AI driving systems are significant, see my indication here: </span><a href=\"http://aitrends.com/selfdrivingcars/ethically-ambiguous-self-driving-cars/\"><span>http://aitrends.com/selfdrivingcars/ethically-ambiguous-self-driving-cars/</span></a><span> </span><span> </span><span> </span></p>\n",
      "<p><span>Be aware of the pitfalls of normalization of deviance when it comes to self-driving cars, here’s my call to arms: </span><a href=\"https://aitrends.com/ai-insider/normalization-of-deviance-endangers-ai-self-driving-cars/\"><span>https://aitrends.com/ai-insider/normalization-of-deviance-endangers-ai-self-driving-cars/</span></a><span> </span></p>\n",
      "<p><b><span>Self-Driving Cars And Acts Of Novelty</span></b><span> </span><span> </span><span> </span></p>\n",
      "<p><span>For Level 4 and Level 5 true self-driving vehicles, there won’t be a human driver involved in the driving task. All occupants will be passengers; the AI is doing the driving.</span><span> </span><span> </span><span> </span></p>\n",
      "<p><span>You could say that the AI is playing a game, a driving game, requiring tactical decision-making and strategic planning, akin to when playing Go or chess, though in this case involving life-or-death matters driving a multi-ton car on our public roadways.</span><span> </span><span> </span><span> </span></p>\n",
      "<p><span>Our base assumption is that the AI driving system is going to always take a tried-and-true approach to any driving decisions. This assumption is somewhat shaped around a notion that AI is a type of robot or automata that is bereft of any human biases or human foibles.</span><span> </span><span> </span><span> </span></p>\n",
      "<p><span>In reality, there is no reason to make this kind of assumption. Yes, we can generally rule out the aspect that the AI is not going to display the emotion of a human ilk, and we also know that the AI will not be drunk or DUI in its driving efforts. Nonetheless, if the AI has been trained using Machine Learning (ML) and Deep Learning (DL), it can pick up subtleties of human behavioral patterns in the data about human driving, out of which it will likewise utilize or mimic in choosing its driving actions (for example, see my column postings involving an analysis of potential racial biases in AI and the possibility of gender biases).</span><span> </span><span> </span><span> </span></p>\n",
      "<p><span>Turning back to the topic of novelty, let’s ponder a specific use case.</span><span> </span><span> </span><span> </span></p>\n",
      "<p><span>A few years ago, I was driving on an open highway, going at the prevailing speed of around 65 miles per hour, and something nearly unimaginable occurred. A car coming toward me in the opposing lane, and likely traveling at around 60 to 70 miles per hour, suddenly and unexpectedly veered into my lane. It was one of those moments that you cannot anticipate.</span><span> </span><span> </span><span> </span></p>\n",
      "<p><span>There did not appear to be any reason for the other driver to be headed toward me, in my lane of traffic, and coming at me for an imminent and bone-chillingly terrifying head-on collision. If there had been debris on the other lane, it might have been a clue that perhaps this other driver was simply trying to swing around the obstruction. No debris. If there was a slower moving car, the driver might have wanted to do a fast end-around to get past it. Nope, there was absolutely no discernible basis for this radical and life-threatening maneuver.</span><span> </span></p>\n",
      "<p><span>What would you do?</span><span> </span></p>\n",
      "<p><span>Come on, hurry, the clock is ticking, and you have just a handful of split seconds to make a life-or-death driving decision.</span><span> </span><span> </span><span> </span></p>\n",
      "<p><span>You could stay in your lane and hope that the other driver realizes the error of their ways, opting to veer back into their lane at the last moment. Or, you could proactively go into the opposing lane, giving the other driver a clear path in your lane, but this could be a chancy game of chicken whereby the other driver chooses to go back into their lane (plus, there was other traffic further behind that driver, so going into the opposing lane was quite dicey).</span><span> </span><span> </span><span> </span></p>\n",
      "<p><span>Okay, so do you stay in your lane or veer away into the opposing lane?</span><span> </span><span> </span><span> </span></p>\n",
      "<p><span>I dare say that most people would be torn between those two options. Neither one is palatable.</span><span> </span></p>\n",
      "<p><span>Suppose the AI of a self-driving car was faced with the same circumstance.</span><span> </span><span> </span><span> </span></p>\n",
      "<p><span>What would the AI do?</span><span> </span><span> </span><span> </span></p>\n",
      "<p><span>The odds are that even if the AI had been fed with thousands upon thousands of miles of driving via a database about human driving while undergoing the ML/DL training, there might not be any instances of a head-to-head nature and thus no prior pattern to utilize for making this onerous decision.</span><span> </span><span> </span><span> </span></p>\n",
      "<p><span>Anyway, here’s a twist.</span><span> </span><span> </span><span> </span></p>\n",
      "<p><span>Imagine that the AI calculated the probabilities involving which way to go, and in some computational manner came to the conclusion that the self-driving car should go into the ditch that was at the right of the roadway. This was intended to avoid entirely a collision with the other car (the AI estimated that a head-on collision would be near-certain death for the occupants). The AI estimated that going into the ditch at such high speed would indisputably wreck the car and cause great bodily injury to the occupants, but the odds of assured death were (let’s say) calculated as lower than the head-on option possibilities (this is a variant of the infamous Trolley Problem, as covered in my columns).</span><span> </span><span> </span><span> </span></p>\n",
      "<p><span>I’m betting that you would concede that most humans would be relatively unwilling to aim purposely into that ditch, which they know for sure is going to be a wreck and potential death, while instead willing (reluctantly) to take a hoped-for chance of either veering into the other lane or staying on course and wishing for the best.</span><span> </span><span> </span><span> </span></p>\n",
      "<p><span>In some sense, the AI might seem to have made a novel choice. It is one that (we’ll assume) few humans would have given any explicit thought toward.</span><span> </span><span> </span><span> </span></p>\n",
      "<p><span>Returning to the earlier recap of the points about AI novelty, you could suggest that in this example, the AI has exceeded a human self-imposed limitation by the AI having considered otherwise “unthinkable” options. From this, perhaps we can learn to broaden our view for options that otherwise don’t seem apparent.</span><span> </span><span> </span><span> </span></p>\n",
      "<p><span>The other recap element was that the AI novelty can be a dual-edged sword.</span><span> </span><span> </span><span> </span></p>\n",
      "<p><span>If the AI did react by driving into the ditch, and you were inside the self-driving car, and you got badly injured, would you later believe that the AI acted in a novel manner or that it acted mistakenly or adversely?</span><span> </span><span> </span><span> </span></p>\n",
      "<p><span>Some might say that if you lived to ask that question, apparently the AI made the right choice. The counter-argument is that if the AI had gone with one of the other choices, perhaps you would have sailed right past the other car and not gotten a single scratch.</span><span> </span><span> </span><span> </span></p>\n",
      "<p><span>For more details about ODDs, see my indication at this link here: </span><a href=\"https://www.aitrends.com/ai-insider/amalgamating-of-operational-design-domains-odds-for-ai-self-driving-cars/\"><span>https://www.aitrends.com/ai-insider/amalgamating-of-operational-design-domains-odds-for-ai-self-driving-cars/</span></a><span> </span></p>\n",
      "<p><span>On the topic of off-road self-driving cars, here’s my details elicitation: </span><a href=\"https://www.aitrends.com/ai-insider/off-roading-as-a-challenging-use-case-for-ai-autonomous-cars/\"><span>https://www.aitrends.com/ai-insider/off-roading-as-a-challenging-use-case-for-ai-autonomous-cars/</span></a><span> </span></p>\n",
      "<p><span>I’ve urged that there must be a Chief Safety Officer at self-driving car makers, here’s the scoop: </span><a href=\"https://www.aitrends.com/ai-insider/chief-safety-officers-needed-in-ai-the-case-of-ai-self-driving-cars/\"><span>https://www.aitrends.com/ai-insider/chief-safety-officers-needed-in-ai-the-case-of-ai-self-driving-cars/</span></a><span> </span></p>\n",
      "<p><span>Expect that lawsuits are going to gradually become a significant part of the self-driving car industry, see my explanatory details here: </span><a href=\"http://aitrends.com/selfdrivingcars/self-driving-car-lawsuits-bonanza-ahead/\"><span>http://aitrends.com/selfdrivingcars/self-driving-car-lawsuits-bonanza-ahead/</span></a><span> </span></p>\n",
      "<p><b><span>Conclusion</span></b><span> </span><span> </span><span> </span></p>\n",
      "<p><span>For those of you wondering what actually did happen, my lucky stars were looking over me that day, and I survived with nothing more than a close call. I decided to remain in my lane, though it was tempting to veer into the opposing lane, and by some miracle, the other driver suddenly went back into the opposing lane.</span><span> </span><span> </span><span> </span></p>\n",
      "<p><span>When I tell the story, my heart still gets pumping, and I begin to sweat.</span><span> </span><span> </span><span> </span></p>\n",
      "<p><span>Overall, AI that appears to engage in novel approaches to problems can be advantageous and in some circumstances such as playing a board game can be right or wrong, for which being wrong does not especially put human lives at stake.</span><span> </span><span> </span><span> </span></p>\n",
      "<p><span>For AI-based true self-driving cars, lives are at stake.</span><span> </span><span> </span><span> </span></p>\n",
      "<p><span>We’ll need to proceed mindfully and with our eyes wide open about how we want AI driving systems to operate, including calculating odds and deriving choices while at the wheel of the vehicle.</span><span> </span><span> </span></p>\n",
      "<p><i><span>Copyright 2021 Dr. Lance Eliot </span></i><span> </span></p>\n",
      "<p><a href=\"http://ai-selfdriving-cars.libsyn.com/website\"><i><span>http://ai-selfdriving-cars.libsyn.com/website</span></i></a><span> </span></p>\n",
      "Getting Government AI Engineers to Tune into AI Ethics Seen as Challenge\n",
      "<img alt=\"\" class=\"webfeedsFeaturedVisual wp-post-image\" height=\"70\" src=\"https://www.aitrends.com/wp-content/uploads/2021/10/10-22AIWorld-Ethics-2-100x70.jpeg\" style=\"float: left; margin-right: 5px;\" width=\"100\" /><p><i><span>By John P. Desmond, AI Trends Editor</span></i><span> </span><span> </span></p>\n",
      "<p><span>Engineers tend to see things in unambiguous terms, which some may call Black and White terms, such as a choice between right or wrong and good and bad. The consideration of ethics in AI is highly nuanced, with vast gray areas, making it  challenging for AI software engineers to apply it in their work.</span><span> </span><span> </span></p>\n",
      "<p><span>That was a takeaway from a session on the Future of Standards and Ethical AI at the </span><a href=\"https://www.aiworldgov.com/\"><span>AI World Government</span></a><span> conference held in-person and virtually in Alexandria, Va. this week. </span><span> </span><span> </span></p>\n",
      "<p><span>An overall impression from the conference is that the discussion of AI and ethics is happening in virtually every quarter of AI in the vast enterprise of the federal government, and the consistency of points being made across all these different and independent efforts stood out.</span><span> </span><span> </span></p>\n",
      "<figure class=\"wp-caption alignleft\" id=\"attachment_21144\" style=\"width: 200px;\"><img alt=\"\" class=\"size-full wp-image-21144\" height=\"200\" src=\"https://www.aitrends.com/wp-content/uploads/2021/10/10-22Beth-AnnSchuelke-2.jpeg\" width=\"200\" /><figcaption class=\"wp-caption-text\" id=\"caption-attachment-21144\">Beth-Ann Schuelke-Leech, associate professor, engineering management, University of Windsor</figcaption></figure>\n",
      "<p><span>“We engineers often think of ethics as a fuzzy thing that no one has really explained,” stated Beth-Anne Schuelke-Leech, an associate professor, Engineering Management and Entrepreneurship at the University of Windsor, Ontario, Canada, speaking at the Future of Ethical AI session. “It can be difficult for engineers looking for solid constraints to be told to be ethical. That becomes really complicated because we don’t know what it really means.”</span><span> </span><span> </span></p>\n",
      "<p><span>Schuelke-Leech started her career as an engineer, then decided to pursue a PhD in public policy, a background which enables her to see things as an engineer and as a social scientist. “I got a PhD in social science, and have been pulled back into the engineering world where I am involved in AI projects, but based in a mechanical engineering faculty,” she said. </span><span> </span><span> </span></p>\n",
      "<p><span>An engineering project has a goal, which describes the purpose, a set of needed features and functions, and a set of constraints, such as budget and timeline “The standards and regulations become part of the constraints,” she said. “If I know I have to comply with it, I will do that. But if you tell me it’s a good thing to do, I may or may not adopt that.”</span><span> </span><span> </span></p>\n",
      "<p><span>Schuelke-Leech also serves as chair of the IEEE Society’s Committee on the Social Implications of Technology Standards. She commented, “Voluntary compliance standards such as from the IEEE are essential from people in the industry getting together to say this is what we think we should do as an industry.”</span><span> </span><span> </span></p>\n",
      "<p><span>Some standards, such as around interoperability, do not have the force of law but engineers comply with them, so their systems will work. Other standards are described as good practices, but are not required to be followed. “Whether it helps me to achieve my goal or hinders me getting to the objective, is how the engineer looks at it,” she said. </span><span> </span><span> </span></p>\n",
      "<p><b><span>The Pursuit of AI Ethics Described as “Messy and Difficult”</span></b><span> </span><span> </span></p>\n",
      "<figure class=\"wp-caption alignleft\" id=\"attachment_21143\" style=\"width: 178px;\"><img alt=\"\" class=\"size-medium wp-image-21143\" height=\"300\" src=\"https://www.aitrends.com/wp-content/uploads/2021/10/10-22SaraJordan-2-178x300.jpeg\" width=\"178\" /><figcaption class=\"wp-caption-text\" id=\"caption-attachment-21143\">Sara Jordan, senior counsel, Future of Privacy Forum</figcaption></figure>\n",
      "<p><span>Sara Jordan, senior counsel with the Future of Privacy Forum, in the session with Schuelke-Leech, works on the ethical challenges of AI and machine learning and is an active member of the IEEE Global Initiative on Ethics and Autonomous and Intelligent Systems. “Ethics is messy and difficult, and is context-laden. We have a proliferation of theories, frameworks and constructs,” she said, adding, “The practice of ethical AI will require repeatable, rigorous thinking in context.”</span><span> </span><span> </span></p>\n",
      "<p><span>Schuelke-Leech offered, “Ethics is not an end outcome. It is the process being followed. But I’m also looking for someone to tell me what I need to do to do my job, to tell me how to be ethical, what rules I’m supposed to follow, to take away the ambiguity.”</span><span> </span><span> </span></p>\n",
      "<p><span>“Engineers shut down when you get into funny words that they don’t understand, like ‘ontological,’ They’ve been taking math and science since they were 13-years-old,” she said.</span><span> </span><span> </span></p>\n",
      "<p><span>She has found it difficult to get engineers involved in attempts to draft standards for ethical AI. “Engineers are missing from the table,” she said. “The debates about whether we can get to 100% ethical are conversations engineers do not have.”</span><span> </span><span> </span></p>\n",
      "<p><span>She concluded, “If their managers tell them to figure it out, they will do so. We need to help the engineers cross the bridge halfway. It is essential that social scientists and engineers don’t give up on this.”</span><span> </span><span> </span></p>\n",
      "<p><b><span>Leader’s Panel Described Integration of Ethics into AI Development Practices</span></b><span> </span><span> </span></p>\n",
      "<p><span>The topic of ethics in AI is coming up more in the curriculum of the US Naval War College of Newport, R.I., which was established to provide advanced study for US Navy officers and now educates leaders from all services. Ross Coffey, a military professor of National Security Affairs at the institution, participated in a Leader&#8217;s Panel on AI, Ethics and Smart Policy at AI World Government. </span><span> </span></p>\n",
      "<p><span>“The ethical literacy of students increases over time as they are working with these ethical issues, which is why it is an urgent matter because it will take a long time,” Coffey said.</span><span> </span><span> </span></p>\n",
      "<p><span>Panel member Carole Smith, a senior research scientist with Carnegie Mellon University who studies human-machine interaction, has been involved in integrating ethics into AI systems development since 2015. She cited the importance of “demystifying” AI.  </span><span> </span><span> </span></p>\n",
      "<p><span>“My interest is in understanding what kind of interactions we can create where the human is appropriately trusting the system they are working with, not over- or under-trusting it,” she said, adding, “In general, people have higher expectations than they should for the systems.”</span><span> </span><span> </span></p>\n",
      "<p><span>As an example, she cited the Tesla Autopilot features, which implement self-driving car capability to a degree but not completely. “People assume the system can do a much broader set of activities than it was designed to do. Helping people understand the limitations of a system is important. Everyone needs to understand the expected outcomes of a system and what some of the mitigating circumstances might be,” she said. </span><span> </span><span> </span></p>\n",
      "<p><span>Panel member Taka Ariga, the first chief data scientist appointed to the US Government Accountability Office and director of the GAO’s Innovation Lab, sees a gap in AI literacy for the young workforce coming into the federal government.  “Data scientist training does not always include ethics. Accountable AI is a laudable construct, but I’m not sure everyone buys into it. We need their responsibility to go beyond technical aspects and be accountable to the end user we are trying to serve,” he said.</span><span> </span><span> </span></p>\n",
      "<p><span>Panel moderator Alison Brooks, PhD, research VP of Smart Cities and Communities at the IDC market research firm, asked whether principles of ethical AI can be shared across the boundaries of nations. </span><span> </span><span> </span></p>\n",
      "<p><span>“We will have a limited ability for every nation to align on the same exact approach, but we will have to align in some ways on what we will not allow AI to do, and what people will also be responsible for,” stated Smith of CMU. </span><span> </span><span> </span></p>\n",
      "<p><span>The panelists credited the European Commission for being out front on these issues of ethics, especially in the enforcement realm.</span><span> </span></p>\n",
      "<p><span>Ross of the Naval War Colleges acknowledged the importance of finding common ground around AI ethics. “From a military perspective, our interoperability needs to go to a whole new level. We need to find common ground with our partners and our allies on what we will allow AI to do and what we will not allow AI to do.” Unfortunately, “I don’t know if that discussion is happening,” he said.</span><span> </span><span> </span></p>\n",
      "<p><span>Discussion on AI ethics could perhaps be pursued as part of certain existing treaties, Smith suggested</span><span> </span><span> </span></p>\n",
      "<p><span>The many AI ethics principles, frameworks, and road maps being offered in many federal agencies can be challenging to follow and be made consistent. Take said, “I am hopeful that over the next year or two, we will see a coalescing.”</span><span> </span><span> </span></p>\n",
      "<p><span>For more information and access to recorded sessions, go to </span><a href=\"https://www.aiworldgov.com/\"><span>AI World Government</span></a><span>.</span><span> </span></p>\n",
      "Digital Natives Seen Having Advantages as Part of Government AI Engineering Teams\n",
      "<img alt=\"\" class=\"webfeedsFeaturedVisual wp-post-image\" height=\"70\" src=\"https://www.aitrends.com/wp-content/uploads/2021/10/10-22NSFBuilding-2-100x70.jpeg\" style=\"float: left; margin-right: 5px;\" width=\"100\" /><p><i><span>By John P. Desmond, AI Trends Editor </span></i><span> </span></p>\n",
      "<p><span>AI is more accessible to young people in the workforce who grew up as ‘digital natives’ with Alexa and self-driving cars as part of the landscape, giving them expectations grounded in their experience of what is possible.</span><span> </span><span> </span></p>\n",
      "<p><span>That idea set the foundation for a panel discussion at </span><a href=\"https://www.aiworldgov.com/\"><span>AI World Government</span></a><span> on Mindset Needs and Skill Set Myths for AI engineering teams, held this week virtually and in-person in Alexandria, Va.</span><span> </span><span> </span></p>\n",
      "<figure class=\"wp-caption alignleft\" id=\"attachment_21139\" style=\"width: 200px;\"><img alt=\"\" class=\"size-full wp-image-21139\" height=\"255\" src=\"https://www.aitrends.com/wp-content/uploads/2021/10/10-22DorothyAronson-2.jpeg\" width=\"200\" /><figcaption class=\"wp-caption-text\" id=\"caption-attachment-21139\">Dorothy Aronson, CIO and Chief Data Officer, National Science Foundation</figcaption></figure>\n",
      "<p><span>“People feel that AI is within their grasp because the technology is available, but the technology is ahead of our cultural maturity,” said panel member Dorothy Aronson, CIO and Chief Data Officer for the National Science Foundation. “It’s like giving a sharp object to a child. We might have access to big data, but it might not be the right thing to do,” to work with it in all cases. </span><span> </span><span> </span></p>\n",
      "<p><span>Things are accelerating, which is raising expectations. When panel member Vivek Rao, lecturer and researcher at the University of California at Berkeley, was working on his PhD, a paper on natural language processing might be a master’s thesis. “Now we assign it as a homework assignment with a two-day turnaround. We have an enormous amount of compute power that was not available even two years ago,” he said of his students, who he described as “digital natives” with high expectations of what AI makes possible.</span><span> </span><span> </span></p>\n",
      "<figure class=\"wp-caption alignleft\" id=\"attachment_21137\" style=\"width: 200px;\"><img alt=\"\" class=\"size-full wp-image-21137\" height=\"200\" src=\"https://www.aitrends.com/wp-content/uploads/2021/10/10-22RachelDzombak-2.jpeg\" width=\"200\" /><figcaption class=\"wp-caption-text\" id=\"caption-attachment-21137\">Rachel Dzombak, digital transformation lead, Software Engineering Institute, Carnegie Mellon University</figcaption></figure>\n",
      "<p><span>Panel moderator Rachel Dzombak, digital transformation lead at the </span><a href=\"https://www.sei.cmu.edu/\"><span>Software Engineering Institute </span></a><span>of Carnegie Mellon University, asked the panelists what is unique about working on AI in the government. </span><span> </span><span> </span></p>\n",
      "<p><span>Aronson said the government cannot get too far ahead with the technology, or the users will not know how to interact with it. “We’re not building iPhones,” she said. “We have experimentation going on, and we are always looking ahead, anticipating the future, so we can make the most cost-effective decisions. In the government right now, we are seeing the convergence of the emerging generation and the close-to-retiring generation, who we also have to serve.” </span><span> </span><span> </span></p>\n",
      "<p><span>Early in her career, Aronson did not want to work in the government. “I thought it meant you were either in the armed services or the Peace Corps,” she said. “But what I learned after a while is what motivates federal employees is service to larger, problem-solving institutions. We are trying to solve really big problems of equity and diversity, and getting food to people and keeping people safe. People that work for the government are dedicated to those missions.” </span><span> </span><span> </span></p>\n",
      "<p><span>She referred to her two children in their 20s, who like the idea of service, but in “tiny chunks,” meaning, “They don’t look at the government as a place where they have freedom, and they can do whatever they want. They see it as a lockdown situation. But it’s really not.” </span><span> </span><span> </span></p>\n",
      "<p><b><span>Berkeley Students Learn About Role of Government in Disaster Response </span></b><span> </span></p>\n",
      "<p><span>Rao of Berkeley said his students are seeing wildfires in California and asking who is working on the challenge of doing something about them. When he tells them it is almost always local, state and federal government entities, “Students are generally surprised to find that out.” </span><span> </span><span> </span></p>\n",
      "<p><span>In one example, he developed a course on innovation in disaster response, in collaboration with CMU and the Department of Defense, the Army Futures Lab and Coast Guard search and rescue. “This was eye-opening for students,” he said. At the outset, two of 35 students expressed interest in a federal government career. By the end of the course, 10 of the 35 students were expressing interest. One of them was hired by the Naval Surface Warfare Center outside Corona, Calif. as a software engineer, Rao said.</span><span> </span><span> </span></p>\n",
      "<p><span>Aronson described the process of bringing on new federal employees as a “heavy lift,” suggesting, “if we could prepare in advance, it would move a lot faster.”</span><span> </span></p>\n",
      "<figure class=\"wp-caption alignleft\" id=\"attachment_21140\" style=\"width: 188px;\"><img alt=\"\" class=\"size-medium wp-image-21140\" height=\"300\" src=\"https://www.aitrends.com/wp-content/uploads/2021/10/10-22BryanLane-2-188x300.jpeg\" width=\"188\" /><figcaption class=\"wp-caption-text\" id=\"caption-attachment-21140\">Bryan Lane, director of Data &amp; AI, General Services Administration</figcaption></figure>\n",
      "<p><span>Asked by Dzombak what skill sets and mindsets are seen as essential to AI engineering teams, panel member Bryan Lane, director of Data &amp; AI at the General Services Administration (who announced during the session that he is taking on a new role at FDIC), said resiliency is a necessary quality. </span><span> </span></p>\n",
      "<p><span>Lane is a technology executive within the GSA IT Modernization Centers of Excellence (CoE) with over 15 years of experience leading advanced analytics and technology initiatives. He has led the GSA partnership with the DoD Joint Artificial Intelligence Center (JAIC). </span><i><span>[Ed. Note: Known as “the Jake.”] </span></i><span>Lane also is the founder of </span><a href=\"https://www.dataxd.io/\"><span>DATA XD.</span></a><span> He also has experience in industry, managing acquisition portfolios. </span><span> </span><span> </span></p>\n",
      "<p><span>“The most important thing about resilient teams going on an AI journey is that you need to be ready for the unexpected, and the mission persists,” he said. “If you are all aligned on the importance of the mission, the team can be held together.”</span><span> </span><span> </span></p>\n",
      "<p><b><span>Good Sign that Team Members Acknowledge Having “Never Done This Before”</span></b><span> </span><span> </span></p>\n",
      "<p><span>Regarding mindset, he said more of his team members are coming to him and saying, “I’ve never done this before.” He sees that as a good sign that offers an opportunity to talk about risk and alternative solutions. “When your team has the psychological safety to say that they don’t know something,” Lane sees it as positive. “The focus is always on what you have done and what you have delivered. Rarely is the focus on what you have not done before and what you want to grow into,” he said,</span><span> </span><span> </span></p>\n",
      "<p><span>Aronson has found it challenging to get AI projects off the ground. “It’s hard to tell management that you have a use case or problem to solve and want to go at it, and there is a 50-50 chance it will get done, and you don’t know how much it’s going to cost,” she said. “It comes down to articulating the rationale and convincing others it’s the right thing to do to move forward.”</span><span> </span><span> </span></p>\n",
      "<p><span>Rao said he talks to students about experimentation and having an experimental mindset. “AI tools can be easily accessible, but they can mask the challenges you can encounter. When you apply the vision API, for example in the context of challenges in your business or government agency, things may not be smooth,” he said.</span><span> </span><span> </span></p>\n",
      "<p><span>Moderator Dzombak asked the panelists how they build teams. Arson said, “You need a mix of people.” She has tried “communities of practice” around solving specific problems, where people can come and go. “You bring people together around a problem and not a tool,” she said.</span><span> </span><span> </span></p>\n",
      "<p><span>Lane seconded this. “I really have stopped focusing on tools in general,” he said. He ran experiments at JAIC in accounting, finance and other areas. “We found it’s not really about the tools. It’s about getting the right people together to understand the problems, then looking at the tools available,” he said.</span><span> </span><span> </span></p>\n",
      "<p><span>Lane said he sets up “cross-functional teams” that are “a little more formal than a community of interest.” He has found them to be effective for working together on a problem for maybe 45 days. He also likes working with customers of the needed services inside the organization, and has seen customers learn about data management and AI as a result. “We will pick up one or two along the way who become advocates for accelerating AI throughout the organization,” Lane said.</span><span> </span><span> </span></p>\n",
      "<p><span>Lane sees it taking five years to work out proven methods of thinking, working, and best practices for developing AI systems to serve the government. He mentioned </span><a href=\"https://opportunity.census.gov/\"><span>The Opportunity Project</span></a><span> (TOP) of the US Census Bureau, begun in 2016 to work on challenges such as ocean plastic pollution, COVID-19 economic recovery and disaster response. TOP has engaged in over 135 public-facing projects in that time, and has over 1,300 alumni including developers, designers, community leaders, data and policy experts, students and government agencies. </span><span> </span><span> </span></p>\n",
      "<p><span>“It’s based on a way of thinking and how to organize work,” Lane said. “We have to scale the model of delivery, but five years from now, we will have enough proof of concept to know what works and what does not.”</span><span> </span></p>\n",
      "<p><span>Learn more at </span><a href=\"https://www.aiworldgov.com/\"><span>AI World Government</span></a><span>, at the </span><a href=\"https://www.sei.cmu.edu/\"><span>Software Engineering Institute</span></a><span>, at </span><a href=\"https://www.dataxd.io/\"><span>DATA XD</span></a><span> and at </span><a href=\"https://opportunity.census.gov/\"><span>The Opportunity Project</span></a><span>.</span><span> </span></p>\n",
      "How Accountability Practices Are Pursued by AI Engineers in the Federal Government\n",
      "<img alt=\"\" class=\"webfeedsFeaturedVisual wp-post-image\" height=\"70\" src=\"https://www.aitrends.com/wp-content/uploads/2021/10/10-22GAOOffice-1-100x70.jpeg\" style=\"float: left; margin-right: 5px;\" width=\"100\" /><p><i><span>By John P. Desmond, AI Trends Editor </span></i><span> </span><span> </span></p>\n",
      "<p><span>Two experiences of how AI developers within the federal government are pursuing AI accountability practices were outlined at the </span><a href=\"https://www.aiworldgov.com/\"><span>AI World Government </span></a><span>event held virtually and in-person this week in Alexandria, Va.</span><span> </span></p>\n",
      "<figure class=\"wp-caption alignleft\" id=\"attachment_21132\" style=\"width: 200px;\"><img alt=\"\" class=\"size-full wp-image-21132\" height=\"280\" src=\"https://www.aitrends.com/wp-content/uploads/2021/10/10-22TakaAriga-2.jpeg\" width=\"200\" /><figcaption class=\"wp-caption-text\" id=\"caption-attachment-21132\">Taka Ariga, chief data scientist and director, US Government Accountability Office</figcaption></figure>\n",
      "<p><span>Taka Ariga, chief data scientist and director at the US </span><a href=\"https://www.gao.gov/\"><span>Government Accountability Office,</span></a><span> described an AI accountability framework he uses within his agency and plans to make available to others.</span><span> </span><span> </span></p>\n",
      "<p><span>And Bryce Goodman, chief strategist for AI and machine learning at the </span><a href=\"https://www.diu.mil/\"><span>Defense Innovation Unit </span></a><span>(DIU), a unit of the Department of Defense founded to help the US military make faster use of emerging commercial technologies, described work in his unit to apply principles of AI development to terminology that an engineer can apply.</span><span> </span><span> </span></p>\n",
      "<p><span>Ariga, the first chief data scientist appointed to the US Government Accountability Office and director of the GAO’s Innovation Lab, discussed an</span><a href=\"https://www.gao.gov/products/gao-21-519sp\"><span> AI Accountability Framework</span></a><span> he helped to develop by convening a forum of experts in the government, industry, nonprofits, as well as federal inspector general officials and AI experts. </span><span> </span><span> </span></p>\n",
      "<p><span>“We are adopting an auditor’s perspective on the AI accountability framework,” Ariga said. “GAO is in the business of verification.”</span><span> </span><span> </span></p>\n",
      "<p><span>The effort to produce a formal framework began in September 2020 and included 60% women, 40% of whom were underrepresented minorities, to discuss over two days. The effort was spurred by a desire to ground the AI accountability framework in the reality of an engineer’s day-to-day work. The resulting framework was first published in June as what Ariga described as “version 1.0.”</span><span> </span><span> </span></p>\n",
      "<p><b><span>Seeking to Bring a “High-Altitude Posture” Down to Earth</span></b><span> </span><span> </span></p>\n",
      "<p><span>“We found the AI accountability framework had a very high-altitude posture,” Ariga said. “These are laudable ideals and aspirations, but what do they mean to the day-to-day AI practitioner? There is a gap, while we see AI proliferating across the government.”</span><span> </span><span> </span></p>\n",
      "<p><span>“We landed on a lifecycle approach,” which steps through stages of design, development, deployment and continuous monitoring. The development effort stands on four “pillars” of Governance, Data, Monitoring and Performance. </span><span> </span></p>\n",
      "<p><span>Governance reviews what the organization has put in place to oversee the AI efforts. “The chief AI officer might be in place, but what does it mean? Can the person make changes? Is it multidisciplinary?”  At a system level within this pillar, the team will review individual AI models to see if they were “purposely deliberated.”</span><span> </span><span> </span></p>\n",
      "<p><span>For the Data pillar, his team will examine how the training data was evaluated, how representative it is, and is it functioning as intended.</span><span> </span><span> </span></p>\n",
      "<p><span>For the Performance pillar, the team will consider the “societal impact” the AI system will have in deployment, including whether it risks a violation of the Civil Rights Act. “Auditors have a long-standing track record of evaluating equity. We grounded the evaluation of AI to a proven system,” Ariga said. </span><span> </span><span> </span></p>\n",
      "<p><span>Emphasizing the importance of continuous monitoring, he said, “AI is not a technology you deploy and forget.” he said. “We are preparing to continually monitor for model drift and the fragility of algorithms, and we are scaling the AI appropriately.” The evaluations will determine whether the AI system continues to meet the need “or whether a sunset is more appropriate,” Ariga said.</span><span> </span><span> </span></p>\n",
      "<p><span>He is part of the discussion with NIST on an overall government AI accountability framework. “We don’t want an ecosystem of confusion,” Ariga said. “We want a whole-government approach. We feel that this is a useful first step in pushing high-level ideas down to an altitude meaningful to the practitioners of AI.”</span><span> </span><span> </span></p>\n",
      "<p><b><span>DIU Assesses Whether Proposed Projects Meet Ethical AI Guidelines</span></b><span> </span><span> </span></p>\n",
      "<figure class=\"wp-caption alignleft\" id=\"attachment_21134\" style=\"width: 200px;\"><img alt=\"\" class=\"size-full wp-image-21134\" height=\"299\" src=\"https://www.aitrends.com/wp-content/uploads/2021/10/10-22BryceGoodman-2.jpeg\" width=\"200\" /><figcaption class=\"wp-caption-text\" id=\"caption-attachment-21134\">Bryce Goodman, chief strategist for AI and machine learning, the Defense Innovation Unit</figcaption></figure>\n",
      "<p><span>At the DIU, Goodman is involved in a similar effort to develop guidelines for developers of AI projects within the government. </span><span> </span><span> </span></p>\n",
      "<p><span>Projects Goodman has been involved with implementation of AI for humanitarian assistance and disaster response, predictive maintenance, to counter-disinformation, and predictive health. He heads the Responsible AI Working Group. He is a faculty member of Singularity University, has a wide range of consulting clients from inside and outside the government, and holds a PhD in AI and Philosophy from the University of Oxford.</span><span> </span><span> </span></p>\n",
      "<p><span>The DOD in February 2020 adopted five areas of </span><a href=\"https://www.defense.gov/News/Releases/Release/Article/2091996/dod-adopts-ethical-principles-for-artificial-intelligence/\"><span>Ethical Principles for AI</span></a><span> after 15 months of consulting with AI experts in commercial industry, government academia and the American public.  These areas are: Responsible, Equitable, Traceable, Reliable and Governable. </span><span> </span><span> </span></p>\n",
      "<p><span>“Those are well-conceived, but it’s not obvious to an engineer how to translate them into a specific project requirement,” Good said in a presentation on Responsible AI Guidelines at the AI World Government event. “That’s the gap we are trying to fill.”</span><span> </span></p>\n",
      "<p><span>Before the DIU even considers a project, they run through the ethical principles to see if it passes muster. Not all projects do. “There needs to be an option to say the technology is not there or the problem is not compatible with AI,” he said. </span><span> </span><span> </span></p>\n",
      "<p><span>All project stakeholders, including from commercial vendors and within the government, need to be able to test and validate and go beyond minimum legal requirements to meet the principles. “The law is not moving as fast as AI, which is why these principles are important,” he said.</span><span> </span><span> </span></p>\n",
      "<p><span>Also, collaboration is going on across the government to ensure values are being preserved and maintained. “Our intention with these guidelines is not to try to achieve perfection, but to avoid catastrophic consequences,” Goodman said. “It can be difficult to get a group to agree on what the best outcome is, but it’s easier to get the group to agree on what the worst-case outcome is.” </span><span> </span></p>\n",
      "<p><span>The DIU guidelines along with case studies and supplemental materials will be published on the DIU website “soon,” Goodman said, to help others leverage the experience.</span><span> </span><span> </span></p>\n",
      "<p><b><span>Here are Questions DIU Asks Before Development Starts</span></b><span> </span><span> </span></p>\n",
      "<p><span>The first step in the guidelines is to define the task.  “That&#8217;s the single most important question,” he said. “Only if there is an advantage, should you use AI.”</span><span> </span></p>\n",
      "<p><span>Next is a benchmark, which needs to be set up front to know if the project has delivered. </span><span> </span><span> </span></p>\n",
      "<p><span>Next, he evaluates ownership of the candidate data. “Data is critical to the AI system and is the place where a lot of problems can exist.” Goodman said. “We need a certain contract on who owns the data. If ambiguous, this can lead to problems.”</span><span> </span><span> </span></p>\n",
      "<p><span>Next, Goodman’s team wants a sample of data to evaluate. Then, they need to know how and why the information was collected. “If consent was given for one purpose, we cannot use it for another purpose without re-obtaining consent,” he said.</span><span> </span><span> </span></p>\n",
      "<p><span>Next, the team asks if the responsible stakeholders are identified, such as pilots who could be affected if a component fails. </span><span> </span><span> </span></p>\n",
      "<p><span>Next, the responsible mission-holders must be identified. “We need a single individual for this,” Goodman said. “Often we have a tradeoff between the performance of an algorithm and its explainability. We might have to decide between the two. Those kinds of decisions have an ethical component and an operational component. So we need to have someone who is accountable for those decisions, which is consistent with the chain of command in the DOD.” </span><span> </span><span> </span></p>\n",
      "<p><span>Finally, the DIU team requires a process for rolling back if things go wrong. “We need to be cautious about abandoning the previous system,” he said. </span><span> </span><span> </span></p>\n",
      "<p><span>Once all these questions are answered in a satisfactory way, the team moves on to the development phase.</span><span> </span><span> </span></p>\n",
      "<p><span>In lessons learned, Goodman said, “Metrics are key. And simply measuring accuracy might not be adequate. We need to be able to measure success.”</span><span> </span></p>\n",
      "<p><span>Also, fit the technology to the task. “High risk applications require low-risk technology. And when potential harm is significant, we need to have high confidence in the technology,” he said. </span><span> </span></p>\n",
      "<p><span>Another lesson learned is to set expectations with commercial vendors. “We need vendors to be transparent,” he said. ”When someone says they have a proprietary algorithm they cannot tell us about, we are very wary. We view the relationship as a collaboration. It’s the only way we can ensure that the AI is developed responsibly.”</span><span> </span><span> </span></p>\n",
      "<p><span>Lastly, “AI is not magic. It will not solve everything. It should only be used when necessary and only when we can prove it will provide an advantage.”</span><span> </span><span> </span></p>\n",
      "<p><span>Learn more at </span><a href=\"https://www.aiworldgov.com/\"><span>AI World Government</span></a><span>, at the </span><a href=\"https://www.gao.gov/\"><span>Government Accountability Office, </span></a><span>at the</span><a href=\"https://www.gao.gov/products/gao-21-519sp\"><span> AI Accountability Framework</span></a><span> and at the </span><a href=\"https://www.diu.mil/\"><span>Defense Innovation Unit</span></a><span> site.</span><span> </span></p>\n",
      "Startup: AssemblyAI Represents New Generation Speech Recognition\n",
      "<img alt=\"\" class=\"webfeedsFeaturedVisual wp-post-image\" height=\"70\" src=\"https://www.aitrends.com/wp-content/uploads/2021/10/10-22SpeechRecognition-2-100x70.jpeg\" style=\"float: left; margin-right: 5px;\" width=\"100\" /><p><i><span>By AI Trends Staff</span></i><span> </span><span> </span></p>\n",
      "<p><span>Advances in the AI behind speech recognition are driving growth in the market, attracting venture capital and funding startups, posing challenges to established players.</span><span> </span><span> </span></p>\n",
      "<p><span>The growing acceptance and use of speech recognition devices are driving the market, which according to an estimate by Meticulous Research is expected to reach $26.8 billion globally by 2025, according to a recent account in </span><a href=\"https://www.analyticsinsight.net/voice-recognition-market-will-be-worth-26-79-billion-by-2025/\"><span>Analytics Insight</span></a><span>. Better speed and accuracy are among the benefits of the evolving technology.</span><span> </span></p>\n",
      "<figure class=\"wp-caption alignleft\" id=\"attachment_21128\" style=\"width: 200px;\"><img alt=\"\" class=\"size-full wp-image-21128\" height=\"256\" src=\"https://www.aitrends.com/wp-content/uploads/2021/10/10-22DylanFox-2.jpeg\" width=\"200\" /><figcaption class=\"wp-caption-text\" id=\"caption-attachment-21128\">Dylan Fox, CEO and Founder, AssemblyAI</figcaption></figure>\n",
      "<p><span>One company in the throes of this new growth, AssemblyAI of San Francisco, is offering an API for speech recognition capable of transcribing videos, podcasts, phone calls, and remote meetings. The company was founded by CEO Dylan Fox in 2017 and has received backing from Y Combinator, a startup accelerator, as well as NVIDIA. </span><span> </span><span> </span></p>\n",
      "<p><span>Fox has an unusual background for a high tech entrepreneur. He is a graduate of George Washington University with a degree in business administration, business economics, and public policy. He got a job as a software engineer for machine learning in the emerging product lab of Cisco in San Francisco, working on deep neural networks and machine learning. He got the idea for AssemblyAi and attracted capital from Y Combinator, which enabled him to hire data scientists and data engineers to get the technology off the ground. </span><span> </span><span> </span></p>\n",
      "<p><span>Asked in an interview with </span><i><span>AI Trends</span></i><span> how he made this transition from undergrad in business administration and economics to high-tech entrepreneur, Fox said, “I taught myself how to program, which led me to a path of machine learning. I was looking for a harder software challenge, which led to natural language processing, which took me to Cisco.” They were working on Siri for the Enterprise for Apple at the time,</span><span> </span></p>\n",
      "<p><span>To speed up the work, Cisco was looking to acquire speech recognition software; Fox was in the catbird’s seat for the search. “We looked at Nuance,” for example, acknowledged as a market leader and owner of more speech recognition software than its competitors. (The acquisition of Nuance by Microsoft for $19.6 billion is expected to be finalized by year-end.) The young, budding entrepreneur was not impressed. “It was crazy how bad all the options were from an accuracy and a developer point of view,” he stated.</span><span> </span><span> </span></p>\n",
      "<p><span>He was impressed by Twilio, a San Francisco-based company founded in 2008, which that year released the Twilio Voice API to make and receive phone calls hosted in the cloud. The company has since raised $103 million in venture capital. “They were setting new standards for a good API for developers,” Fox said.</span><span> </span><span> </span></p>\n",
      "<p><span>Fox’s idea was to use AI and machine learning to achieve “super accurate results, and make it easy for developers to incorporate the API into their products. One customer is CallRail, offering call tracking and marketing analytics software, which plans to incorporate AssembyAI’s API to gain insight into why people are calling. Other customers include NBC and the Wall Street Journal, using the product to transcribe content and interviews, and provide closed captioning. </span><span> </span></p>\n",
      "<p><span>“We’ve been working on building as close to human speech recognition quality as possible. It’s been a lot of work” Fox said. He expects to reach that plateau in 2022.</span><span> </span><span> </span></p>\n",
      "<p><span>He targets companies incorporating speech recognition into their products and makes it easy to buy. Customers pay on a usage basis; for every second of audio transcribed, AssemblyAI charges a fraction of a penny. Clients get billed monthly. If a customer uses 10 hours a month, it costs about nine dollars. If a customer uses a million hours a month, it costs about $900,000.  </span><span> </span><span> </span></p>\n",
      "<p><span>Voice recognition is a hot market. “Many new startups are being launched,” Fox said, providing opportunity. “Many interesting new businesses are being built on voice data.” </span><span> </span><span> </span></p>\n",
      "<p><span>AssemblyAI’s product can detect sensitive topics such as hate speech and profanity, so customers can save on human content moderation.</span><span> </span></p>\n",
      "<p><span>Asked to describe what differentiates his technology, Fox said, “We are an experienced team of deep learning researchers,” with experience from companies including BMW, Apple, and Facebook. “We build very large, very accurate deep learning models that have recognition results far more accurate than a traditional machine learning approach. We build really large models using advanced neural network technologies.” He compared the approach to what OpenAI uses to develop its GPT-3 large language model. </span><span> </span></p>\n",
      "<p><span>In addition, they build AI features on top of the transcriptions, to provide summaries of audio and video content, which can be searched and indexed. “It goes beyond just transcription,” Fox said. </span><span> </span><span> </span></p>\n",
      "<p><span>The company currently has 25 employees and expects to double in about four months. Business has been good. “There is an explosion of audio and video data online and customers want to be able to take advantage of it, so we see a lot of demand,” Fox said.</span><span> </span></p>\n",
      "<p><span>Learn more at </span><a href=\"https://www.assemblyai.com/\"><span>AssemblyAI.</span></a><span> </span></p>\n",
      "Pursuit of Autonomous Cars May Pose Risk of AI Tapping Forbidden Knowledge\n",
      "<img alt=\"\" class=\"webfeedsFeaturedVisual wp-post-image\" height=\"70\" src=\"https://www.aitrends.com/wp-content/uploads/2021/10/10-22ForbiddenKnowledge-2-100x70.jpeg\" style=\"float: left; margin-right: 5px;\" width=\"100\" /><p><i><span>By Lance Eliot, the AI Trends Insider</span></i><span> </span><span>  </span><span> </span></p>\n",
      "<p><span>Are there things that we must not know? </span><span> </span><span> </span></p>\n",
      "<p><span>This is an age-old question. Some assert that there is the potential for knowledge that ought to not be known. In other words, there are ideas, concepts, or mental formulations that should we become aware of that knowledge it could be our downfall. The discovery or invention of some new innovation or way of thinking could be unduly dangerous. It would be best to not go there, as it were, and avoid ever landing on such knowledge: forbidden knowledge.</span><span> </span><span> </span><span> </span></p>\n",
      "<p><span>The typical basis for wanting to forbid the discovery or emergence of forbidden knowledge is that the adverse consequences are overwhelming. The end result is so devastating and undercutting that the bad side outweighs the good that could be derived from the knowledge.</span><span> </span><span> </span><span> </span></p>\n",
      "<p><span>It is conceivable that there might be knowledge that is so bad that it has no good possibilities at all. Thus, rather than trying to balance or weigh the good versus the bad, the knowledge has no counterbalancing effects. It is just plain bad. </span><span> </span><span> </span><span> </span></p>\n",
      "<p><span>We are usually faced with the matter of knowledge that has both the good and the bad as to how it might be utilized or employed. This then leads to a dogged debate about whether the bad is so bad that it outweighs the good. On top of this, there is the unrealized bad and the unrealized good, which could be differentiated from the realized bad and the realized good (in essence, the knowledge might be said to be either good or bad, though this is purely conceptual and not put into real-world conditions to attest or become realized as such).</span><span> </span><span> </span><span> </span></p>\n",
      "<p><span>The most familiar reference to forbidden knowledge is likely evoked via the Garden of Eden and the essence of forbidden fruit.</span><span> </span></p>\n",
      "<p><span>A contemporary down-to-earth example often discussed about forbidden knowledge consists of the atomic bomb. Some suggest that the knowledge devised or invented to ultimately produce a nuclear bomb provides a quite visible and overt exemplar of the problems associated with knowledge. Had the knowledge about being able to attain an atomic bomb never been achieved, there presumably would not be any such device. In debates about the topic, it is feasible to take a resolute position favoring the attainment of an atomic bomb and there are equally counterbalancing contentions sternly disfavoring this attainment.</span><span> </span><span> </span><span> </span></p>\n",
      "<p><span>One perplexing problem about forbidden knowledge encompasses knowing beforehand the kind of knowledge that might end up in the forbidden category. This is a bit of a Catch-22 or circular type of puzzle. You might discover knowledge and then ascertain it ought to be forbidden, but the cat is kind of out of the bag due to the knowledge having been already uncovered or rendered. Oopsie, you should have in advance decided to not go there and therefore have avoided falling into the forbidden knowledge zone.</span><span> </span><span> </span><span> </span></p>\n",
      "<p><span>On a related twist, suppose that we could beforehand declare what type of knowledge is to be averted because it is predetermined as forbidden. Some people might accidentally discover the knowledge, doing so by happenstance, and now they’ve again potentially opened Pandora’s box. Meanwhile, there might be others that, regardless of being instructed to not derive any such stated forbidden knowledge, do so anyway.</span><span> </span><span> </span><span> </span></p>\n",
      "<p><span>This then takes us to a frequently used retort about forbidden knowledge, namely, if you don’t seek the forbidden knowledge there is a chance that someone else will, and you’ll be left in the dust because they got there first. In that preemptive viewpoint, the claim is that it is better to go ahead and forage for the forbidden knowledge and not get caught behind the eight-ball when someone else beats you to the punch.</span><span> </span><span> </span><span> </span></p>\n",
      "<p><span>Round and round we can go.</span><span> </span><span> </span><span> </span></p>\n",
      "<p><span>The main thing that most would agree to is that knowledge is power.</span><span> </span></p>\n",
      "<p><span>The alluded to power could be devastating and destroy others, possibly even leading to the self-destruction of the wielder of the knowledge. Yet there is also the potential for knowledge to be advantageous and save humanity from other ills.</span><span> </span><span> </span><span> </span></p>\n",
      "<p><span>Maybe we ought to say that knowledge is powerful. Despite that perhaps obvious proclamation, we might also add that knowledge can decay and gradually become outdated or less potent. Furthermore, since we are immersing ourselves herein into the cauldron of the love-it or hate-it knowledge conundrum, knowledge can be known and yet undervalued, perhaps only becoming valuable at a later time and in a different light.</span><span> </span><span> </span><span> </span></p>\n",
      "<p><span>There is a case to be made that humankind has a seemingly irresistible allure toward more and more knowledge. Some philosophers suggest you are unlikely to be able to bottle up or stop this quest for knowledge. If that’s the manner of how humanity will be, this implies that you must find ways to control or contain knowledge and give up on the belief that we can altogether avoid landing into forbidden knowledge.</span><span> </span><span> </span><span> </span></p>\n",
      "<p><span>There is a relatively new venue prompting a lot of anxious hand wringing pertaining to forbidden knowledge, namely the advent of Artificial Intelligence (AI).</span><span> </span><span> </span><span> </span></p>\n",
      "<p><span>Here’s the rub.</span><span> </span><span> </span><span> </span></p>\n",
      "<p><span>Suppose that we are able to craft AI systems that make use of knowledge about how humans can think. There are two major potential gotchas.</span><span> </span><span> </span><span> </span></p>\n",
      "<p><span>First, the AI systems themselves might end up doing good things, and they also might end up doing bad things. If the bad outweighs the good, maybe we are shooting our own foot by allowing AI to be put into use.</span><span> </span></p>\n",
      "<p><span>Secondly, perhaps this could be averted entirely by deciding that there is forbidden knowledge about how humans think, and we ought to not discover or reveal those mental mechanisms. It is the classic stepwise logic that step A axiomatically leads to step B. We won’t need to worry about AI systems (step B), if we never allow the achievement of step A (figuring out how humans think and then imparting that into computers), since the attainment of AI would presumably not arise.</span><span> </span><span> </span><span> </span></p>\n",
      "<p><span>In any case, there is inarguably a growing concern about AI.</span><span> </span><span> </span><span> </span></p>\n",
      "<p><span>Plenty of efforts are underway to promulgate a semblance of AI Ethics, meaning that those developers and indeed all stakeholders that are conceiving of, building, and putting into use an AI system needs to consider the ethical aspects of their efforts. AI systems have been unveiled and placed into use replete with all sorts of notable concerns, including incorporating unsavory biases and other problems.</span><span> </span><span> </span><span> </span></p>\n",
      "<p><span>All told, one bold and somewhat stark argument is that the pursuit of AI is being underpinned or stoked by the discovery and then exploitation of forbidden knowledge.</span><span> </span><span> </span><span> </span></p>\n",
      "<p><span>Be aware that many would scoff at this allegation.</span><span> </span><span> </span><span> </span></p>\n",
      "<p><span>There are those deeply immersed in the field of AI who would laugh that there is anything in the entirety of AI to date that constitutes potential forbidden knowledge. The technology and technological elements are relatively ho-hum, they would argue. You would be hard-pressed to pinpoint what AI-related knowledge that is already known comes anywhere near the ballpark of forbidden knowledge.</span><span> </span></p>\n",
      "<p><span>For those that concur with that posture, there is the reply that it might be future knowledge that we have not yet attained that is the upcoming forbidden kind, and for which we are heading pell-mell down that path. Thus, they would concede that we haven’t arrived at forbidden knowledge at this juncture, but this is an insidious distractor due to the aspect that it masks or belies our qualms entailing the possibility that it lays in wait at the next turn.</span><span> </span></p>\n",
      "<p><span>One area where AI is being actively used is to create Autonomous Vehicles (AVs).</span><span> </span></p>\n",
      "<p><span>We are gradually seeing the emergence of self-driving cars and can expect self-driving trucks, self-driving motorcycles, self-driving drones, self-driving planes, self-driving ships, self-driving submersibles, etc.</span><span> </span><span> </span><span> </span></p>\n",
      "<p><span>Today’s conventional cars are eventually going to give way to the advent of AI-based, true self-driving cars. Self-driving cars are driven via an AI driving system. There isn’t a need for a human driver at the wheel, and nor is there a provision for a human to drive the vehicle.</span><span> </span><span> </span><span> </span></p>\n",
      "<p><span>Here’s an intriguing question that has arisen: </span><i><span>Might the crafting of AI-based true self-driving cars take us into the realm of discovering forbidden knowledge, and if so, what should be done about this?</span></i><span> </span><span> </span><span> </span></p>\n",
      "<p><span>Before jumping into the details, I’d like to clarify what is meant when referring to true self-driving cars.</span><span> </span><span> </span><span> </span></p>\n",
      "<p><span>For my framework about AI autonomous cars, see the link here: </span><a href=\"https://aitrends.com/ai-insider/framework-ai-self-driving-driverless-cars-big-picture/\"><span>https://aitrends.com/ai-insider/framework-ai-self-driving-driverless-cars-big-picture/</span></a><span> </span><span> </span><span> </span></p>\n",
      "<p><span>Why this is a moonshot effort, see my explanation here: </span><a href=\"https://aitrends.com/ai-insider/self-driving-car-mother-ai-projects-moonshot/\"><span>https://aitrends.com/ai-insider/self-driving-car-mother-ai-projects-moonshot/</span></a><span> </span><span> </span><span> </span></p>\n",
      "<p><span>For more about the levels as a type of Richter scale, see my discussion here: </span><a href=\"https://aitrends.com/ai-insider/richter-scale-levels-self-driving-cars/\"><span>https://aitrends.com/ai-insider/richter-scale-levels-self-driving-cars/</span></a><span> </span><span> </span><span> </span></p>\n",
      "<p><span>For the argument about bifurcating the levels, see my explanation here: </span><a href=\"https://aitrends.com/ai-insider/reframing-ai-levels-for-self-driving-cars-bifurcation-of-autonomy/\"><span>https://aitrends.com/ai-insider/reframing-ai-levels-for-self-driving-cars-bifurcation-of-autonomy/</span></a><span> </span><span> </span><span> </span></p>\n",
      "<p><b><span>Understanding The Levels Of Self-Driving Cars</span></b><span> </span><span> </span><span> </span></p>\n",
      "<p><span>As a clarification, true self-driving cars are ones where the AI drives the car entirely on its own and there isn’t any human assistance during the driving task.</span><span> </span><span> </span><span> </span></p>\n",
      "<p><span>These driverless vehicles are considered Level 4 and Level 5, while a car that requires a human driver to co-share the driving effort is usually considered at Level 2 or Level 3. The cars that co-share the driving task are described as being semi-autonomous, and typically contain a variety of automated add-on’s that are referred to as ADAS (Advanced Driver-Assistance Systems).</span><span> </span></p>\n",
      "<p><span>There is not yet a true self-driving car at Level 5, which we don’t yet even know if this will be possible to achieve, and nor how long it will take to get there.</span><span> </span></p>\n",
      "<p><span>Meanwhile, the Level 4 efforts are gradually trying to get some traction by undergoing very narrow and selective public roadway trials, though there is controversy over whether this testing should be allowed per se (we are all life-or-death guinea pigs in an experiment taking place on our highways and byways, some contend).</span><span> </span><span> </span><span> </span></p>\n",
      "<p><span>Since semi-autonomous cars require a human driver, the adoption of those types of cars won’t be markedly different from driving conventional vehicles, so there’s not much new per se to cover about them on this topic (though, as you’ll see in a moment, the points next made are generally applicable).</span><span> </span><span> </span></p>\n",
      "<p><span>For semi-autonomous cars, it is important that the public needs to be forewarned about a disturbing aspect that’s been arising lately, namely that despite those human drivers that keep posting videos of themselves falling asleep at the wheel of a Level 2 or Level 3 car, we all need to avoid being misled into believing that the driver can take away their attention from the driving task while driving a semi-autonomous car.</span><span> </span><span> </span><span> </span></p>\n",
      "<p><span>You are the responsible party for the driving actions of the vehicle, regardless of how much automation might be tossed into a Level 2 or Level 3.</span><span> </span><span> </span><span> </span></p>\n",
      "<p><span>For why remote piloting or operating of self-driving cars is generally eschewed, see my explanation here: </span><a href=\"https://aitrends.com/ai-insider/remote-piloting-is-a-self-driving-car-crutch/\"><span>https://aitrends.com/ai-insider/remote-piloting-is-a-self-driving-car-crutch/</span></a><span> </span><span> </span><span> </span></p>\n",
      "<p><span>To be wary of fake news about self-driving cars, see my tips here: </span><a href=\"https://aitrends.com/ai-insider/ai-fake-news-about-self-driving-cars/\"><span>https://aitrends.com/ai-insider/ai-fake-news-about-self-driving-cars/</span></a><span> </span></p>\n",
      "<p><span>The ethical implications of AI driving systems are significant, see my indication here: </span><a href=\"http://aitrends.com/selfdrivingcars/ethically-ambiguous-self-driving-cars/\"><span>http://aitrends.com/selfdrivingcars/ethically-ambiguous-self-driving-cars/</span></a><span> </span><span> </span><span> </span></p>\n",
      "<p><span>Be aware of the pitfalls of normalization of deviance when it comes to self-driving cars, here’s my call to arms: </span><a href=\"https://aitrends.com/ai-insider/normalization-of-deviance-endangers-ai-self-driving-cars/\"><span>https://aitrends.com/ai-insider/normalization-of-deviance-endangers-ai-self-driving-cars/</span></a><span> </span><span> </span><span> </span></p>\n",
      "<p><b><span>Self-Driving Cars And Forbidden Knowledge</span></b><span> </span><span> </span><span> </span></p>\n",
      "<p><span>For Level 4 and Level 5 true self-driving vehicles, there won’t be a human driver involved in the driving task.</span><span> </span><span> </span><span> </span></p>\n",
      "<p><span>All occupants will be passengers.</span><span> </span><span> </span><span> </span></p>\n",
      "<p><span>The AI is doing the driving.</span><span> </span><span> </span><span> </span></p>\n",
      "<p><span>One aspect to immediately discuss entails the fact that the AI involved in today’s AI driving systems is not sentient. In other words, the AI is altogether a collective of computer-based programming and algorithms, and most assuredly not able to reason in the same manner that humans can.</span><span> </span><span> </span><span> </span></p>\n",
      "<p><span>Why this added emphasis about the AI not being sentient?</span><span> </span><span> </span><span> </span></p>\n",
      "<p><span>Because I want to underscore that when discussing the role of the AI driving system, I am not ascribing human qualities to the AI. Please be aware that there is an ongoing and dangerous tendency these days to anthropomorphize AI. In essence, people are assigning human-like sentience to today’s AI, despite the undeniable and inarguable fact that no such AI exists as yet.</span><span> </span></p>\n",
      "<p><span>With that clarification, you can envision that the AI driving system won’t natively somehow “know” about the facets of driving. Driving and all that it entails will need to be programmed as part of the hardware and software of the self-driving car.</span><span> </span><span> </span><span> </span></p>\n",
      "<p><span>Let’s dive into the myriad of aspects that come to play on this topic.</span><span> </span><span> </span><span> </span></p>\n",
      "<p><span>The crux here is whether there is forbidden knowledge lurking within the existing and ongoing efforts to achieve AI-based true self-driving cars. We’ll begin by considering the status of the existent efforts and then shift into speculation about the future of such efforts.</span><span> </span><span> </span><span> </span></p>\n",
      "<p><span>Per the earlier discussion about whether there is forbidden knowledge that has already perchance been revealed or discovered via the efforts toward today’s AI systems all told, the odds seem stacked against such a notion at this time, and likewise the same could be said about the pursuit of self-driving cars. Essentially, there doesn’t seem to be any forbidden knowledge per se that has been discovered or revealed during the self-driving cars development journey so far, at least with respect to the conventional wisdom about what forbidden knowledge might entail.</span><span> </span><span> </span><span> </span></p>\n",
      "<p><span>One could try to argue that it is premature to reach such a conclusion and that we might, later on, realize that forbidden knowledge was indeed uncovered or invented, and we just didn’t realize it. That is a rabbit hole that we’ll not go down for now, though you are welcome to keep that presumption at hand if so desired.</span><span> </span><span> </span><span> </span></p>\n",
      "<p><span>That covers the present, and ergo we can turn our attention to the future.</span><span> </span><span> </span><span> </span></p>\n",
      "<p><span>Generally, the efforts underway today have been primarily aimed at achieving Level 4, and the hope is that someday we will go beyond Level 4 and attain Level 5. To get to a robust Level 4, most would likely say that we can continue the existing approaches.</span><span> </span><span> </span><span> </span></p>\n",
      "<p><span>Not everyone would agree with that assumption. Some believe that we will get stymied within Level 4. Furthermore, the inability to produce a robust Level 4 will ostensibly preclude us from being able to attain Level 5. There is a contingent that suggests we need to start over and set aside the existing AI approaches, which otherwise are taking us down a dead-end or blind alley. An entirely new way of devising AI for autonomous vehicles is needed, they would vehemently argue.</span><span> </span><span> </span><span> </span></p>\n",
      "<p><span>There is also a contingent that asserts the Level 4 itself is a type of dead-end. In brief, those proponents would say that we will achieve a robust Level 4, though this will do little good towards attaining Level 5. Once again, their view is similar to the preceding remark that we will need to come up with some radically new understandings about AI and the nature of cognitive acumen in order to get self-driving cars into the Level 5 realm.</span><span> </span><span> </span><span> </span></p>\n",
      "<p><span>Aha, it is within that scope of having to dramatically revisit and revamp what AI is and how we can advance significantly in the pursuit of AI that the forbidden knowledge question can reside. In theory, perhaps the only means of attaining Level 5 will be to strike upon some knowledge that we do not yet know and that for which bodes for falling within the realm of forbidden knowledge.</span><span> </span></p>\n",
      "<p><span>To some, this seems farfetched.</span><span> </span></p>\n",
      "<p><span>They would emphatically ask; just what kind of knowledge are you even talking about?</span><span> </span><span> </span><span> </span></p>\n",
      "<p><span>Here’s their logic. Humans are able to drive cars. Humans do not seem to need or possess forbidden knowledge as it relates to the act of driving a car. Therefore, it seems ridiculous on the face of things to claim or contend that the only means to get AI-based true self-driving cars, for which they would be driven on an equal basis as human drivers can drive, would require the discovery or invention of whatever might be construed as forbidden knowledge.</span><span> </span><span> </span><span> </span></p>\n",
      "<p><span>Seems like pretty ironclad logic.</span><span> </span><span> </span><span> </span></p>\n",
      "<p><span>The retort is that humans have common-sense reasoning. With common-sense reasoning, we seem to know all sorts of things about the world around us. When we drive a car, we intrinsically make use of our common-sense reasoning. We take for granted that we do have a common-sense reasoning capacity, and similarly, we take for granted that it integrally comes to the fore when driving a car.</span><span> </span><span> </span><span> </span></p>\n",
      "<p><span>Attempts to create AI that can exhibit the equivalent of human common-sense reasoning have made ostensibly modest or some would say minimal progress (to clarify, those pursuing this line of inquiry are to be lauded, it’s just that no earth-shattering breakthroughs seem to have been reached and none seem on the immediate horizon). Yes, there are some quite fascinating and exciting efforts underway, but when you measure those against the everyday common-sense reasoning of humans, there is no comparison. They are night and day. If this were a contest, the humans win hands down, no doubt about it, and the AI experimental efforts encompassing common-sense reasoning are mere playthings in contrast.</span><span> </span><span> </span><span> </span></p>\n",
      "<p><span>You might have gleaned where this line of thought is headed.</span><span> </span><span> </span><span> </span></p>\n",
      "<p><span>The belief by some is that until we crack open the enigma of common-sense reasoning, there is little chance of achieving a Level 5, and perhaps also this will hold back the Level 4 too. It could be that a secret ingredient of sorts for autonomous vehicles is the need to figure out and include common-sense reasoning into AI-based driving and piloting systems.</span><span> </span><span> </span><span> </span></p>\n",
      "<p><span>If you buy into that logic, the added assertion is that maybe within the confines of how common-sense reasoning takes place is a semblance of forbidden knowledge. On the surface, you would certainly assume that if we knew entirely how common-sense reasoning works, there would not appear to be any cause for alarm or concern. The act of employing common-sense reasoning does not seem to necessarily embody forbidden knowledge.</span><span> </span><span> </span><span> </span></p>\n",
      "<p><span>The twist is that perhaps the underlying cognitive means that gives rise to the advent of common-sense reasoning is where there is forbidden knowledge. Some deep-rooted elements in the nature of human thought and how we form common sense and undertake common-sense reasoning are possibly a type of knowledge that will be shown as crucial and a forbidden knowledge formulation.</span><span> </span><span> </span><span> </span></p>\n",
      "<p><span>For more details about ODDs, see my indication at this link here: </span><a href=\"https://www.aitrends.com/ai-insider/amalgamating-of-operational-design-domains-odds-for-ai-self-driving-cars/\"><span>https://www.aitrends.com/ai-insider/amalgamating-of-operational-design-domains-odds-for-ai-self-driving-cars/</span></a><span> </span></p>\n",
      "<p><span>On the topic of off-road self-driving cars, here’s my details elicitation: </span><a href=\"https://www.aitrends.com/ai-insider/off-roading-as-a-challenging-use-case-for-ai-autonomous-cars/\"><span>https://www.aitrends.com/ai-insider/off-roading-as-a-challenging-use-case-for-ai-autonomous-cars/</span></a><span> </span></p>\n",
      "<p><span>I’ve urged that there must be a Chief Safety Officer at self-driving car makers, here’s the scoop: </span><a href=\"https://www.aitrends.com/ai-insider/chief-safety-officers-needed-in-ai-the-case-of-ai-self-driving-cars/\"><span>https://www.aitrends.com/ai-insider/chief-safety-officers-needed-in-ai-the-case-of-ai-self-driving-cars/</span></a><span> </span></p>\n",
      "<p><span>Expect that lawsuits are going to gradually become a significant part of the self-driving car industry, see my explanatory details here: </span><a href=\"http://aitrends.com/selfdrivingcars/self-driving-car-lawsuits-bonanza-ahead/\"><span>http://aitrends.com/selfdrivingcars/self-driving-car-lawsuits-bonanza-ahead/</span></a><span> </span><span> </span><span> </span></p>\n",
      "<p><b><span>Conclusion</span></b><span> </span><span> </span><span> </span></p>\n",
      "<p><span>Wow, that’s quite a bit of pondering, contemplation, and (some would say) wild thinking.</span><span> </span><span> </span><span> </span></p>\n",
      "<p><span>Maybe so, but it is a consideration that some would wish that we gave at least some credence toward and devoted attention to. There is the angst that we might find ourselves by happenstance stumbling into forbidden knowledge on these voracious self-driving cars quests.</span><span> </span><span> </span><span> </span></p>\n",
      "<p><span>For however you might emphasize that having AI-based true self-driving cars will be a potential blessing, proffering mobility-for-all and leading to reducing the number of car crash-related fatalities, there is a sneaking suspicion that it will not be all-good. The catch or trap could be that there is some kind of forbidden knowledge that will get brought to the eye and we will inevitably kick ourselves that we didn’t see it coming.</span><span> </span><span> </span><span> </span></p>\n",
      "<p><span>The next time you are munching on a delicious apple, give some thought to whether self-driving cars might be forbidden fruit.</span><span> </span><span> </span><span> </span></p>\n",
      "<p><span>We are on the path to taking a big bite, and we’ll have to see where that takes us.</span><span> </span></p>\n",
      "<p><i><span>Copyright 2021 Dr. Lance Eliot </span></i><span> </span></p>\n",
      "<p><a href=\"http://ai-selfdriving-cars.libsyn.com/website\"><i><span>http://ai-selfdriving-cars.libsyn.com/website</span></i></a><span> </span></p>\n"
     ]
    }
   ],
   "source": [
    "for e in feed.entries:\n",
    "    print(e.title)\n",
    "    print(e.content[0].value)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "5f2b4451",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove HTML tags from the content\n",
    "def clean_html(text):\n",
    "    if text == \"\":\n",
    "        return \"\"\n",
    "    else: \n",
    "        return BeautifulSoup(text, \"html.parser\").get_text() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "446b25ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'By Lance Eliot, the AI Trends Insider\\xa0\\xa0\\xa0\\xa0\\nAre there things that we must not know?\\xa0\\xa0\\xa0\\nThis is an age-old question.\\xa0Some assert that there is the potential for knowledge that ought to not be known. In other words, there are ideas, concepts, or mental formulations that should we become aware of that knowledge it could be our downfall. The discovery or invention of some new innovation or way of thinking could be unduly dangerous. It would be best to not go there, as it were, and avoid ever landing on such knowledge: forbidden knowledge.\\xa0\\xa0\\xa0\\nThe typical basis for wanting to forbid the discovery or emergence of forbidden knowledge is that the adverse consequences are overwhelming.\\xa0The end result is so devastating and undercutting that the bad side outweighs the good that could be derived from the knowledge.\\xa0\\xa0\\xa0\\nIt is conceivable that there might be knowledge that is so bad that it has no good possibilities at all. Thus, rather than trying to balance or weigh the good versus the bad, the knowledge has no counterbalancing effects. It is just plain bad.\\xa0\\xa0\\xa0\\xa0\\nWe are usually faced with the matter of knowledge that has both the good and the bad as to how it might be utilized or employed. This then leads to a dogged debate about whether the bad is so bad that it outweighs the good. On top of this, there is the unrealized bad and the unrealized good, which could be differentiated from the realized bad and the realized good (in essence, the knowledge might be said to be either good or bad, though this is purely conceptual and not put into real-world conditions to attest or become realized as such).\\xa0\\xa0\\xa0\\nThe most familiar reference to forbidden knowledge is likely evoked via the Garden of Eden and the essence of forbidden fruit.\\xa0\\nA contemporary down-to-earth example often discussed about forbidden knowledge consists of the atomic bomb. Some suggest that the knowledge devised or invented to ultimately produce a nuclear bomb provides a quite visible and overt\\xa0exemplar\\xa0of the problems associated with knowledge. Had the knowledge about being able to attain an atomic bomb never been achieved, there presumably would not be any such device. In debates about the topic, it is feasible to take a resolute position favoring the attainment of an atomic bomb and there are equally counterbalancing contentions sternly disfavoring this attainment.\\xa0\\xa0\\xa0\\nOne perplexing problem about forbidden knowledge encompasses knowing beforehand the kind of knowledge that might end up in the forbidden category. This is a bit of a Catch-22 or circular type of puzzle. You might discover knowledge and then ascertain it ought to be forbidden, but the cat is kind of out of the bag due to the knowledge having been already uncovered or rendered. Oopsie, you should have in advance decided to not go there and therefore have avoided falling into the forbidden knowledge zone.\\xa0\\xa0\\xa0\\nOn a related twist, suppose that we could beforehand declare what type of knowledge is to be averted because it is predetermined as forbidden. Some people might accidentally discover the knowledge, doing so by happenstance, and now they’ve again potentially opened Pandora’s box. Meanwhile, there might be others that, regardless of being instructed to not derive any such stated forbidden knowledge, do so anyway.\\xa0\\xa0\\xa0\\nThis then takes us to a frequently used retort about forbidden knowledge, namely, if you don’t seek the forbidden knowledge there is a chance that someone else will, and you’ll be left in the dust because they got there first. In that preemptive viewpoint, the claim is that it is better to go ahead and forage for the forbidden knowledge and not get caught behind the eight-ball when someone else beats you to the punch.\\xa0\\xa0\\xa0\\nRound and round we can go.\\xa0\\xa0\\xa0\\nThe main thing that most would agree to is that knowledge is power.\\xa0\\nThe alluded to power could be devastating and destroy others, possibly even leading to the self-destruction of the wielder of the knowledge. Yet there is also the potential for knowledge to be advantageous and save humanity from other ills.\\xa0\\xa0\\xa0\\nMaybe we ought to say that knowledge is powerful. Despite that perhaps obvious proclamation, we might also add that knowledge can decay and gradually become outdated or less potent. Furthermore, since we are immersing ourselves herein into the cauldron of the love-it or hate-it knowledge conundrum, knowledge can be known and yet undervalued, perhaps only becoming valuable at a later time and in a different light.\\xa0\\xa0\\xa0\\nThere is a case to be made that humankind has a seemingly irresistible allure toward more and more knowledge. Some philosophers suggest you are unlikely to be able to bottle up or stop this quest for knowledge. If that’s the manner of how humanity will be, this implies that you must find ways to control or contain knowledge and give up on the belief that we can altogether avoid landing into forbidden knowledge.\\xa0\\xa0\\xa0\\nThere is a relatively new venue prompting a lot of anxious hand wringing pertaining to forbidden knowledge, namely the advent of Artificial Intelligence (AI).\\xa0\\xa0\\xa0\\nHere’s the rub.\\xa0\\xa0\\xa0\\nSuppose that we are able to craft AI systems that make use of knowledge about how humans can think. There are two major potential gotchas.\\xa0\\xa0\\xa0\\nFirst, the AI systems themselves might end up doing good things, and they also might end up doing bad things. If the bad outweighs the good, maybe we are shooting our own foot by allowing AI to be put into use.\\xa0\\nSecondly, perhaps this could be averted entirely by deciding that there is forbidden knowledge about how humans think, and we ought to not discover or reveal those mental mechanisms. It is the classic stepwise logic that step A axiomatically leads to step B. We won’t need to worry about AI systems (step B), if we never allow the achievement of step A (figuring out how humans think and then imparting that into computers), since the attainment of AI would presumably not arise.\\xa0\\xa0\\xa0\\nIn any case, there is inarguably a growing concern about AI.\\xa0\\xa0\\xa0\\nPlenty of efforts are underway to promulgate a semblance of AI Ethics, meaning that those developers and indeed all stakeholders that are conceiving of, building, and putting into use an AI system needs to consider the ethical aspects of their efforts. AI systems have been unveiled and placed into use replete with all sorts of notable concerns, including incorporating unsavory biases and other problems.\\xa0\\xa0\\xa0\\nAll told, one bold and somewhat stark argument is that the pursuit of AI is being underpinned or stoked by the discovery and then exploitation of forbidden knowledge.\\xa0\\xa0\\xa0\\nBe aware that many would scoff at this allegation.\\xa0\\xa0\\xa0\\nThere are those deeply immersed in the field of AI who would laugh that there is anything in the entirety of AI to date that constitutes potential forbidden knowledge. The technology and technological elements are relatively ho-hum, they would argue. You would be hard-pressed to pinpoint what AI-related knowledge that is already known comes anywhere near the ballpark of forbidden knowledge.\\xa0\\nFor those that concur with that posture, there is the reply that it might be future knowledge that we have not yet attained that is the upcoming forbidden kind, and for which we are heading pell-mell down that path. Thus, they would concede that we haven’t arrived at forbidden knowledge at this juncture, but this is an insidious distractor due to the aspect that it masks or belies our qualms entailing the possibility that it lays in wait at the next turn.\\xa0\\nOne area where AI is being actively used is to create Autonomous Vehicles (AVs).\\xa0\\nWe are gradually seeing the emergence of self-driving cars and can expect self-driving trucks, self-driving motorcycles, self-driving drones, self-driving planes, self-driving ships, self-driving submersibles, etc.\\xa0\\xa0\\xa0\\nToday’s conventional cars are eventually going to give way to the advent of AI-based, true self-driving cars. Self-driving cars are driven via an AI driving system. There isn’t a need for a human driver at the wheel, and nor is there a provision for a human to drive the vehicle.\\xa0\\xa0\\xa0\\nHere’s an intriguing question that has arisen:\\xa0Might the crafting of AI-based true self-driving cars take us into the realm of discovering forbidden knowledge, and if so, what should be done about this?\\xa0\\xa0\\xa0\\nBefore jumping into the details, I’d like to clarify what is meant when referring to true self-driving cars.\\xa0\\xa0\\xa0\\nFor my framework about AI autonomous cars, see the link here:\\xa0https://aitrends.com/ai-insider/framework-ai-self-driving-driverless-cars-big-picture/\\xa0\\xa0\\xa0\\nWhy this is a moonshot effort, see my explanation here:\\xa0https://aitrends.com/ai-insider/self-driving-car-mother-ai-projects-moonshot/\\xa0\\xa0\\xa0\\nFor more about the levels as a type of Richter scale, see my discussion here:\\xa0https://aitrends.com/ai-insider/richter-scale-levels-self-driving-cars/\\xa0\\xa0\\xa0\\nFor the argument about bifurcating the levels, see my explanation here:\\xa0https://aitrends.com/ai-insider/reframing-ai-levels-for-self-driving-cars-bifurcation-of-autonomy/\\xa0\\xa0\\xa0\\nUnderstanding The Levels Of Self-Driving Cars\\xa0\\xa0\\xa0\\nAs a clarification, true self-driving cars are ones where the AI drives the car entirely on its own and there isn’t any human assistance during the driving task.\\xa0\\xa0\\xa0\\nThese driverless vehicles are considered Level 4 and Level 5, while a car that requires a human driver to co-share the driving effort is usually considered at Level 2 or Level 3. The cars that co-share the driving task are described as being semi-autonomous, and typically contain a variety of automated add-on’s that are referred to as ADAS (Advanced Driver-Assistance Systems).\\xa0\\nThere is not yet a true self-driving car at Level 5, which we don’t yet even know if this will be possible to achieve, and nor how long it will take to get there.\\xa0\\nMeanwhile, the Level 4 efforts are gradually trying to get some traction by undergoing very narrow and selective public roadway trials, though there is controversy over whether this testing should be allowed per se (we are all life-or-death guinea pigs in an experiment taking place on our highways and byways, some contend).\\xa0\\xa0\\xa0\\nSince semi-autonomous cars require a human driver, the adoption of those types of cars won’t be markedly different from driving conventional vehicles, so there’s not much new per se to cover about them on this topic (though, as you’ll see in a moment, the points next made are generally applicable).\\xa0\\xa0\\nFor semi-autonomous cars, it is important that the public needs to be forewarned about a disturbing aspect that’s been arising lately, namely that despite those human drivers that keep posting videos of themselves falling asleep at the wheel of a Level 2 or Level 3 car, we all need to avoid being misled into believing that the driver can take away their attention from the driving task while driving a semi-autonomous car.\\xa0\\xa0\\xa0\\nYou are the responsible party for the driving actions of the vehicle, regardless of how much automation might be tossed into a Level 2 or Level 3.\\xa0\\xa0\\xa0\\nFor why remote piloting or operating of self-driving cars is generally eschewed, see my explanation here:\\xa0https://aitrends.com/ai-insider/remote-piloting-is-a-self-driving-car-crutch/\\xa0\\xa0\\xa0\\nTo be wary of fake news about self-driving cars, see my tips here:\\xa0https://aitrends.com/ai-insider/ai-fake-news-about-self-driving-cars/\\xa0\\nThe ethical implications of AI driving systems are significant, see my indication here:\\xa0http://aitrends.com/selfdrivingcars/ethically-ambiguous-self-driving-cars/\\xa0\\xa0\\xa0\\nBe aware of the pitfalls of normalization of deviance when it comes to self-driving cars, here’s my call to arms:\\xa0https://aitrends.com/ai-insider/normalization-of-deviance-endangers-ai-self-driving-cars/\\xa0\\xa0\\xa0\\nSelf-Driving Cars And Forbidden Knowledge\\xa0\\xa0\\xa0\\nFor Level 4 and Level 5 true self-driving vehicles, there won’t be a human driver involved in the driving task.\\xa0\\xa0\\xa0\\nAll occupants will be passengers.\\xa0\\xa0\\xa0\\nThe AI is doing the driving.\\xa0\\xa0\\xa0\\nOne aspect to immediately discuss entails the fact that the AI involved in today’s AI driving systems is not sentient. In other words, the AI is altogether a collective of computer-based programming and algorithms, and most assuredly not able to reason in the same manner that humans can.\\xa0\\xa0\\xa0\\nWhy this added emphasis about the AI not being sentient?\\xa0\\xa0\\xa0\\nBecause I want to underscore that when discussing the role of the AI driving system, I am not ascribing human qualities to the AI. Please be aware that there is an ongoing and dangerous tendency these days to anthropomorphize AI. In essence, people are assigning human-like sentience to today’s AI, despite the undeniable and inarguable fact that no such AI exists as yet.\\xa0\\nWith that clarification, you can envision that the AI driving system won’t natively somehow “know” about the facets of driving. Driving and all that it entails will need to be programmed as part of the hardware and software of the self-driving car.\\xa0\\xa0\\xa0\\nLet’s dive into the myriad of aspects that come to play on this topic.\\xa0\\xa0\\xa0\\nThe crux here is whether there is forbidden knowledge lurking within the existing and ongoing efforts to achieve AI-based true self-driving cars. We’ll begin by considering the status of the existent efforts and then shift into speculation about the future of such efforts.\\xa0\\xa0\\xa0\\nPer the earlier discussion about whether there is forbidden knowledge that has already perchance been revealed or discovered via the efforts toward today’s AI systems all told, the odds seem stacked against such a notion at this time, and likewise the same could be said about the pursuit of self-driving cars. Essentially, there doesn’t seem to be any forbidden knowledge per se that has been discovered or revealed during the self-driving cars development journey so far, at least with respect to the conventional wisdom about what forbidden knowledge might entail.\\xa0\\xa0\\xa0\\nOne could try to argue that it is premature to reach such a conclusion and that we might, later on, realize that forbidden knowledge was indeed uncovered or invented, and we just didn’t realize it. That is a rabbit hole that we’ll not go down for now, though you are welcome to keep that presumption at hand if so desired.\\xa0\\xa0\\xa0\\nThat covers the present, and ergo we can turn our attention to the future.\\xa0\\xa0\\xa0\\nGenerally, the efforts underway today have been primarily aimed at achieving Level 4, and the hope is that someday we will go beyond Level 4 and attain Level 5. To get to a robust Level 4, most would likely say that we can continue the existing approaches.\\xa0\\xa0\\xa0\\nNot everyone would agree with that assumption. Some believe that we will get stymied within Level 4. Furthermore, the inability to produce a robust Level 4 will ostensibly preclude us from being able to attain Level 5. There is a contingent that suggests we need to start over and set aside the existing AI approaches, which otherwise are taking us down a dead-end or blind alley. An entirely new way of devising AI for autonomous vehicles is needed, they would vehemently argue.\\xa0\\xa0\\xa0\\nThere is also a contingent that asserts the Level 4 itself is a type of dead-end. In brief, those proponents would say that we will achieve a robust Level 4, though this will do little good towards attaining Level 5. Once again, their view is similar to the preceding remark that we will need to come up with some radically new understandings about AI and the nature of cognitive acumen in order to get self-driving cars into the Level 5 realm.\\xa0\\xa0\\xa0\\nAha, it is within that scope of having to dramatically revisit and revamp what AI is and how we can advance significantly in the pursuit of AI that the forbidden knowledge question can reside. In theory, perhaps the only means of attaining Level 5 will be to strike upon some knowledge that we do not yet know and that for which bodes for falling within the realm of forbidden knowledge.\\xa0\\nTo some, this seems\\xa0farfetched.\\xa0\\nThey would emphatically ask; just what kind of knowledge are you even talking about?\\xa0\\xa0\\xa0\\nHere’s their logic. Humans are able to drive cars. Humans do not seem to need or possess forbidden knowledge as it relates to the act of driving a car. Therefore, it seems ridiculous on the face of things to claim or contend that the only means to get AI-based true self-driving cars, for which they would be driven on an equal basis as human drivers can drive, would require the discovery or invention of whatever might be construed as forbidden knowledge.\\xa0\\xa0\\xa0\\nSeems like pretty ironclad logic.\\xa0\\xa0\\xa0\\nThe retort is that humans have common-sense reasoning. With common-sense reasoning, we seem to know all sorts of things about the world around us. When we drive a car, we intrinsically make use of our common-sense reasoning. We take for granted that we do have a common-sense reasoning capacity, and similarly, we take for granted that it integrally comes to the fore when driving a car.\\xa0\\xa0\\xa0\\nAttempts to create AI that can exhibit the equivalent of human common-sense reasoning have made ostensibly modest or some would say minimal progress (to clarify, those pursuing this line of inquiry are to be lauded, it’s just that no earth-shattering breakthroughs seem to have been reached and none seem on the immediate horizon). Yes, there are some quite fascinating and exciting efforts underway, but when you measure those against the everyday common-sense reasoning of humans, there is no comparison. They are night and day. If this were a contest, the humans win hands down, no doubt about it, and the AI experimental efforts encompassing common-sense reasoning are mere playthings in contrast.\\xa0\\xa0\\xa0\\nYou might have gleaned where this line of thought is headed.\\xa0\\xa0\\xa0\\nThe belief by some is that until we crack open the enigma of common-sense reasoning, there is little chance of achieving a Level 5, and perhaps also this will hold back the Level 4 too. It could be that a secret ingredient of sorts for autonomous vehicles is the need to figure out and include common-sense reasoning into AI-based driving and piloting systems.\\xa0\\xa0\\xa0\\nIf you buy into that logic, the added assertion is that maybe within the confines of how common-sense reasoning takes place is a semblance of forbidden knowledge. On the surface, you would certainly assume that if we knew entirely how common-sense reasoning works, there would not appear to be any cause for alarm or concern. The act of employing common-sense reasoning does not seem to necessarily embody forbidden knowledge.\\xa0\\xa0\\xa0\\nThe twist is that perhaps the underlying cognitive means that gives rise to the advent of common-sense reasoning is where there is forbidden knowledge. Some deep-rooted elements in the nature of human thought and how we form common sense and undertake common-sense reasoning are possibly a type of knowledge that will be shown as crucial and a forbidden knowledge formulation.\\xa0\\xa0\\xa0\\nFor more details about ODDs, see my indication at this link here:\\xa0https://www.aitrends.com/ai-insider/amalgamating-of-operational-design-domains-odds-for-ai-self-driving-cars/\\xa0\\nOn the topic of off-road self-driving cars, here’s my details elicitation:\\xa0https://www.aitrends.com/ai-insider/off-roading-as-a-challenging-use-case-for-ai-autonomous-cars/\\xa0\\nI’ve urged that there must be a Chief Safety Officer at self-driving car makers, here’s the scoop:\\xa0https://www.aitrends.com/ai-insider/chief-safety-officers-needed-in-ai-the-case-of-ai-self-driving-cars/\\xa0\\nExpect that lawsuits are going to gradually become a significant part of the self-driving car industry, see my explanatory details here:\\xa0http://aitrends.com/selfdrivingcars/self-driving-car-lawsuits-bonanza-ahead/\\xa0\\xa0\\xa0\\nConclusion\\xa0\\xa0\\xa0\\nWow, that’s quite a bit of pondering, contemplation, and (some would say) wild thinking.\\xa0\\xa0\\xa0\\nMaybe so, but it is a consideration that some would wish that we gave at least some credence toward and devoted attention to. There is the angst that we might find ourselves by happenstance stumbling into forbidden knowledge on these voracious self-driving cars quests.\\xa0\\xa0\\xa0\\nFor however you might emphasize that having AI-based true self-driving cars will be a potential blessing, proffering mobility-for-all and leading to reducing the number of car crash-related fatalities, there is a sneaking suspicion that it will not be all-good. The catch or trap could be that there is some kind of forbidden knowledge that will get brought to the eye and we will inevitably kick ourselves that we didn’t see it coming.\\xa0\\xa0\\xa0\\nThe next time you are munching on a delicious apple, give some thought to whether self-driving cars might be forbidden fruit.\\xa0\\xa0\\xa0\\nWe are on the path to taking a big bite, and we’ll have to see where that takes us.\\xa0\\nCopyright 2021 Dr. Lance Eliot\\xa0\\xa0\\nhttp://ai-selfdriving-cars.libsyn.com/website\\xa0'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_html(e.content[0].value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "bda2fa0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "articles = []\n",
    "for e in feed.entries:\n",
    "    articles.append({\"title\": e.title, \"content\": clean_html(e.content[0].value)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "02d615d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'title': 'Best Practices for Building the AI Development Platform in Government',\n",
       "  'content': 'By John P. Desmond, AI Trends Editor\\xa0\\nThe AI stack defined by Carnegie Mellon University is fundamental to the approach being taken by the US Army for its AI development platform efforts, according to Isaac Faber, Chief Data Scientist at the US Army AI Integration Center, speaking at the\\xa0AI World Government\\xa0event held in-person and virtually from Alexandria, Va., last week.\\xa0\\xa0\\nIsaac Faber, Chief Data Scientist, US Army AI Integration Center\\n“If we want to move the Army from legacy systems through digital modernization, one of the biggest issues I have found is the difficulty in abstracting away the differences in applications,” he said. “The most important part of digital transformation is the middle layer, the platform that makes it easier to be on the cloud or on a local computer.” The desire is to be able to move your software platform to another platform, with the same ease\\xa0with\\xa0which a new smartphone carries over the user’s contacts and histories.\\xa0\\xa0\\nEthics cuts across all layers of the AI application stack, which positions the planning stage at the top, followed by decision support, modeling, machine learning, massive data management and the device layer or platform at the bottom.\\xa0\\xa0\\n“I am advocating that we think of the stack as a core infrastructure and a way for applications to be deployed and not to be siloed in our approach,” he said. “We need to create a development environment for a globally-distributed workforce.”\\xa0\\xa0\\xa0\\nThe Army has been working on a Common Operating Environment Software (Coes) platform, first announced in 2017, a design for DOD work that is scalable, agile, modular, portable and open. “It is suitable for a broad range of AI projects,” Faber said. For executing the effort, “The devil is in the details,” he said.\\xa0\\xa0\\xa0\\nThe Army is working with CMU and private companies on a prototype platform, including with\\xa0Visimo\\xa0of Coraopolis, Pa., which offers AI development services. Faber said he prefers to collaborate and coordinate with private industry rather than buying products off the shelf. “The problem with that is, you are stuck with the value you are being provided by that one vendor, which is usually not designed for the challenges of DOD networks,” he said.\\xa0\\xa0\\nArmy Trains a Range of Tech Teams in AI\\xa0\\nThe Army engages in AI workforce development efforts for several teams, including:\\xa0 leadership, professionals with graduate degrees; technical staff, which is put through training to get certified; and AI users.\\xa0\\xa0\\xa0\\nTech teams in the Army have different areas of focus include: general purpose software development, operational data science, deployment which includes analytics, and a machine learning operations team, such as a large team required to build a computer vision system. “As folks come through the workforce, they need a place to collaborate, build and share,” Faber said.\\xa0\\xa0\\xa0\\nTypes of projects include diagnostic, which might be combining streams of historical data, predictive and prescriptive, which recommends a course of action based on a prediction. “At the far end is AI; you don’t start with that,” said Faber. The developer has to solve three problems: data engineering, the AI development platform, which he called “the green bubble,” and the deployment platform, which he called “the red bubble.”\\xa0\\xa0\\xa0\\n“These are mutually exclusive and all interconnected. Those teams of different people need to programmatically coordinate. Usually a good project team will have people from each of those bubble areas,” he said. “If you have not done this yet, do not try to solve the green bubble problem. It makes no sense to pursue AI until you have an operational need.”\\xa0\\xa0\\xa0\\nAsked by a participant which group is the most difficult to reach and train, Faber said without hesitation, “The hardest to reach are the executives. They need to learn what the value is to be provided by the AI ecosystem. The biggest challenge is how to communicate that value,” he said.\\xa0\\xa0\\xa0\\nPanel Discusses AI Use Cases with the Most Potential\\xa0\\xa0\\nIn a panel on Foundations of Emerging AI, moderator Curt Savoie, program director, Global Smart Cities Strategies for IDC, the market research firm, asked what emerging AI use case has the most potential.\\xa0\\xa0\\nJean-Charles Lede, autonomy tech advisor for the US Air Force, Office of Scientific Research, said,” I would point to decision advantages at the edge, supporting pilots and operators, and decisions at the back, for mission and resource planning.”\\xa0\\xa0\\xa0\\nKrista Kinnard, Chief of Emerging Technology for the Department of Labor\\nKrista Kinnard, Chief of Emerging Technology for the Department of Labor, said, “Natural language processing is an opportunity to open the doors to AI in the Department of Labor,” she said. “Ultimately, we are dealing with data on people, programs,\\xa0and organizations.”\\xa0\\xa0\\xa0\\xa0\\nSavoie asked what are the big risks and dangers the panelists see when implementing AI.\\xa0\\xa0\\xa0\\nAnil Chaudhry, Director of Federal AI Implementations for the General Services Administration (GSA), said in a typical IT organization using traditional software development, the impact of a decision by a developer only goes so far. With AI, “You have to consider the impact on a whole class of people, constituents,\\xa0and stakeholders. With a simple change in algorithms, you could be delaying benefits to millions of people or making incorrect inferences at scale. That’s the most important risk,” he said.\\xa0\\xa0\\nHe said he asks his contract partners to have “humans in the loop and humans on the loop.”\\xa0\\xa0\\xa0\\nKinnard seconded this, saying, “We have no intention of removing humans from the loop. It’s really about empowering people to make better decisions.”\\xa0\\xa0\\xa0\\nShe emphasized the importance of monitoring the AI models after they are deployed. “Models can drift as the data underlying the changes,” she said. “So you need a level of critical thinking to not only do the task, but to assess whether what the AI model is doing is acceptable.”\\xa0\\xa0\\xa0\\nShe added, “We have built out use cases and partnerships across the government to make sure we’re implementing responsible AI. We will never replace people with algorithms.”\\xa0\\xa0\\nLede of the Air Force said, “We often have use cases where the data does not exist. We cannot explore 50 years of war data, so we use simulation. The risk is in teaching an algorithm that you have a ‘simulation to real gap’ that is a real risk. You are not sure how the algorithms will map to the real world.”\\xa0\\xa0\\nChaudhry emphasized the importance of a testing strategy for AI systems. He warned of developers “who get enamored with a tool and forget the purpose of the exercise.” He recommended the development manager design in independent verification and validation strategy. “Your testing, that is where you have to focus your energy as a leader. The leader needs an idea in mind, before committing resources, on how they will justify whether the investment was a success.”\\xa0\\xa0\\xa0\\nLede of the Air Force talked about the importance of\\xa0explainability. “I am a technologist. I don’t\\xa0do laws. The ability for the AI function to explain in a way a human can interact with, is important. The AI is a partner that we have a dialogue with, instead of the AI coming up with a conclusion that we have no way of verifying,” he said.\\xa0\\xa0\\nLearn more at\\xa0AI World Government.\\xa0'},\n",
       " {'title': 'Advance Trustworthy AI and ML, and Identify Best Practices for Scaling AI',\n",
       "  'content': 'By John P. Desmond, AI Trends Editor\\xa0\\xa0\\nAdvancing trustworthy AI and machine learning to mitigate agency risk is a priority for the US Department of Energy (DOE), and identifying best practices for implementing AI at scale is a priority for the US General Services Administration (GSA).\\xa0\\xa0\\nThat’s what attendees learned in two sessions at the\\xa0AI World Government\\xa0live and virtual event held in Alexandria, Va. last week.\\xa0\\xa0\\xa0\\nPamela Isom, Director of the AI and Technology Office, DOE\\nPamela Isom, Director of the AI and Technology Office at the DOE, who spoke on Advancing Trustworthy AI and ML Techniques for Mitigating Agency Risks, has been involved in proliferating the use of AI across the agency for several years. With an emphasis on applied AI and data science, she oversees risk mitigation policies and standards and has been involved with applying AI to save lives, fight fraud, and strengthen the cybersecurity infrastructure.\\xa0\\xa0\\nShe emphasized the need for the AI project effort to be part of a strategic portfolio. “My office is there to drive a holistic view on AI and to mitigate risk by bringing us together to address challenges,” she said. The effort is assisted by the DOE’s AI and Technology Office, which is focused on transforming the DOE into a world-leading AI enterprise by accelerating research, development, delivery and the adoption of AI.\\xa0\\xa0\\n“I am telling my organization to be mindful of the fact that you can have tons and tons of data, but it might not be representative,” she said. Her team looks at examples from international partners, industry, academia and other agencies for outcomes “we can trust” from systems incorporating AI.\\xa0\\xa0\\n“We know that AI is disruptive, in trying to do what humans do and do it better,” she said. “It is beyond human capability; it goes beyond data in spreadsheets; it can tell me what I’m going to do next before I contemplate it myself. It’s that powerful,” she said.\\xa0\\xa0\\nAs a result, close attention must be paid to data sources. “AI is vital to the economy and our national security. We need precision; we need algorithms we can trust; we need accuracy. We don’t need biases,” Isom said, adding, “And don’t forget that you need to monitor the output of the models long after they have been deployed.”\\xa0\\xa0\\xa0\\nExecutive Orders Guide GSA AI Work\\xa0\\nExecutive Order 14028, a detailed set of actions to address the cybersecurity of government agencies, issued in May of this year, and Executive Order 13960, promoting the use of trustworthy AI in the Federal government, issued in December 2020, provide valuable guides to her work.\\xa0\\xa0\\xa0\\nTo help manage the risk of AI development and deployment, Isom has produced the AI Risk Management Playbook, which provides guidance around system features and mitigation techniques. It also has a filter for ethical and trustworthy principles which are considered throughout AI lifecycle stages and risk types. Plus, the playbook ties to relevant Executive Orders.\\xa0\\xa0\\nAnd it provides examples, such as your results came in at 80%\\xa0accuracy, but you wanted 90%. “Something is wrong there,” Isom said, adding, “The playbook helps you look at these types of problems and what you can do to mitigate risk, and what factors you should weigh as you design and build your project.”\\xa0\\xa0\\nWhile internal to DOE at present, the agency is looking into next steps for an external version. “We will share it with other federal agencies soon,” she said.\\xa0\\xa0\\xa0\\nGSA Best Practices for Scaling AI Projects Outlined\\xa0\\xa0\\nAnil Chaudhry, Director of Federal AI Implementations, AI Center of Excellence (CoE), GSA\\nAnil Chaudhry, Director of Federal AI Implementations for the AI Center of Excellence (CoE) of the GSA, who spoke on Best Practices for Implementing AI at Scale, has over 20 years of experience in technology delivery, operations and program management in the defense, intelligence and national security sectors.\\xa0\\xa0\\xa0\\nThe mission of the\\xa0CoE\\xa0is to accelerate technology modernization across the government, improve the public experience and increase operational efficiency. “Our business model is to partner with industry subject matter experts to solve problems,” Chaudhry said, adding, “We are not in the business of recreating industry solutions and duplicating them.”\\xa0\\xa0\\xa0\\nThe\\xa0CoE\\xa0is providing recommendations to partner agencies and working with them to implement AI systems as the federal government engages heavily in AI development. “For AI, the government landscape is vast. Every federal agency has some sort of AI project going on right now,” he said, and the maturity of AI experience varies widely across agencies.\\xa0\\xa0\\nTypical use cases he is seeing include having AI focus on increasing speed and efficiency, on cost savings and cost avoidance, on improved response time and increased quality and compliance. As one best practice, he recommended the agencies\\xa0vet their commercial experience\\xa0with the large datasets they will encounter in government.\\xa0\\xa0\\xa0\\n“We’re talking petabytes and exabytes here, of structured and unstructured data,” Chaudhry said.\\xa0[Ed. Note: A petabyte is 1,000 terabytes.]\\xa0“Also ask industry partners about their strategies and processes on how they do macro and micro trend analysis, and what their experience has been in the deployment of bots such as in Robotic Process Automation, and how they demonstrate sustainability as a result of drift of data.”\\xa0\\xa0\\xa0\\nHe also asks potential industry partners to\\xa0describe the AI talent on their team\\xa0or what talent they can access. If the company is weak on AI talent, Chaudhry would ask, “If you buy something, how will you know you got what you wanted when you have no way of evaluating it?”\\xa0\\xa0\\nHe added, “A best practice in implementing AI is defining how you train your workforce to leverage AI tools, techniques and practices, and to define how you grow and mature your workforce. Access to talent leads to either success or failure in AI projects, especially when it comes to scaling a pilot up to a fully deployed system.”\\xa0\\xa0\\nIn another best practice, Chaudhry recommended examining the industry partner’s\\xa0access to financial capital.\\xa0“AI is a field where the flow of capital is highly volatile. “You cannot predict or project that you will spend X amount of dollars this year to get where you want to be,” he said, because an AI development team may need to explore another hypothesis, or clean up some data that may not be transparent or is potentially biased. “If you don’t have access to funding, it is a risk your project will fail,” he said.\\xa0\\xa0\\nAnother best practice is\\xa0access to logistical capital, such as the data\\xa0 that sensors collect for an AI IoT system. “AI requires an enormous amount of data that is authoritative and timely. Direct access to that data is critical,” Chaudhry said. He recommended that data sharing agreements\\xa0 be in place with organizations relevant to the AI system. “You might not need it right away, but having access to the data, so you could immediately use it and to have thought through the privacy issues before you need the data, is a good practice for scaling AI programs,” he said.\\xa0\\xa0\\xa0\\nA final best practice is planning of\\xa0physical infrastructure,\\xa0such as data center space. “When you are in a pilot, you need to know how much capacity you need to reserve at your data center, and how many end points you need to manage” when the application scales up, Chaudhry said, adding, “This all ties back to access to capital and all the other best practices.“\\xa0\\nLearn more at\\xa0AI World Government.\\xa0'},\n",
       " {'title': 'Promise and Perils of Using AI for Hiring: Guard Against Data Bias',\n",
       "  'content': 'By AI Trends Staff\\xa0\\xa0\\nWhile\\xa0AI in hiring is now widely used for writing job descriptions, screening candidates, and automating interviews, it poses a risk of wide discrimination if not implemented carefully.\\xa0\\nKeith Sonderling, Commissioner, US Equal Opportunity Commission\\nThat was the message from Keith Sonderling, Commissioner with the US Equal Opportunity Commision, speaking at the\\xa0AI World Government\\xa0event held live and virtually in Alexandria, Va., last week. Sonderling is responsible for enforcing federal laws that prohibit discrimination against job applicants because of race, color, religion, sex, national origin, age or\\xa0disability.\\xa0\\xa0\\xa0\\n“The thought that AI would become mainstream in HR departments was closer to science fiction two year ago, but the pandemic has accelerated the rate at which AI is being used by employers,” he said. “Virtual recruiting is now here to stay.”\\xa0\\xa0\\nIt’s a busy time for HR professionals. “The great resignation is leading to the great rehiring, and AI will play a role in that like we have not seen before,” Sonderling said.\\xa0\\xa0\\nAI has been employed for years in hiring—“It did not happen overnight.”—for tasks including chatting with applications, predicting whether a candidate would take the job, projecting what type of employee they would be and mapping out upskilling and reskilling opportunities. “In short, AI is now making all the decisions once made by HR personnel,” which he did not characterize as good or bad.\\xa0\\xa0\\xa0\\n“Carefully designed and properly used, AI has the potential to make the workplace more fair,” Sonderling said. “But carelessly implemented, AI could discriminate on a scale we have never seen before by an HR professional.”\\xa0\\xa0\\nTraining Datasets for AI Models Used for Hiring Need to Reflect Diversity\\xa0\\xa0\\nThis is because AI models rely on training data. If the company’s current workforce is used as the basis for training, “It will replicate the status quo. If it’s one gender or one race primarily, it will replicate that,” he said. Conversely, AI can help mitigate risks of hiring bias by race, ethnic background, or disability status. “I want to see AI improve on workplace discrimination,” he said.\\xa0\\xa0\\nAmazon began building a hiring application in 2014, and found over time that it discriminated against women in its recommendations, because the AI model was trained on a dataset of the company’s own hiring record for the previous 10 years, which was primarily of males. Amazon developers tried to correct it but ultimately scrapped the system in 2017.\\xa0\\xa0\\xa0\\nFacebook has recently agreed to pay $14.25 million to settle civil claims by the US government that the social media company discriminated against American workers and violated federal recruitment rules, according to an account from\\xa0Reuters. The case centered on Facebook’s use of what it called its PERM program for labor certification. The government found that Facebook refused to hire American workers for jobs that had been reserved for temporary visa holders under the PERM program.\\xa0\\xa0\\xa0\\n“Excluding people from the hiring pool is a violation,” Sonderling said.\\xa0 If the AI program “withholds the existence of the job opportunity to that\\xa0class, so\\xa0they cannot exercise their rights, or if it downgrades a protected class, it is within our domain,” he said.\\xa0\\xa0\\xa0\\nEmployment assessments, which became more common after World War II, have provided\\xa0 high value to HR managers and with help from AI they have the potential to minimize bias in hiring. “At the same time, they are vulnerable to claims of discrimination, so\\xa0employers\\xa0need to be careful and cannot take a hands-off approach,” Sonderling said. “Inaccurate data will amplify bias in decision-making. Employers must be vigilant against discriminatory outcomes.”\\xa0\\xa0\\nHe recommended researching solutions from vendors who vet data for risks of bias on the basis of race, sex, and other factors.\\xa0\\xa0\\xa0\\nOne example is from\\xa0HireVue\\xa0of South Jordan, Utah, which has\\xa0built a\\xa0hiring platform predicated on the US Equal Opportunity Commission’s Uniform Guidelines, designed specifically to mitigate unfair hiring practices, according to an account from\\xa0allWork.\\xa0\\xa0\\xa0\\nA post on AI ethical principles on its website states in part, “Because\\xa0HireVue\\xa0uses AI technology in our products, we actively work to prevent the introduction or\\xa0propagation\\xa0of bias against any group or individual. We will continue to carefully review the datasets we use in our work and ensure that they are as accurate and diverse as possible. We also continue to advance our abilities to monitor, detect, and mitigate bias. We strive to build teams from diverse backgrounds with diverse knowledge, experiences, and perspectives to best represent the people our systems serve.”\\xa0\\xa0\\nAlso, “Our data scientists and IO psychologists build\\xa0HireVue\\xa0Assessment algorithms in a way that removes data from consideration by the algorithm that contributes to adverse impact without significantly impacting the assessment’s predictive accuracy. The result is a highly valid, bias-mitigated assessment that helps to enhance human decision making while actively promoting diversity and equal opportunity regardless of gender, ethnicity, age, or disability status.”\\xa0\\xa0\\nDr. Ed\\xa0Ikeguchi, CEO,\\xa0AiCure\\nThe issue of bias in datasets used to train AI models is not confined to hiring. Dr. Ed\\xa0Ikeguchi, CEO of\\xa0AiCure, an AI analytics company working in the life sciences industry, stated in a recent account in\\xa0HealthcareITNews, “AI is only as strong as the data it’s fed, and lately that data backbone’s credibility is being increasingly called into question. Today’s AI developers lack access to large, diverse data sets on which to train and validate new tools.”\\xa0\\xa0\\nHe added, “They often need to leverage open-source datasets, but many of these were trained using computer programmer volunteers, which is a predominantly white population. Because algorithms are often trained on single-origin data samples with limited diversity, when applied in real-world scenarios to a broader population of different races, genders, ages, and more, tech that appeared highly accurate in research may prove unreliable.”\\xa0\\nAlso, “There needs to be an element of governance and peer review for all algorithms, as even the most solid and tested algorithm is bound to have unexpected results arise. An algorithm is never done learning—it must be constantly developed and fed more data to improve.”\\xa0\\nAnd, “As an industry, we need to become more skeptical of AI’s conclusions and encourage transparency in the industry. Companies should readily answer basic questions, such as ‘How was the algorithm trained? On what basis did it draw this conclusion?”\\xa0\\nRead the source articles and information at\\xa0AI World Government, from\\xa0Reuters\\xa0and from\\xa0HealthcareITNews.\\xa0'},\n",
       " {'title': 'Predictive Maintenance Proving Out as Successful AI Use Case',\n",
       "  'content': 'By John P. Desmond, AI Trends Editor\\xa0\\xa0\\nMore companies are successfully exploiting predictive maintenance systems that combine AI and IoT sensors to collect data that anticipates breakdowns and recommends preventive action before break or machines fail, in a demonstration of an AI use case with proven value.\\xa0\\xa0\\nThis growth is reflected in optimistic market forecasts. The predictive maintenance market is sized at $6.9 billion today and is projected to grow to $28.2 billion by 2026, according to a report from\\xa0IoT Analytics\\xa0of Hamburg, Germany. The firm\\xa0counts over 280 vendors offering solutions in the market today, projected to grow to over 500 by 2026.\\xa0\\xa0\\nFernando Bruegge, Analyst, IoT Analytics, Hamburg, Germany\\n“This research is a wake-up call to those that claim IoT is failing,” stated analyst Fernando Bruegge, author of the report, adding, “For companies that own industrial assets or sell equipment, now is the time to invest in predictive maintenance-type solutions.” And, “Enterprise technology firms need to prepare to integrate predictive maintenance solutions into their offerings,” Bruegge suggested.\\xa0\\xa0\\nHere is a review of some specific experience with predictive maintenance systems that combine AI and IoT sensors.\\xa0\\nAircraft engine manufacturer\\xa0Rolls-Royce\\xa0is\\xa0deploying predictive analytics\\xa0to help reduce the amount of carbon its engines produce, while also optimizing maintenance to help customers keep planes in the air longer, according to a recent account in\\xa0CIO.\\xa0\\nRolls-Royce built an Intelligent Engine platform to monitor engine flight, gathering data on weather conditions and how pilots are flying. Machine learning is applied to the data to customize maintenance regimes for individual engines.\\xa0\\nStuart Hughes, chief information and digital officer, Rolls-Royce\\n“We’re tailoring our maintenance regimes to make sure that we’re optimizing for the life an engine has, not the life the manual says it should have,” stated Stuart Hughes, chief information and digital officer at Rolls-Royce. “It’s truly variable service, looking at each engine as an individual engine.”\\xa0\\nCustomers are seeing less service interruption. “Rolls-Royce has been monitoring engines and charging per hour for at least 20 years,” Hughes stated. “That part of the business isn’t new. But as we’ve evolved, we’ve begun to treat the engine as a singular engine. It’s much more about the personalization of that engine.”\\xa0\\xa0\\nPredictive analytics is being applied in healthcare as well as in the manufacturing industry. Kaiser Permanente, the integrated managed care consortium based in Oakland, Calif. Is using predictive analytics to identify non-intensive care unit (ICU) patients at risk of rapid deterioration.\\xa0\\xa0\\xa0\\nWhile non-ICU patients that require unexpected transfers to the ICU constitute less than\\xa04%\\xa0of the total hospital population, they account for 20%\\xa0of all hospital deaths, according to Dr. Gabriel Escobar, research scientist, Division of Research, and regional director, Hospital Operations Research, Kaiser Permanente Northern California.\\xa0\\nKaiser Permanente Practicing Predictive Maintenance in Healthcare\\xa0\\nKaiser Permanente developed the Advanced Alert Monitor (AAM) system, leveraging three predictive analytic models to analyze more than 70 factors in a given patient’s electronic health record to generate a composite risk score.\\xa0\\n“The AAM system synthesizes and analyzes vital statistics, lab results, and other variables to generate hourly deterioration risk scores for adult hospital patients in the medical-surgical and transitional care units,” stated Dick Daniels, executive vice president and CIO of Kaiser Permanente in the CIO account. “Remote hospital teams evaluate the risk scores every hour and notify rapid response teams in the hospital when potential deterioration is detected. The rapid response team conducts bedside evaluation of the patient and calibrates the course treatment with the hospitalist.”\\xa0\\nIn advice to other practitioners, Daniels recommended a focus on how the tool will be fit into the workflow of health care teams. “It took us about five years to perform the initial mapping of the electronic medical record backend and develop the predictive models,” Daniels stated. “It then took us another two to three years to transition these models into a live web services application that could be used operationally.”\\xa0\\nIn an example from the food industry, a PepsiCo Frito-Lay plant in Fayetteville, Tenn. is using predictive maintenance successfully, with year-to-date equipment downtime at 0.75%\\xa0and unplanned downtime at 2.88%, according to Carlos Calloway, the site’s reliability engineering manager, in an account in\\xa0PlantServices.\\xa0\\nExamples of monitoring include: vibration readings confirmed by ultrasound helped to prevent a PC combustion blower motor from failing and shutting down the whole potato chip department; infrared analysis of the main pole for the plant’s GES automated warehouse detected a hot fuse holder, which helped to avoid a shutdown of the entire warehouse; and increased acid levels were detected in oil samples from a baked extruder gearbox, indicating oil degradation, which enabled prevention of a shutdown of Cheetos Puffs production.\\xa0\\nThe Frito-Lay plant produces more than 150 million pounds of product per year, including Lays, Ruffles, Cheetos, Doritos, Fritos, and Tostitos.\\xa0\\xa0\\nThe types of monitoring include vibration analysis, used on mechanical applications, which is processed with the help of a third-party company which sends alerts to the plant for investigation and resolution.\\xa0Another service partner performs quarterly vibration monitoring on selected equipment. All motor control center rooms and electrical panels are monitored with quarterly infrared analysis, which is also used on electrical equipment, some rotating equipment, and heat exchangers. In addition, the plant has done ultrasonic monitoring for more than 15 years, and it is “kind of like the pride and joy of our site from a predictive standpoint,” stated Calloway.\\xa0\\xa0\\nThe plan has a number of products in place from UE Systems of Elmsford, NY, supplier of ultrasonic instruments, hardware and software, and training for predictive maintenance.\\xa0\\xa0\\xa0\\nLouisiana Alumina Plant Automating Bearing Maintenance\\xa0\\xa0\\xa0\\nBearings, which wear over time under varying conditions of weather and temperature in the case of automobiles, are a leading candidate for IoT monitoring and predictive maintenance with AI. The\\xa0Noranda Alumina\\xa0plant in Gramercy, La. is finding a big payoff from its investment in a system to improve the lubrication of bearings in its production equipment.\\xa0\\xa0\\nThe system has resulted in a 60%\\xa0decline in bearing changes in the second year of using the new lubrication system, translating to some $900,000 in savings on bearings that did not need to be replaced and avoided downtime.\\xa0\\xa0\\n“Four hours of downtime is about $1 million dollars’ worth of lost production,” stated Russell Goodwin, a reliability engineer and millwright instructor at Noranda Alumina, in the\\xa0PlantServices\\xa0account, which was based on presentations at the Leading Reliability 2021 event.\\xa0\\nThe Noranda Alumina plant is the only alumina plant operating in the US.\\xa0“If we shut down, you’ll need to import it,” stated Goodwin. The plant experiences pervasive dust, dirt, and caustic substances, which complicate efforts at improved reliability and maintenance practices.\\xa0\\xa0\\nNoranda Alumina tracks all motors and gearboxes at 1,500 rpm and higher with vibration readings, and most below 1,500 with ultrasound. Ultrasonic monitoring, of sound in ranges beyond human hearing, was introduced to the plant after Goodwin joined the company in 2019. At the time, grease monitoring had room for improvement. “If grease was not visibly coming out of the seal, the mechanical supervisor did not count the round as complete,” stated Goodwin.\\xa0\\xa0\\nAfter introducing automation, the greasing system has improved dramatically, he stated. The system was also able to detect bearings in a belt whose bearings were wearing out too quickly due to contamination. “Tool-enabled tracking helped to prove that it wasn’t improper greasing, but rather the bearing was made improperly,” stated Goodwin.\\xa0\\xa0\\nRead the source articles and information in\\xa0\\xa0IoT Analytics,\\xa0in\\xa0CIO\\xa0and in\\xa0PlantServices.\\xa0'},\n",
       " {'title': 'Novelty In The Game Of Go Provides Bright Insights For AI And Autonomous Vehicles',\n",
       "  'content': 'By Lance Eliot, the AI Trends Insider\\xa0\\xa0\\nWe already\\xa0expect that humans to exhibit flashes of brilliance.\\xa0It might not happen all the time, but the act itself is welcomed and not altogether disturbing when\\xa0it occurs.\\xa0\\xa0\\xa0\\nWhat about when Artificial Intelligence (AI) seems to display an act of novelty?\\xa0Any such instance is bound to get our attention; questions arise right away.\\xa0\\xa0\\xa0\\nHow did the AI come up with the apparent out-of-the-blue insight or novel indication? Was it a mistake, or did it fit within the parameters of what the AI was expected to produce? There is also the immediate consideration of whether the AI somehow is slipping toward the precipice of becoming sentient.\\xa0\\xa0\\xa0\\nPlease be aware that no AI system in existence is anywhere close to reaching sentience, despite the\\xa0claims and falsehoods tossed around in the media. As such, if today’s AI seems to do something that appears to be a novel act, you should not leap to the conclusion that this is a sign of human insight within technology or the emergence of human ingenuity among AI.\\xa0\\xa0\\xa0\\nThat’s an anthropomorphic bridge too far.\\xa0\\xa0\\xa0\\nThe reality is that any such AI “insightful” novelties are based on various concrete computational algorithms and tangible data-based pattern matching.\\xa0\\xa0\\xa0\\nIn today’s column, we’ll be taking a close look at an example of an AI-powered novel act, illustrated via the game of Go, and relate these facets to the advent of AI-based true self-driving cars as a means of understanding the AI-versus-human related ramifications.\\xa0\\nRealize that the capacity to spot or suggest a novelty is being done methodically by an AI system, while, in contrast, no one can say for sure how humans can devise novel thoughts or intuitions.\\xa0\\nPerhaps we too are bound by some internal mechanistic-like facets, or maybe there is something else going on. Someday, hopefully, we will crack open the secret inner workings of the mind and finally know how we think. I suppose it might undercut the mystery and magical aura that oftentimes goes along with those of us that have moments of outside-the-box visions, though I’d trade that enigma to know how the cups-and-balls trickery truly functions (going behind the curtain, as it were).\\xa0\\xa0\\xa0\\nSpeaking of novelty, a famous game match involving the playing of Go can provide useful illumination on this overall topic.\\xa0\\xa0\\xa0\\nGo is a popular board game\\xa0in the same complexity category as chess. Arguments are made about which is tougher, chess or Go, but I’m not going to get mired into that morass. For the sake of civil discussion, the key point is that Go is highly complex and requires intense mental concentration especially\\xa0at the\\xa0tournament level.\\xa0\\xa0\\xa0\\nGenerally, Go consists of trying to capture territory on a standard Go board, consisting of a 19 by 19 grid of intersecting lines. For those of you that have never tried playing Go, the closest similar kind of game might be the connect-the-dots that you played in childhood, which involves grabbing up territory, though Go is magnitudes more involved.\\xa0\\xa0\\xa0\\xa0\\nThere is no need for you to know anything in particular about Go to get the gist of what will be discussed next regarding the act of human novelty and the act of AI novelty.\\xa0\\xa0\\xa0\\nA famous Go competition took place about four years ago that pitted one of the world’s top professional Go players, Lee Sedol, against an AI program that had been crafted to play Go, coined as AlphaGo. There is a riveting documentary about the contest and plenty of write-ups and online videos that have in detail covered the match, including post-game analysis.\\xa0\\xa0\\xa0\\nPut yourself back in time to 2016 and relive what happened.\\xa0\\nMost AI developers did not anticipate that the AI of that time would be proficient enough to beat a top Go player. Sure, AI had already been able to best some top chess players, and thus offered a glimmer of expectation that Go would eventually be equally undertaken, but there weren’t any Go programs that had been able to compete at the pinnacle levels of human Go players. Most expected that it would probably be around the year 2020 or so before the capabilities of AI would be sufficient to compete in world-class Go tournaments.\\xa0\\xa0\\nDeepMind Created AlphaGo Using Deep Learning, Machine Learning\\xa0\\xa0\\xa0\\nA small-sized tech company named DeepMind Technologies devised the AlphaGo AI playing system (the firm was later acquired by Google). Using techniques from Machine Learning and Deep Learning, the AlphaGo program was being revamped and adjusted right up to the actual tournament, a typical kind of last-ditch developer contortions that many of us have done when trying to get the last bit of added edge into something that is about to be demonstrated.\\xa0\\xa0\\xa0\\nThis was a monumental competition that had garnered global interest.\\xa0\\xa0\\xa0\\nHuman players of Go were doubtful that the AlphaGo program would win. Many AI techies were doubtful that AlphaGo would win. Even the AlphaGo developers were unsure of how well the program would do, including the stay-awake-at-night fears that the AlphaGo program would hit a bug or go into a kind of delusional mode and make outright mistakes and play foolishly.\\xa0\\xa0\\xa0\\nA million dollars in prize money was put into the pot for the competition. There would be five Go games played, one per day, along with associated rules about taking breaks, etc. Some predicted that Sedol would handily win all five games, doing so without cracking a sweat. AI pundits were clinging to the hope that AlphaGo would win at least one of the five games, and otherwise, present itself as a respectable level of Go player throughout the contest.\\xa0\\nIn the first match, AlphaGo won.\\xa0\\xa0\\xa0\\nThis was pretty much a worldwide shocker. Sedol was taken aback. Lots of Go players were surprised that a computer program could compete and beat someone at Sedol’s level of play. Everyone began to give some street cred to the AlphaGo program and the efforts by the AI developers.\\xa0\\xa0\\xa0\\nTension grew for the next match.\\xa0\\xa0\\xa0\\nFor the second game, it was anticipated that Sedol might significantly change his approach to the contest. Perhaps he had been overconfident coming into the competition, some harshly asserted, and the loss of the first game would awaken him to the importance of putting all his concentration into the tournament. Or, possibly he had played as though he was competing with a lesser capable player and thus was not pulling out all the stops to try and win the match.\\xa0\\xa0\\xa0\\nWhat happened in the second game?\\xa0\\nTurns out that AlphaGo prevailed, again, and also did something that was seemingly remarkable for those that avidly play Go. On the 37th\\xa0move of the match, the AlphaGo program opted to make placement onto the Go board in a spot that nobody especially anticipated. It was a surprise move, coming partway through a match that otherwise was relatively conventional in the nature of the moves being made by both Sedol and AlphaGo.\\xa0\\xa0\\xa0\\nAt the time, in real-time, rampant speculation was that the move was an utter gaffe on the part of the AlphaGo program.\\xa0\\xa0\\xa0\\nInstead, it became famous as a novel move, known now as “Move 37” and heralded in Go and used colloquially overall to suggest any instance when AI does something of a novel or unexpected manner.\\xa0\\xa0\\xa0\\nIn the third match, AlphaGo won again, now having successfully beaten Sedol in a 3-out-of-5 winner competition. They\\xa0continued though to\\xa0play a fourth and a fifth game.\\xa0\\xa0\\xa0\\nDuring the fourth game, things were tight as usual and the match play was going head-to-head (well, head versus AI). Put yourself into the shoes of Sedol. In one sense, he wasn’t just a Go player, he was somehow representing all of humanity (an unfair and misguided viewpoint, but pervasive anyway), and the pressure was on him to win at least one game. Just even one game would be something to hang your hat on, and bolster faith in mankind (again, a nonsensical way to look at it).\\xa0\\xa0\\xa0\\nAt the seventy-eighth move of the fourth game, Sedol made a so-called “wedge” play that was not conventional and surprised onlookers. The next move by AlphaGo was rotten and diminished the likelihood of a win by the AI system. After additional play, ultimately AlphaGo tossed in the towel and resigned from the match, thus Sedol finally had a win against the AI in his belt. He ended-up losing the fifth game, so AlphaGo won four games, Sedol won one). His move also became famous, generally known as “Move 78” in the lore of Go playing.\\xa0\\nSomething else that is worthwhile to know about involves the overarching strategy that AlphaGo was crafted to utilize.\\xa0\\xa0\\xa0\\nWhen you play a game, let’s say connect-the-dots, you can aim to grab as many squares at each moment of play, doing so under the belief that inevitably you will then win by the accumulation of those tactically-oriented successes. Human players of Go are often apt to play that way, as it can be said too of chess players, and nearly any kind of game playing altogether.\\xa0\\xa0\\xa0\\nAnother approach involves playing to win, even if only by the thinnest of margins, as long as you win. In that case, you might not be motivated for each tactical move to gain near-term territory or score immediate points, and be willing instead to play a larger scope game per se. The proverbial mantra is that if you are shortsighted, you might win some of the battles, but could eventually lose the war. Therefore, it might be a better strategy to keep your eye on the prize, winning the war, albeit if it means that there are battles and skirmishes to be lost along the way.\\xa0\\xa0\\xa0\\nThe AI developers devised AlphaGo with that kind of macro-perspective underlying how the AI system functioned.\\xa0\\xa0\\xa0\\nHumans can have an especially hard time choosing at the moment to make a move that might look bad or ill-advised, such as giving up territory, finding themselves to be unable to grit their teeth, and taking a lump or two during play. The embarrassment at the instant is difficult to offset by betting that it is going to ultimately be okay, and you will prevail in the end.\\xa0\\xa0\\xa0\\nFor an AI system, there is no semblance of that kind of sentiment involved, and it is all about calculated odds and probabilities.\\xa0\\xa0\\xa0\\nNow that we’ve covered the legendary Go match, let’s consider some lessons learned about novelty.\\xa0\\xa0\\xa0\\nThe “Move 38” made by the AI system was not magical. It was an interesting move, for sure, and the AI developers later indicated that the move was one that the AI had calculated would rarely be undertaken by a human player.\\xa0\\xa0\\xa0\\nThis can be interpreted in two ways (at least).\\xa0\\xa0\\xa0\\nOne interpretation is that a human player would not make that move because humans are right and know that it would be a lousy move.\\xa0\\xa0\\xa0\\nAnother interpretation is that humans would not make that move due to a belief that the move is unwise, but this could be a result of the humans insufficiently assessing the ultimate value of the move, in the long-run, and getting caught up in a shorter time frame semblance of play.\\xa0\\nIn this instance, it turned out to be a good move—maybe a brilliant move—and turned the course of the game to the advantage of the AI. Thus, what looked like brilliance was in fact a calculated move that few humans would have imagined as valuable and for which jostled humans to rethink how they think about such matters.\\xa0\\xa0\\xa0\\nSome useful recap lessons:\\xa0\\xa0\\xa0\\nShowcasing Human Self-Limited Insight.\\xa0When the AI does something seemingly novel, it might be viewed as novel simply because humans have already predetermined what is customary and anything beyond that is blunted by the assumption that it is unworthy or mistaken. You could say that we are mentally trapped by our own drawing of the lines of what is considered as inside versus outside the box.\\xa0\\xa0\\xa0\\nHumans Exploiting AI For Added Insight. Humans can gainfully assess an AI-powered novelty to potentially re-calibrate human thinking on a given topic, enlarging our understanding via leveraging something that the AI, via its vast calculative capacity, might detect or spot that we have not yet so ascertained. Thus, besides admiring the novelty, we ought to seek to improve our mental prowess by whatever source shines brightly including an AI system.\\xa0\\xa0\\xa0\\nAI Novelty Is A Dual-Edged Sword.\\xa0We need to be mindful of all AI systems and their possibility of acting in a novel way, which could be good or could be bad. In the Go game, it worked out well. In other circumstances, the AI exploiting the novelty route might go off the tracks, as it were.\\xa0\\xa0\\xa0\\nLet’s see how this can be made tangible via exploring the advent of AI-based true self-driving cars.\\xa0\\xa0\\xa0\\nFor my framework about AI autonomous cars, see the link here:\\xa0https://aitrends.com/ai-insider/framework-ai-self-driving-driverless-cars-big-picture/\\xa0\\xa0\\xa0\\nWhy this is a moonshot effort, see my explanation here:\\xa0https://aitrends.com/ai-insider/self-driving-car-mother-ai-projects-moonshot/\\xa0\\xa0\\xa0\\nFor more about the levels as a type of Richter scale, see my discussion here:\\xa0https://aitrends.com/ai-insider/richter-scale-levels-self-driving-cars/\\xa0\\xa0\\xa0\\nFor the argument about bifurcating the levels, see my explanation here:\\xa0https://aitrends.com/ai-insider/reframing-ai-levels-for-self-driving-cars-bifurcation-of-autonomy/\\xa0\\xa0\\xa0\\nUnderstanding The Levels Of Self-Driving Cars\\xa0\\nAs a clarification, true self-driving cars are ones where the AI drives the car entirely on its own and there isn’t any human assistance during the driving task.\\xa0\\nThese driverless vehicles are considered a Level 4 and Level 5, while a car that requires a human driver to co-share the driving effort is usually considered at a Level 2 or Level 3. The cars that co-share the driving task are described as being semi-autonomous, and typically contain a variety of automated add-on’s that are referred to as ADAS (Advanced Driver-Assistance Systems).\\xa0\\xa0\\xa0\\nThere is not yet a true self-driving car at Level 5, which we don’t yet even know if this will be possible to achieve, and nor how long it will take to get there.\\xa0\\xa0\\xa0\\nMeanwhile, the Level 4 efforts are gradually trying to get some traction by undergoing very narrow and selective public roadway trials, though there is controversy over whether this testing should be allowed per se (we are all life-or-death guinea pigs in an experiment taking place on our highways and byways, some contend).\\xa0\\xa0\\xa0\\nFor why remote piloting or operating of self-driving cars is generally eschewed, see my explanation here:\\xa0https://aitrends.com/ai-insider/remote-piloting-is-a-self-driving-car-crutch/\\xa0\\xa0\\xa0\\nTo be wary of fake news about self-driving cars, see my tips here:\\xa0https://aitrends.com/ai-insider/ai-fake-news-about-self-driving-cars/\\xa0\\nThe ethical implications of AI driving systems are significant, see my indication here:\\xa0http://aitrends.com/selfdrivingcars/ethically-ambiguous-self-driving-cars/\\xa0\\xa0\\xa0\\nBe aware of the pitfalls of normalization of deviance when it comes to self-driving cars, here’s my call to arms:\\xa0https://aitrends.com/ai-insider/normalization-of-deviance-endangers-ai-self-driving-cars/\\xa0\\nSelf-Driving Cars And Acts Of Novelty\\xa0\\xa0\\xa0\\nFor Level 4 and Level 5 true self-driving vehicles, there won’t be a human driver involved in the driving task.\\xa0All occupants will be passengers; the AI is doing the driving.\\xa0\\xa0\\xa0\\nYou could say that the AI is playing a game, a driving game, requiring tactical decision-making and strategic planning, akin to when playing Go or chess, though in this case involving life-or-death matters driving a multi-ton car on our public roadways.\\xa0\\xa0\\xa0\\nOur base assumption is that the AI driving system is going to always take a tried-and-true approach to any driving decisions. This assumption is somewhat shaped around a notion that AI is a type of robot or automata that is bereft of any human biases or human foibles.\\xa0\\xa0\\xa0\\nIn reality, there is no reason to make this kind of assumption. Yes, we can generally rule out the aspect that the AI is not going to display the emotion of a human ilk, and we also know that the AI will not be drunk or DUI in its driving efforts. Nonetheless, if the AI has been trained using Machine Learning (ML) and Deep Learning (DL), it can pick up subtleties of human behavioral patterns in the data about human driving, out of which it will likewise utilize or mimic in choosing its driving actions (for example, see my column postings involving an analysis of potential racial biases in AI and the possibility of gender biases).\\xa0\\xa0\\xa0\\nTurning back to the topic of novelty, let’s ponder a specific use case.\\xa0\\xa0\\xa0\\nA few years ago, I was driving on an open highway, going at the prevailing speed of around 65 miles per hour, and something nearly unimaginable occurred. A car coming toward me in the opposing lane, and likely traveling at around 60 to 70 miles per hour, suddenly and unexpectedly veered into my lane. It was one of those moments that you cannot anticipate.\\xa0\\xa0\\xa0\\nThere did not appear to be any reason for the other driver to be headed toward me, in my lane of traffic, and coming at me for an imminent and bone-chillingly terrifying head-on collision. If there had been debris on the other lane, it might have been a clue that perhaps this other driver was simply trying to swing around the obstruction. No debris. If there was a slower moving car, the driver might have wanted to do a fast end-around to get past it. Nope, there was absolutely no discernible basis for this radical and life-threatening maneuver.\\xa0\\nWhat would you do?\\xa0\\nCome on, hurry, the clock is ticking, and you have just a handful of split seconds to make a life-or-death driving decision.\\xa0\\xa0\\xa0\\nYou could stay in your lane and hope that the other driver realizes the error of their ways, opting to veer back into their lane at the last moment. Or, you could proactively go into the opposing lane, giving the other driver a clear path in your lane, but this could be a chancy game of chicken whereby the other driver chooses to go back into their lane (plus, there was other traffic further behind that driver, so going into the opposing lane was quite dicey).\\xa0\\xa0\\xa0\\nOkay, so do you stay in your lane or veer away into the opposing lane?\\xa0\\xa0\\xa0\\nI dare say that most people would be torn between those two options. Neither one is palatable.\\xa0\\nSuppose the AI of a self-driving car was faced with the same circumstance.\\xa0\\xa0\\xa0\\nWhat would the AI do?\\xa0\\xa0\\xa0\\nThe odds are that even if the AI had been fed with thousands upon thousands of miles of driving via a database about human driving while undergoing the ML/DL training, there might not be any instances of a head-to-head nature and thus no prior pattern to utilize for making this onerous decision.\\xa0\\xa0\\xa0\\nAnyway, here’s a twist.\\xa0\\xa0\\xa0\\nImagine that the AI calculated the probabilities involving which way to go, and in some computational manner came to the conclusion that the self-driving car should go into the ditch that was at the right of the roadway. This was intended to avoid entirely a collision with the other car (the AI estimated that a head-on collision would be near-certain death for the occupants). The AI estimated that going into the ditch at such high speed would indisputably wreck the car and cause great bodily injury to the occupants, but the odds of assured death were (let’s say) calculated as lower than the head-on option possibilities (this is a variant of the infamous Trolley Problem, as covered in my columns).\\xa0\\xa0\\xa0\\nI’m betting that you would concede that most humans would be relatively unwilling to aim purposely into that ditch, which they know for sure is going to be a wreck and potential death, while instead willing (reluctantly) to take a hoped-for chance of either veering into the other lane or staying on course and wishing for the best.\\xa0\\xa0\\xa0\\nIn some sense, the AI might seem to have made a novel choice. It is one that (we’ll assume) few humans would have given any explicit thought toward.\\xa0\\xa0\\xa0\\nReturning to the earlier recap of the points about AI novelty, you could suggest that in this example, the AI has exceeded a human self-imposed limitation by the AI having considered otherwise “unthinkable” options. From this, perhaps we can learn to broaden our view for options that otherwise don’t seem apparent.\\xa0\\xa0\\xa0\\nThe other recap element was that the AI novelty can be a dual-edged sword.\\xa0\\xa0\\xa0\\nIf the AI did react by driving into the ditch, and you were inside the self-driving car, and you got badly injured, would you later believe that the AI acted in a novel manner or that it acted mistakenly or adversely?\\xa0\\xa0\\xa0\\nSome might say that if you lived to ask that question, apparently the AI made the right choice. The counter-argument is that if the AI had gone with one of the other choices, perhaps you would have sailed right past the other car and not gotten a single scratch.\\xa0\\xa0\\xa0\\nFor more details about ODDs, see my indication at this link here:\\xa0https://www.aitrends.com/ai-insider/amalgamating-of-operational-design-domains-odds-for-ai-self-driving-cars/\\xa0\\nOn the topic of off-road self-driving cars, here’s my details elicitation:\\xa0https://www.aitrends.com/ai-insider/off-roading-as-a-challenging-use-case-for-ai-autonomous-cars/\\xa0\\nI’ve urged that there must be a Chief Safety Officer at self-driving car makers, here’s the scoop:\\xa0https://www.aitrends.com/ai-insider/chief-safety-officers-needed-in-ai-the-case-of-ai-self-driving-cars/\\xa0\\nExpect that lawsuits are going to gradually become a significant part of the self-driving car industry, see my explanatory details here:\\xa0http://aitrends.com/selfdrivingcars/self-driving-car-lawsuits-bonanza-ahead/\\xa0\\nConclusion\\xa0\\xa0\\xa0\\nFor those of you wondering what actually did happen, my lucky stars were looking over me that day, and I survived with nothing more than a close call. I decided to remain in my lane, though it was tempting to veer into the opposing lane, and by some miracle, the other driver suddenly went back into the opposing lane.\\xa0\\xa0\\xa0\\nWhen I tell the story, my heart still gets pumping, and I begin to sweat.\\xa0\\xa0\\xa0\\nOverall, AI that appears to engage in novel approaches to problems can be advantageous and in some circumstances such as playing a board game can be right or wrong, for which being wrong does not especially put human lives at stake.\\xa0\\xa0\\xa0\\nFor AI-based true self-driving cars, lives are at stake.\\xa0\\xa0\\xa0\\nWe’ll need to proceed mindfully and with our eyes wide open about how we want AI driving systems to operate, including calculating odds and deriving choices while at the wheel of the vehicle.\\xa0\\xa0\\nCopyright 2021 Dr. Lance Eliot\\xa0\\xa0\\nhttp://ai-selfdriving-cars.libsyn.com/website\\xa0'},\n",
       " {'title': 'Getting Government AI Engineers to Tune into AI Ethics Seen as Challenge',\n",
       "  'content': 'By John P. Desmond, AI Trends Editor\\xa0\\xa0\\nEngineers tend to see things in unambiguous terms, which some may call Black and White terms, such as a choice between right or wrong and good and bad.\\xa0The consideration of ethics in AI is highly nuanced, with vast gray areas, making it\\xa0 challenging for AI software engineers to apply it in their work.\\xa0\\xa0\\nThat was a takeaway from a session on the Future of Standards and Ethical AI at the\\xa0AI World Government\\xa0conference held in-person and virtually in Alexandria, Va. this week.\\xa0\\xa0\\xa0\\nAn overall impression from the conference is that the discussion of AI and ethics is happening in virtually every quarter of AI in the vast enterprise of the federal government, and the consistency of points being made across all these different and independent efforts stood out.\\xa0\\xa0\\nBeth-Ann Schuelke-Leech, associate professor, engineering management, University of Windsor\\n“We engineers often think of ethics as a fuzzy thing that no one has really explained,” stated Beth-Anne Schuelke-Leech, an associate professor, Engineering Management and Entrepreneurship at the University of Windsor, Ontario, Canada, speaking at the Future of Ethical AI session. “It can be difficult for engineers looking for solid constraints to be told to be ethical. That becomes really complicated because we don’t know what it really means.”\\xa0\\xa0\\nSchuelke-Leech started her career as an engineer, then decided to pursue a PhD in public policy, a background which\\xa0enables\\xa0her to see things as an engineer and as a social scientist. “I got a PhD in social science, and have been pulled back into the engineering world where I am involved in AI projects, but based in a mechanical engineering faculty,” she said.\\xa0\\xa0\\xa0\\nAn engineering project has a goal, which describes the purpose, a set of needed features and functions, and a set of constraints, such as budget and timeline “The standards and regulations become part of the constraints,” she said. “If I know I have to comply with it, I will do that. But if you tell me it’s a good thing to do, I may or may not adopt that.”\\xa0\\xa0\\nSchuelke-Leech also serves as chair of the IEEE Society’s Committee on the Social Implications of Technology Standards. She commented, “Voluntary compliance standards such as from the IEEE are essential from people in the industry getting together to say this is what we think we should do as an industry.”\\xa0\\xa0\\nSome standards, such as around interoperability, do not have the force of law but engineers comply with them, so their systems will work. Other standards are described as good practices, but are not required to be followed. “Whether it helps me to achieve my goal or hinders me getting to the objective, is how the engineer looks at it,” she said.\\xa0\\xa0\\xa0\\nThe Pursuit of AI Ethics Described as “Messy and Difficult”\\xa0\\xa0\\nSara Jordan, senior counsel, Future of Privacy Forum\\nSara Jordan, senior counsel with the Future of Privacy Forum, in the session with Schuelke-Leech, works on the ethical challenges of AI and machine learning and is an active member of the IEEE Global Initiative on Ethics and Autonomous and Intelligent Systems. “Ethics is messy and difficult, and is context-laden. We have a proliferation of theories, frameworks and constructs,” she said, adding, “The practice of ethical AI will require repeatable, rigorous thinking in context.”\\xa0\\xa0\\nSchuelke-Leech offered, “Ethics is not an end outcome. It is the process being followed. But I’m also looking for someone to tell me what I need to do to do my job, to tell me how to be ethical, what rules I’m supposed to follow, to take away the ambiguity.”\\xa0\\xa0\\n“Engineers shut down when you get into funny words that they don’t understand, like ‘ontological,’ They’ve been taking math and science since they were 13-years-old,” she said.\\xa0\\xa0\\nShe has found it difficult to get engineers involved in attempts to draft standards for ethical AI. “Engineers are missing from the table,” she said. “The debates about whether we can get to 100%\\xa0ethical are\\xa0conversations\\xa0engineers do not have.”\\xa0\\xa0\\nShe concluded, “If their managers tell them to figure it out, they will do so. We need to help the engineers cross the bridge halfway. It is essential that social scientists and engineers don’t give up on this.”\\xa0\\xa0\\nLeader’s Panel Described Integration of Ethics into AI Development Practices\\xa0\\xa0\\nThe topic of ethics in AI is coming up more in the curriculum of the US Naval War College of Newport, R.I., which was established to provide advanced study for US Navy officers and now educates leaders from all services. Ross Coffey, a military professor of National Security Affairs at the institution, participated in a Leader’s Panel on AI, Ethics and Smart Policy at AI World Government.\\xa0\\xa0\\n“The ethical literacy of students increases over time as they are working with these ethical issues, which is why it is an urgent matter because it will take a long time,” Coffey said.\\xa0\\xa0\\nPanel member Carole Smith, a senior research scientist with Carnegie Mellon University who\\xa0studies human-machine interaction, has been involved in integrating ethics into AI systems development since 2015. She cited the importance of “demystifying” AI.\\xa0\\xa0\\xa0\\xa0\\n“My interest is in understanding what kind of interactions we can create where the human is appropriately trusting the system they are working with, not over- or under-trusting it,” she said, adding, “In general, people have higher expectations than they should for the systems.”\\xa0\\xa0\\nAs an example, she cited the Tesla Autopilot features, which implement self-driving car capability to a degree but not completely. “People assume the system can do a much broader set of activities than it was designed to do. Helping people understand the limitations of a system is important. Everyone needs to understand the expected outcomes of a system and what some of the mitigating circumstances might be,” she said.\\xa0\\xa0\\xa0\\nPanel member Taka Ariga, the first chief data scientist appointed to the US Government Accountability Office and director of the GAO’s Innovation Lab, sees a gap in AI literacy for the young workforce coming into the federal government.\\xa0 “Data scientist training does not always include ethics. Accountable AI is a laudable construct, but I’m not sure everyone buys into it. We need their responsibility to go beyond technical aspects and be accountable to the end user we are trying to serve,” he said.\\xa0\\xa0\\nPanel moderator Alison Brooks, PhD, research VP of Smart Cities and Communities at the IDC market research firm, asked whether principles of ethical AI can be shared across the boundaries of nations.\\xa0\\xa0\\xa0\\n“We will have a limited ability for every nation to align on the same exact approach, but we will have to align in some ways on what we will not allow AI to do, and what people will also be responsible for,” stated Smith of CMU.\\xa0\\xa0\\xa0\\nThe panelists credited the European Commission for being out front on these issues of ethics, especially in the enforcement realm.\\xa0\\nRoss of the Naval War Colleges acknowledged the importance of finding common ground around AI ethics. “From a military perspective, our interoperability needs to go to a whole new level. We need to find common ground with our partners and our allies on what we will allow AI to do and what we will not allow AI to do.” Unfortunately, “I don’t know if that discussion is happening,” he said.\\xa0\\xa0\\nDiscussion on AI ethics could perhaps be pursued as part of certain existing treaties, Smith suggested\\xa0\\xa0\\nThe many AI ethics principles, frameworks,\\xa0and road maps being offered in many federal agencies can be challenging to follow and be made consistent. Take said, “I am hopeful that over the next year or two, we will see a\\xa0coalescing.”\\xa0\\xa0\\nFor more information and access to recorded sessions, go to\\xa0AI World Government.\\xa0'},\n",
       " {'title': 'Digital Natives Seen Having Advantages as Part of Government AI Engineering Teams',\n",
       "  'content': 'By John P. Desmond, AI Trends Editor\\xa0\\xa0\\nAI is more accessible to young people in the workforce who grew up as ‘digital natives’ with Alexa and self-driving cars as part of the landscape, giving them expectations grounded in their experience of what is possible.\\xa0\\xa0\\nThat idea set the foundation for a panel discussion at\\xa0AI World Government\\xa0on Mindset Needs and Skill Set Myths for AI engineering teams, held this week virtually and in-person in Alexandria, Va.\\xa0\\xa0\\nDorothy Aronson, CIO and Chief Data Officer, National Science Foundation\\n“People feel that AI is within their grasp because the technology is available, but the technology is ahead of our cultural maturity,” said panel member Dorothy Aronson, CIO and Chief Data Officer for the National Science Foundation. “It’s like giving a sharp object to a child. We might have access to big data, but it might not be the right thing to do,” to work with it in all cases.\\xa0\\xa0\\xa0\\nThings are accelerating, which is raising expectations. When panel member Vivek Rao, lecturer and researcher at the University of California at Berkeley, was working on his PhD, a paper on natural language processing might be a master’s thesis. “Now we assign it as a homework assignment with a two-day turnaround. We have an enormous amount of compute power that was not available even two years ago,”\\xa0he said of his students, who he described as “digital natives”\\xa0with high expectations of what AI makes possible.\\xa0\\xa0\\nRachel\\xa0Dzombak, digital transformation lead, Software Engineering Institute, Carnegie Mellon University\\nPanel moderator Rachel\\xa0Dzombak, digital transformation lead at the\\xa0Software Engineering Institute\\xa0of Carnegie Mellon University, asked the panelists what is unique about working on AI in the government.\\xa0\\xa0\\xa0\\nAronson said the government cannot get too far ahead with the technology, or the users will not know how to interact with it. “We’re not building iPhones,” she said. “We have experimentation going on, and we are always looking ahead, anticipating the future, so we can make the most cost-effective decisions. In the government right now, we are seeing the convergence of the emerging generation and the close-to-retiring generation, who we also have to serve.”\\xa0\\xa0\\xa0\\nEarly in her career, Aronson did not want to work in the government. “I thought it meant you were either in the armed services or the Peace Corps,” she said. “But what I learned after a while is what motivates federal employees is service to larger, problem-solving institutions. We are trying to solve really big problems of equity and diversity, and getting food to people and keeping people safe. People that work for the government are dedicated to those missions.”\\xa0\\xa0\\xa0\\nShe referred to her two children in their 20s, who like the idea of service, but in “tiny chunks,” meaning, “They don’t look at the government as a place where they have freedom, and they can do whatever they want. They see it as a lockdown situation. But it’s really not.”\\xa0\\xa0\\xa0\\nBerkeley Students Learn About Role of Government in Disaster Response\\xa0\\xa0\\nRao of Berkeley said his students are seeing wildfires in California and asking who is working on the challenge of doing something about them. When he tells them it is almost always local, state and federal government entities, “Students are generally surprised to find that out.”\\xa0\\xa0\\xa0\\nIn one example, he developed a course on innovation in disaster response, in collaboration with CMU and the Department of Defense, the Army Futures Lab and Coast Guard search and rescue. “This was eye-opening for students,” he said. At the outset, two of 35 students expressed interest in a federal government career. By the end of the course, 10 of the 35 students were expressing interest. One of them was hired by the Naval Surface Warfare Center outside Corona, Calif. as a software engineer, Rao said.\\xa0\\xa0\\nAronson described the process of bringing on new federal employees as a “heavy lift,” suggesting, “if we could prepare in advance, it would move a lot faster.”\\xa0\\nBryan Lane, director of Data &\\xa0AI,\\xa0General\\xa0Services Administration\\nAsked by\\xa0Dzombak\\xa0what skill sets and mindsets are seen as essential to AI engineering teams, panel member Bryan Lane, director of Data & AI at the General Services Administration (who announced during the session that he is taking on a new role at FDIC), said resiliency is a necessary quality.\\xa0\\xa0\\nLane is a technology executive within the GSA IT Modernization Centers of Excellence (CoE) with over 15 years of experience leading advanced analytics and technology initiatives. He has led the GSA partnership with the DoD Joint Artificial Intelligence Center (JAIC).\\xa0[Ed. Note: Known as “the Jake.”]\\xa0Lane also is the founder of\\xa0DATA XD.\\xa0He also has experience in industry, managing acquisition portfolios.\\xa0\\xa0\\xa0\\n“The most important thing about resilient teams going on an AI journey is that you need to be ready for the unexpected, and the mission persists,” he said.\\xa0“If you are all aligned on the importance of the mission, the team can be held together.”\\xa0\\xa0\\nGood Sign that Team Members Acknowledge Having “Never Done This Before”\\xa0\\xa0\\nRegarding mindset, he said more of his team members are coming to him and saying, “I’ve never done this before.” He sees that as a good sign that offers an opportunity to talk about risk and alternative solutions. “When your team has the psychological safety to say that they don’t know something,” Lane sees it as positive. “The focus is always on what you have done and what you have delivered. Rarely is the focus on what you have not done before and what you want to grow into,” he said,\\xa0\\xa0\\nAronson has found it challenging to get AI projects off the ground. “It’s hard to tell management that you have a use case or problem to solve and want to go at it, and there is a 50-50 chance it will get done, and you don’t know how much it’s going to cost,” she said. “It comes down to articulating the rationale and convincing others it’s the right thing to do to move forward.”\\xa0\\xa0\\nRao said he talks to students about experimentation and having an experimental mindset. “AI tools can be easily accessible, but they can mask the challenges you can encounter. When you apply the vision API, for example in the context of challenges in your business or government agency, things may not be smooth,” he said.\\xa0\\xa0\\nModerator\\xa0Dzombak\\xa0asked the panelists how they build teams. Arson said, “You need a mix of people.” She has tried “communities of practice” around solving specific problems, where people can come and go. “You bring people together around a problem and not a tool,” she said.\\xa0\\xa0\\nLane seconded this. “I really have stopped focusing on tools in general,” he said. He ran experiments at JAIC in accounting, finance and other areas. “We found it’s not really about the tools. It’s about getting the right people together to understand the problems, then looking at the tools available,” he said.\\xa0\\xa0\\nLane said he sets up “cross-functional teams” that are “a little more formal than a community of interest.”\\xa0He has found them to be effective for working together on a problem for maybe 45 days. He also likes working with customers of the needed services inside the organization, and has seen customers learn about data management and AI as a result. “We will pick up one or two along the way who become advocates for accelerating AI throughout the organization,” Lane said.\\xa0\\xa0\\nLane sees it taking five years to work out proven methods of thinking, working,\\xa0and best practices for developing AI systems to serve the government. He mentioned\\xa0The Opportunity Project\\xa0(TOP) of the US Census Bureau, begun in 2016 to work on challenges such as ocean plastic pollution, COVID-19 economic recovery and disaster response. TOP has engaged in over 135 public-facing projects in that time, and has over 1,300 alumni including developers, designers, community leaders, data and policy experts, students and government agencies.\\xa0\\xa0\\xa0\\n“It’s based on a way of thinking and how to organize work,” Lane said. “We have to scale the model of delivery, but five years from now, we will have enough proof of concept to know what works and what does not.”\\xa0\\nLearn more at\\xa0AI World Government, at the\\xa0Software Engineering Institute, at\\xa0DATA XD\\xa0and at\\xa0The Opportunity Project.\\xa0'},\n",
       " {'title': 'How Accountability Practices\\xa0Are\\xa0Pursued by AI Engineers in the Federal Government',\n",
       "  'content': 'By\\xa0John P. Desmond,\\xa0AI Trends\\xa0Editor\\xa0\\xa0\\xa0\\nTwo experiences of how AI developers within the federal government are pursuing AI accountability practices were outlined at the\\xa0AI World Government\\xa0event held virtually and in-person this week in Alexandria, Va.\\xa0\\nTaka Ariga, chief data scientist and director, US\\xa0Government Accountability Office\\nTaka Ariga, chief data scientist and director at the US\\xa0Government Accountability Office,\\xa0described an AI accountability framework he uses within his agency and plans to make available to others.\\xa0\\xa0\\nAnd Bryce\\xa0Goodman, chief strategist for AI and machine learning at the\\xa0Defense Innovation Unit\\xa0(DIU), a unit of the Department of Defense founded to help the US military make faster use of emerging commercial technologies, described work in his unit to apply principles of AI development to terminology that an engineer can apply.\\xa0\\xa0\\nAriga, the first chief data scientist appointed to the US Government Accountability Office and director of the GAO’s Innovation Lab, discussed an\\xa0AI Accountability Framework\\xa0he helped to develop by convening a forum of experts in the government, industry, nonprofits, as well as federal inspector general officials and AI experts.\\xa0\\xa0\\xa0\\n“We are adopting an auditor’s perspective on the AI accountability framework,” Ariga said. “GAO is in the business of verification.”\\xa0\\xa0\\nThe effort to produce a formal framework began in September 2020 and included 60%\\xa0women, 40%\\xa0of whom were underrepresented minorities, to discuss over two days. The effort was spurred by a desire to ground the AI accountability framework in the reality of an engineer’s day-to-day work. The resulting framework was first published in June as what Ariga described as “version 1.0.”\\xa0\\xa0\\nSeeking to Bring a “High-Altitude Posture” Down to Earth\\xa0\\xa0\\n“We found the AI accountability framework had a very high-altitude posture,” Ariga said. “These are laudable ideals and aspirations, but what do they mean to the day-to-day AI practitioner? There is a gap, while we see AI proliferating across the government.”\\xa0\\xa0\\n“We landed on a lifecycle approach,” which steps through stages of design, development, deployment and continuous monitoring. The development effort stands on four “pillars” of Governance, Data, Monitoring and Performance.\\xa0\\xa0\\nGovernance reviews what the organization has put in place to oversee the AI efforts. “The chief AI officer might be in place, but what does it mean? Can the person make changes? Is it multidisciplinary?”\\xa0 At a system level within this pillar, the team will review individual AI models to see if they were “purposely deliberated.”\\xa0\\xa0\\nFor the Data pillar,\\xa0his team will examine how the training data was evaluated, how representative it is, and is it functioning as intended.\\xa0\\xa0\\nFor the Performance pillar, the team will consider the “societal impact” the AI system will have in deployment, including whether it risks a violation of the Civil Rights Act. “Auditors have a long-standing track record of evaluating equity. We grounded the evaluation of AI to a proven system,” Ariga said.\\xa0\\xa0\\xa0\\nEmphasizing the importance of continuous monitoring, he said, “AI is not a technology you deploy and forget.” he said. “We are preparing to continually monitor for model drift and the fragility of algorithms, and we are scaling the AI appropriately.” The evaluations will determine whether the AI system continues to meet the need “or whether a sunset is more appropriate,” Ariga said.\\xa0\\xa0\\nHe is part of the discussion with NIST on an overall government AI accountability framework. “We don’t want an ecosystem of confusion,” Ariga said. “We want a whole-government approach. We feel that this is a useful first step in pushing high-level ideas down to an altitude meaningful to the practitioners of AI.”\\xa0\\xa0\\nDIU Assesses Whether Proposed Projects Meet Ethical AI Guidelines\\xa0\\xa0\\nBryce Goodman, chief strategist for AI and machine learning, the\\xa0Defense Innovation Unit\\nAt the DIU, Goodman is involved in a similar effort to develop guidelines for developers of AI projects within the government.\\xa0\\xa0\\xa0\\nProjects Goodman has been involved with implementation of AI for humanitarian assistance and disaster response, predictive maintenance,\\xa0to\\xa0counter-disinformation,\\xa0and predictive health. He heads the Responsible AI Working Group. He is a faculty member of Singularity University, has a wide range of consulting clients from inside and outside the government, and holds a PhD in AI and Philosophy from the University of Oxford.\\xa0\\xa0\\nThe DOD in February 2020 adopted five areas of\\xa0Ethical Principles for AI\\xa0after 15 months of consulting with AI experts in commercial industry, government academia and the American public.\\xa0 These areas are: Responsible, Equitable, Traceable, Reliable and Governable.\\xa0\\xa0\\xa0\\n“Those are well-conceived, but it’s not obvious to an engineer how to translate them into a specific project requirement,” Good said in a presentation on Responsible AI Guidelines at the AI World Government event. “That’s the gap we are trying to fill.”\\xa0\\nBefore the DIU even considers a project, they run through the ethical principles to see if it passes muster. Not all projects do. “There needs to be an option to say the technology is not there or the problem is not compatible with AI,” he said.\\xa0\\xa0\\xa0\\nAll project stakeholders, including from commercial vendors and within the government, need to be able to test and validate and go beyond minimum legal requirements to meet the principles. “The law is not moving as fast as AI, which is why these principles are important,” he said.\\xa0\\xa0\\nAlso, collaboration is going on across the government to ensure values are being preserved and maintained. “Our intention with these guidelines is not to try to achieve perfection, but to avoid catastrophic consequences,” Goodman said. “It can be difficult to get a group to agree on what the best outcome is, but it’s easier to get the group to agree on what the worst-case outcome is.”\\xa0\\xa0\\nThe DIU guidelines along with case studies and supplemental materials will be published on the DIU website “soon,” Goodman said, to help others leverage the experience.\\xa0\\xa0\\nHere are Questions DIU Asks Before Development Starts\\xa0\\xa0\\nThe first step in the guidelines is to define the task.\\xa0 “That’s the single most important question,” he said. “Only if there is an advantage, should you use AI.”\\xa0\\nNext is a benchmark, which needs to be set up front to know if the project\\xa0has delivered.\\xa0\\xa0\\xa0\\nNext, he evaluates ownership of the candidate data. “Data is critical to the AI system and is the place where a lot of problems can exist.” Goodman said. “We need a certain contract on who owns the data. If ambiguous, this can lead to problems.”\\xa0\\xa0\\nNext, Goodman’s team wants a sample of data to evaluate. Then, they need to know how and why the information was collected. “If consent was given for one purpose, we cannot use it for another purpose without re-obtaining consent,” he said.\\xa0\\xa0\\nNext, the team asks if the responsible stakeholders are identified, such as pilots who could be affected if a component fails.\\xa0\\xa0\\xa0\\nNext, the responsible mission-holders must be identified. “We need a single individual for this,” Goodman said. “Often we have a tradeoff between the performance of an algorithm and its\\xa0explainability. We might have to decide between the two. Those kinds of decisions have an ethical component and an operational component. So we need to have someone who is accountable for those decisions, which is consistent with the chain of command in the DOD.”\\xa0\\xa0\\xa0\\nFinally, the DIU team requires a process for rolling back if things go wrong. “We need to be cautious about abandoning the previous system,” he said.\\xa0\\xa0\\xa0\\nOnce all these questions are answered in a satisfactory way, the team moves on to the development phase.\\xa0\\xa0\\nIn lessons learned, Goodman said, “Metrics are key. And simply measuring accuracy might not be adequate. We need to be able to measure success.”\\xa0\\nAlso, fit the technology to the task. “High risk applications require low-risk technology. And when potential harm is significant, we need to have high confidence in the technology,” he said.\\xa0\\xa0\\nAnother lesson learned is to set expectations with commercial vendors. “We need vendors to be transparent,” he said. ”When someone says they have a proprietary algorithm they cannot tell us about, we are very wary. We view the relationship as a collaboration. It’s the only way we can ensure\\xa0that the AI\\xa0is developed responsibly.”\\xa0\\xa0\\nLastly, “AI is not magic. It will not solve everything. It should only be used when necessary and only when we can prove it will provide an advantage.”\\xa0\\xa0\\nLearn more at\\xa0AI World Government, at the\\xa0Government Accountability Office,\\xa0at the\\xa0AI Accountability Framework\\xa0and at the\\xa0Defense Innovation Unit\\xa0site.\\xa0'},\n",
       " {'title': 'Startup:\\xa0AssemblyAI\\xa0Represents New Generation Speech Recognition',\n",
       "  'content': 'By AI Trends Staff\\xa0\\xa0\\nAdvances in the AI behind speech recognition are driving growth in the market, attracting venture capital and funding startups, posing challenges to established players.\\xa0\\xa0\\nThe growing acceptance and use of speech recognition devices are driving the market, which according to an estimate by Meticulous Research is expected to reach $26.8 billion globally by 2025, according to a recent account in\\xa0Analytics Insight. Better speed and accuracy are among the benefits of the evolving technology.\\xa0\\nDylan Fox, CEO and Founder,\\xa0AssemblyAI\\nOne company in the throes of this new growth,\\xa0AssemblyAI\\xa0of San Francisco, is offering an API for speech recognition capable of transcribing videos, podcasts, phone calls,\\xa0and remote meetings. The company was founded by CEO Dylan Fox in 2017 and has received backing from Y Combinator, a startup accelerator, as well as NVIDIA.\\xa0\\xa0\\xa0\\nFox has an unusual background for a high tech entrepreneur. He is a graduate of George Washington University with a degree in business administration, business economics,\\xa0and public policy. He got a job as a software engineer for machine learning in the emerging product lab of Cisco in San Francisco, working on deep neural networks and machine learning. He got the idea for\\xa0AssemblyAi\\xa0and attracted capital from Y Combinator, which enabled him to hire data scientists and data engineers to get the technology off the ground.\\xa0\\xa0\\xa0\\nAsked in an interview with\\xa0AI Trends\\xa0how he made this transition from undergrad in business administration and economics to high-tech entrepreneur, Fox said, “I taught myself how to program, which led me to a path of machine learning. I was looking for a harder software challenge, which led to natural language processing, which took me to Cisco.” They were working on Siri for the Enterprise for Apple at the time,\\xa0\\nTo speed up the work, Cisco was looking to acquire speech recognition software; Fox was in the catbird’s seat for the search. “We looked at Nuance,” for example, acknowledged as a market leader and owner of more speech recognition software than its competitors. (The acquisition of Nuance by Microsoft for $19.6 billion is expected to be finalized by year-end.) The young, budding entrepreneur was not impressed. “It was crazy how bad all the options were from an accuracy and a developer point of view,” he stated.\\xa0\\xa0\\nHe was impressed by Twilio, a San Francisco-based company founded in 2008, which that year released the Twilio Voice API to make and receive phone calls hosted in the cloud. The company has since raised $103 million in venture capital. “They were setting new standards for a good API for developers,” Fox said.\\xa0\\xa0\\nFox’s idea was to use AI and machine learning to achieve “super accurate results, and make it easy for developers to incorporate the API into their products. One customer is\\xa0CallRail, offering call tracking and marketing analytics software, which plans to incorporate\\xa0AssembyAI’s\\xa0API to gain insight into why people are calling. Other customers include NBC and the Wall Street\\xa0Journal, using the product to transcribe content and interviews, and provide closed captioning.\\xa0\\xa0\\n“We’ve been working on building as close to human speech recognition quality as possible. It’s been a lot of work” Fox said. He expects to reach that plateau in 2022.\\xa0\\xa0\\nHe targets companies incorporating speech recognition into their products and makes it easy to buy. Customers pay on a usage basis; for every second of audio transcribed,\\xa0AssemblyAI\\xa0charges a fraction of a penny. Clients get billed monthly. If a customer uses 10 hours a month, it costs about nine dollars. If a customer uses a million hours a month, it costs about $900,000.\\xa0\\xa0\\xa0\\xa0\\nVoice recognition is a hot market. “Many new startups are being launched,” Fox said, providing opportunity. “Many interesting new businesses are being built on voice data.”\\xa0\\xa0\\xa0\\nAssemblyAI’s\\xa0product can detect sensitive topics such as hate speech and profanity, so customers can save on human content moderation.\\xa0\\nAsked to describe what differentiates his technology, Fox said, “We are an experienced team of deep learning researchers,” with experience from companies including BMW, Apple,\\xa0and Facebook. “We build very large, very accurate deep learning models that have recognition results far more accurate than a traditional machine learning approach. We build really large models using advanced neural network technologies.” He compared the approach to what\\xa0OpenAI\\xa0uses to develop its GPT-3 large language model.\\xa0\\xa0\\nIn addition, they build AI features on top of the transcriptions, to provide summaries of audio and video content, which can be searched and indexed. “It goes beyond just transcription,” Fox said.\\xa0\\xa0\\xa0\\nThe company currently has 25 employees and expects to double in about four months. Business has been good. “There is an explosion of audio and video data online and customers want to be able to take advantage of it, so we see a lot of demand,” Fox said.\\xa0\\nLearn more at\\xa0AssemblyAI.\\xa0'},\n",
       " {'title': 'Pursuit of Autonomous Cars May Pose Risk of AI Tapping Forbidden Knowledge',\n",
       "  'content': 'By Lance Eliot, the AI Trends Insider\\xa0\\xa0\\xa0\\xa0\\nAre there things that we must not know?\\xa0\\xa0\\xa0\\nThis is an age-old question.\\xa0Some assert that there is the potential for knowledge that ought to not be known. In other words, there are ideas, concepts, or mental formulations that should we become aware of that knowledge it could be our downfall. The discovery or invention of some new innovation or way of thinking could be unduly dangerous. It would be best to not go there, as it were, and avoid ever landing on such knowledge: forbidden knowledge.\\xa0\\xa0\\xa0\\nThe typical basis for wanting to forbid the discovery or emergence of forbidden knowledge is that the adverse consequences are overwhelming.\\xa0The end result is so devastating and undercutting that the bad side outweighs the good that could be derived from the knowledge.\\xa0\\xa0\\xa0\\nIt is conceivable that there might be knowledge that is so bad that it has no good possibilities at all. Thus, rather than trying to balance or weigh the good versus the bad, the knowledge has no counterbalancing effects. It is just plain bad.\\xa0\\xa0\\xa0\\xa0\\nWe are usually faced with the matter of knowledge that has both the good and the bad as to how it might be utilized or employed. This then leads to a dogged debate about whether the bad is so bad that it outweighs the good. On top of this, there is the unrealized bad and the unrealized good, which could be differentiated from the realized bad and the realized good (in essence, the knowledge might be said to be either good or bad, though this is purely conceptual and not put into real-world conditions to attest or become realized as such).\\xa0\\xa0\\xa0\\nThe most familiar reference to forbidden knowledge is likely evoked via the Garden of Eden and the essence of forbidden fruit.\\xa0\\nA contemporary down-to-earth example often discussed about forbidden knowledge consists of the atomic bomb. Some suggest that the knowledge devised or invented to ultimately produce a nuclear bomb provides a quite visible and overt\\xa0exemplar\\xa0of the problems associated with knowledge. Had the knowledge about being able to attain an atomic bomb never been achieved, there presumably would not be any such device. In debates about the topic, it is feasible to take a resolute position favoring the attainment of an atomic bomb and there are equally counterbalancing contentions sternly disfavoring this attainment.\\xa0\\xa0\\xa0\\nOne perplexing problem about forbidden knowledge encompasses knowing beforehand the kind of knowledge that might end up in the forbidden category. This is a bit of a Catch-22 or circular type of puzzle. You might discover knowledge and then ascertain it ought to be forbidden, but the cat is kind of out of the bag due to the knowledge having been already uncovered or rendered. Oopsie, you should have in advance decided to not go there and therefore have avoided falling into the forbidden knowledge zone.\\xa0\\xa0\\xa0\\nOn a related twist, suppose that we could beforehand declare what type of knowledge is to be averted because it is predetermined as forbidden. Some people might accidentally discover the knowledge, doing so by happenstance, and now they’ve again potentially opened Pandora’s box. Meanwhile, there might be others that, regardless of being instructed to not derive any such stated forbidden knowledge, do so anyway.\\xa0\\xa0\\xa0\\nThis then takes us to a frequently used retort about forbidden knowledge, namely, if you don’t seek the forbidden knowledge there is a chance that someone else will, and you’ll be left in the dust because they got there first. In that preemptive viewpoint, the claim is that it is better to go ahead and forage for the forbidden knowledge and not get caught behind the eight-ball when someone else beats you to the punch.\\xa0\\xa0\\xa0\\nRound and round we can go.\\xa0\\xa0\\xa0\\nThe main thing that most would agree to is that knowledge is power.\\xa0\\nThe alluded to power could be devastating and destroy others, possibly even leading to the self-destruction of the wielder of the knowledge. Yet there is also the potential for knowledge to be advantageous and save humanity from other ills.\\xa0\\xa0\\xa0\\nMaybe we ought to say that knowledge is powerful. Despite that perhaps obvious proclamation, we might also add that knowledge can decay and gradually become outdated or less potent. Furthermore, since we are immersing ourselves herein into the cauldron of the love-it or hate-it knowledge conundrum, knowledge can be known and yet undervalued, perhaps only becoming valuable at a later time and in a different light.\\xa0\\xa0\\xa0\\nThere is a case to be made that humankind has a seemingly irresistible allure toward more and more knowledge. Some philosophers suggest you are unlikely to be able to bottle up or stop this quest for knowledge. If that’s the manner of how humanity will be, this implies that you must find ways to control or contain knowledge and give up on the belief that we can altogether avoid landing into forbidden knowledge.\\xa0\\xa0\\xa0\\nThere is a relatively new venue prompting a lot of anxious hand wringing pertaining to forbidden knowledge, namely the advent of Artificial Intelligence (AI).\\xa0\\xa0\\xa0\\nHere’s the rub.\\xa0\\xa0\\xa0\\nSuppose that we are able to craft AI systems that make use of knowledge about how humans can think. There are two major potential gotchas.\\xa0\\xa0\\xa0\\nFirst, the AI systems themselves might end up doing good things, and they also might end up doing bad things. If the bad outweighs the good, maybe we are shooting our own foot by allowing AI to be put into use.\\xa0\\nSecondly, perhaps this could be averted entirely by deciding that there is forbidden knowledge about how humans think, and we ought to not discover or reveal those mental mechanisms. It is the classic stepwise logic that step A axiomatically leads to step B. We won’t need to worry about AI systems (step B), if we never allow the achievement of step A (figuring out how humans think and then imparting that into computers), since the attainment of AI would presumably not arise.\\xa0\\xa0\\xa0\\nIn any case, there is inarguably a growing concern about AI.\\xa0\\xa0\\xa0\\nPlenty of efforts are underway to promulgate a semblance of AI Ethics, meaning that those developers and indeed all stakeholders that are conceiving of, building, and putting into use an AI system needs to consider the ethical aspects of their efforts. AI systems have been unveiled and placed into use replete with all sorts of notable concerns, including incorporating unsavory biases and other problems.\\xa0\\xa0\\xa0\\nAll told, one bold and somewhat stark argument is that the pursuit of AI is being underpinned or stoked by the discovery and then exploitation of forbidden knowledge.\\xa0\\xa0\\xa0\\nBe aware that many would scoff at this allegation.\\xa0\\xa0\\xa0\\nThere are those deeply immersed in the field of AI who would laugh that there is anything in the entirety of AI to date that constitutes potential forbidden knowledge. The technology and technological elements are relatively ho-hum, they would argue. You would be hard-pressed to pinpoint what AI-related knowledge that is already known comes anywhere near the ballpark of forbidden knowledge.\\xa0\\nFor those that concur with that posture, there is the reply that it might be future knowledge that we have not yet attained that is the upcoming forbidden kind, and for which we are heading pell-mell down that path. Thus, they would concede that we haven’t arrived at forbidden knowledge at this juncture, but this is an insidious distractor due to the aspect that it masks or belies our qualms entailing the possibility that it lays in wait at the next turn.\\xa0\\nOne area where AI is being actively used is to create Autonomous Vehicles (AVs).\\xa0\\nWe are gradually seeing the emergence of self-driving cars and can expect self-driving trucks, self-driving motorcycles, self-driving drones, self-driving planes, self-driving ships, self-driving submersibles, etc.\\xa0\\xa0\\xa0\\nToday’s conventional cars are eventually going to give way to the advent of AI-based, true self-driving cars. Self-driving cars are driven via an AI driving system. There isn’t a need for a human driver at the wheel, and nor is there a provision for a human to drive the vehicle.\\xa0\\xa0\\xa0\\nHere’s an intriguing question that has arisen:\\xa0Might the crafting of AI-based true self-driving cars take us into the realm of discovering forbidden knowledge, and if so, what should be done about this?\\xa0\\xa0\\xa0\\nBefore jumping into the details, I’d like to clarify what is meant when referring to true self-driving cars.\\xa0\\xa0\\xa0\\nFor my framework about AI autonomous cars, see the link here:\\xa0https://aitrends.com/ai-insider/framework-ai-self-driving-driverless-cars-big-picture/\\xa0\\xa0\\xa0\\nWhy this is a moonshot effort, see my explanation here:\\xa0https://aitrends.com/ai-insider/self-driving-car-mother-ai-projects-moonshot/\\xa0\\xa0\\xa0\\nFor more about the levels as a type of Richter scale, see my discussion here:\\xa0https://aitrends.com/ai-insider/richter-scale-levels-self-driving-cars/\\xa0\\xa0\\xa0\\nFor the argument about bifurcating the levels, see my explanation here:\\xa0https://aitrends.com/ai-insider/reframing-ai-levels-for-self-driving-cars-bifurcation-of-autonomy/\\xa0\\xa0\\xa0\\nUnderstanding The Levels Of Self-Driving Cars\\xa0\\xa0\\xa0\\nAs a clarification, true self-driving cars are ones where the AI drives the car entirely on its own and there isn’t any human assistance during the driving task.\\xa0\\xa0\\xa0\\nThese driverless vehicles are considered Level 4 and Level 5, while a car that requires a human driver to co-share the driving effort is usually considered at Level 2 or Level 3. The cars that co-share the driving task are described as being semi-autonomous, and typically contain a variety of automated add-on’s that are referred to as ADAS (Advanced Driver-Assistance Systems).\\xa0\\nThere is not yet a true self-driving car at Level 5, which we don’t yet even know if this will be possible to achieve, and nor how long it will take to get there.\\xa0\\nMeanwhile, the Level 4 efforts are gradually trying to get some traction by undergoing very narrow and selective public roadway trials, though there is controversy over whether this testing should be allowed per se (we are all life-or-death guinea pigs in an experiment taking place on our highways and byways, some contend).\\xa0\\xa0\\xa0\\nSince semi-autonomous cars require a human driver, the adoption of those types of cars won’t be markedly different from driving conventional vehicles, so there’s not much new per se to cover about them on this topic (though, as you’ll see in a moment, the points next made are generally applicable).\\xa0\\xa0\\nFor semi-autonomous cars, it is important that the public needs to be forewarned about a disturbing aspect that’s been arising lately, namely that despite those human drivers that keep posting videos of themselves falling asleep at the wheel of a Level 2 or Level 3 car, we all need to avoid being misled into believing that the driver can take away their attention from the driving task while driving a semi-autonomous car.\\xa0\\xa0\\xa0\\nYou are the responsible party for the driving actions of the vehicle, regardless of how much automation might be tossed into a Level 2 or Level 3.\\xa0\\xa0\\xa0\\nFor why remote piloting or operating of self-driving cars is generally eschewed, see my explanation here:\\xa0https://aitrends.com/ai-insider/remote-piloting-is-a-self-driving-car-crutch/\\xa0\\xa0\\xa0\\nTo be wary of fake news about self-driving cars, see my tips here:\\xa0https://aitrends.com/ai-insider/ai-fake-news-about-self-driving-cars/\\xa0\\nThe ethical implications of AI driving systems are significant, see my indication here:\\xa0http://aitrends.com/selfdrivingcars/ethically-ambiguous-self-driving-cars/\\xa0\\xa0\\xa0\\nBe aware of the pitfalls of normalization of deviance when it comes to self-driving cars, here’s my call to arms:\\xa0https://aitrends.com/ai-insider/normalization-of-deviance-endangers-ai-self-driving-cars/\\xa0\\xa0\\xa0\\nSelf-Driving Cars And Forbidden Knowledge\\xa0\\xa0\\xa0\\nFor Level 4 and Level 5 true self-driving vehicles, there won’t be a human driver involved in the driving task.\\xa0\\xa0\\xa0\\nAll occupants will be passengers.\\xa0\\xa0\\xa0\\nThe AI is doing the driving.\\xa0\\xa0\\xa0\\nOne aspect to immediately discuss entails the fact that the AI involved in today’s AI driving systems is not sentient. In other words, the AI is altogether a collective of computer-based programming and algorithms, and most assuredly not able to reason in the same manner that humans can.\\xa0\\xa0\\xa0\\nWhy this added emphasis about the AI not being sentient?\\xa0\\xa0\\xa0\\nBecause I want to underscore that when discussing the role of the AI driving system, I am not ascribing human qualities to the AI. Please be aware that there is an ongoing and dangerous tendency these days to anthropomorphize AI. In essence, people are assigning human-like sentience to today’s AI, despite the undeniable and inarguable fact that no such AI exists as yet.\\xa0\\nWith that clarification, you can envision that the AI driving system won’t natively somehow “know” about the facets of driving. Driving and all that it entails will need to be programmed as part of the hardware and software of the self-driving car.\\xa0\\xa0\\xa0\\nLet’s dive into the myriad of aspects that come to play on this topic.\\xa0\\xa0\\xa0\\nThe crux here is whether there is forbidden knowledge lurking within the existing and ongoing efforts to achieve AI-based true self-driving cars. We’ll begin by considering the status of the existent efforts and then shift into speculation about the future of such efforts.\\xa0\\xa0\\xa0\\nPer the earlier discussion about whether there is forbidden knowledge that has already perchance been revealed or discovered via the efforts toward today’s AI systems all told, the odds seem stacked against such a notion at this time, and likewise the same could be said about the pursuit of self-driving cars. Essentially, there doesn’t seem to be any forbidden knowledge per se that has been discovered or revealed during the self-driving cars development journey so far, at least with respect to the conventional wisdom about what forbidden knowledge might entail.\\xa0\\xa0\\xa0\\nOne could try to argue that it is premature to reach such a conclusion and that we might, later on, realize that forbidden knowledge was indeed uncovered or invented, and we just didn’t realize it. That is a rabbit hole that we’ll not go down for now, though you are welcome to keep that presumption at hand if so desired.\\xa0\\xa0\\xa0\\nThat covers the present, and ergo we can turn our attention to the future.\\xa0\\xa0\\xa0\\nGenerally, the efforts underway today have been primarily aimed at achieving Level 4, and the hope is that someday we will go beyond Level 4 and attain Level 5. To get to a robust Level 4, most would likely say that we can continue the existing approaches.\\xa0\\xa0\\xa0\\nNot everyone would agree with that assumption. Some believe that we will get stymied within Level 4. Furthermore, the inability to produce a robust Level 4 will ostensibly preclude us from being able to attain Level 5. There is a contingent that suggests we need to start over and set aside the existing AI approaches, which otherwise are taking us down a dead-end or blind alley. An entirely new way of devising AI for autonomous vehicles is needed, they would vehemently argue.\\xa0\\xa0\\xa0\\nThere is also a contingent that asserts the Level 4 itself is a type of dead-end. In brief, those proponents would say that we will achieve a robust Level 4, though this will do little good towards attaining Level 5. Once again, their view is similar to the preceding remark that we will need to come up with some radically new understandings about AI and the nature of cognitive acumen in order to get self-driving cars into the Level 5 realm.\\xa0\\xa0\\xa0\\nAha, it is within that scope of having to dramatically revisit and revamp what AI is and how we can advance significantly in the pursuit of AI that the forbidden knowledge question can reside. In theory, perhaps the only means of attaining Level 5 will be to strike upon some knowledge that we do not yet know and that for which bodes for falling within the realm of forbidden knowledge.\\xa0\\nTo some, this seems\\xa0farfetched.\\xa0\\nThey would emphatically ask; just what kind of knowledge are you even talking about?\\xa0\\xa0\\xa0\\nHere’s their logic. Humans are able to drive cars. Humans do not seem to need or possess forbidden knowledge as it relates to the act of driving a car. Therefore, it seems ridiculous on the face of things to claim or contend that the only means to get AI-based true self-driving cars, for which they would be driven on an equal basis as human drivers can drive, would require the discovery or invention of whatever might be construed as forbidden knowledge.\\xa0\\xa0\\xa0\\nSeems like pretty ironclad logic.\\xa0\\xa0\\xa0\\nThe retort is that humans have common-sense reasoning. With common-sense reasoning, we seem to know all sorts of things about the world around us. When we drive a car, we intrinsically make use of our common-sense reasoning. We take for granted that we do have a common-sense reasoning capacity, and similarly, we take for granted that it integrally comes to the fore when driving a car.\\xa0\\xa0\\xa0\\nAttempts to create AI that can exhibit the equivalent of human common-sense reasoning have made ostensibly modest or some would say minimal progress (to clarify, those pursuing this line of inquiry are to be lauded, it’s just that no earth-shattering breakthroughs seem to have been reached and none seem on the immediate horizon). Yes, there are some quite fascinating and exciting efforts underway, but when you measure those against the everyday common-sense reasoning of humans, there is no comparison. They are night and day. If this were a contest, the humans win hands down, no doubt about it, and the AI experimental efforts encompassing common-sense reasoning are mere playthings in contrast.\\xa0\\xa0\\xa0\\nYou might have gleaned where this line of thought is headed.\\xa0\\xa0\\xa0\\nThe belief by some is that until we crack open the enigma of common-sense reasoning, there is little chance of achieving a Level 5, and perhaps also this will hold back the Level 4 too. It could be that a secret ingredient of sorts for autonomous vehicles is the need to figure out and include common-sense reasoning into AI-based driving and piloting systems.\\xa0\\xa0\\xa0\\nIf you buy into that logic, the added assertion is that maybe within the confines of how common-sense reasoning takes place is a semblance of forbidden knowledge. On the surface, you would certainly assume that if we knew entirely how common-sense reasoning works, there would not appear to be any cause for alarm or concern. The act of employing common-sense reasoning does not seem to necessarily embody forbidden knowledge.\\xa0\\xa0\\xa0\\nThe twist is that perhaps the underlying cognitive means that gives rise to the advent of common-sense reasoning is where there is forbidden knowledge. Some deep-rooted elements in the nature of human thought and how we form common sense and undertake common-sense reasoning are possibly a type of knowledge that will be shown as crucial and a forbidden knowledge formulation.\\xa0\\xa0\\xa0\\nFor more details about ODDs, see my indication at this link here:\\xa0https://www.aitrends.com/ai-insider/amalgamating-of-operational-design-domains-odds-for-ai-self-driving-cars/\\xa0\\nOn the topic of off-road self-driving cars, here’s my details elicitation:\\xa0https://www.aitrends.com/ai-insider/off-roading-as-a-challenging-use-case-for-ai-autonomous-cars/\\xa0\\nI’ve urged that there must be a Chief Safety Officer at self-driving car makers, here’s the scoop:\\xa0https://www.aitrends.com/ai-insider/chief-safety-officers-needed-in-ai-the-case-of-ai-self-driving-cars/\\xa0\\nExpect that lawsuits are going to gradually become a significant part of the self-driving car industry, see my explanatory details here:\\xa0http://aitrends.com/selfdrivingcars/self-driving-car-lawsuits-bonanza-ahead/\\xa0\\xa0\\xa0\\nConclusion\\xa0\\xa0\\xa0\\nWow, that’s quite a bit of pondering, contemplation, and (some would say) wild thinking.\\xa0\\xa0\\xa0\\nMaybe so, but it is a consideration that some would wish that we gave at least some credence toward and devoted attention to. There is the angst that we might find ourselves by happenstance stumbling into forbidden knowledge on these voracious self-driving cars quests.\\xa0\\xa0\\xa0\\nFor however you might emphasize that having AI-based true self-driving cars will be a potential blessing, proffering mobility-for-all and leading to reducing the number of car crash-related fatalities, there is a sneaking suspicion that it will not be all-good. The catch or trap could be that there is some kind of forbidden knowledge that will get brought to the eye and we will inevitably kick ourselves that we didn’t see it coming.\\xa0\\xa0\\xa0\\nThe next time you are munching on a delicious apple, give some thought to whether self-driving cars might be forbidden fruit.\\xa0\\xa0\\xa0\\nWe are on the path to taking a big bite, and we’ll have to see where that takes us.\\xa0\\nCopyright 2021 Dr. Lance Eliot\\xa0\\xa0\\nhttp://ai-selfdriving-cars.libsyn.com/website\\xa0'}]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "74601bbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_file = os.path.join(\"articles.json\") # save articles to a JSON file in the machine \n",
    "feed = open(save_file, \"w+\" )\n",
    "feed.write(json.dumps(articles, indent=1))\n",
    "feed.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "cd08cc69",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'title': 'Best Practices for Building the AI Development Platform in Government',\n",
       "  'content': 'By John P. Desmond, AI Trends Editor\\xa0\\nThe AI stack defined by Carnegie Mellon University is fundamental to the approach being taken by the US Army for its AI development platform efforts, according to Isaac Faber, Chief Data Scientist at the US Army AI Integration Center, speaking at the\\xa0AI World Government\\xa0event held in-person and virtually from Alexandria, Va., last week.\\xa0\\xa0\\nIsaac Faber, Chief Data Scientist, US Army AI Integration Center\\n“If we want to move the Army from legacy systems through digital modernization, one of the biggest issues I have found is the difficulty in abstracting away the differences in applications,” he said. “The most important part of digital transformation is the middle layer, the platform that makes it easier to be on the cloud or on a local computer.” The desire is to be able to move your software platform to another platform, with the same ease\\xa0with\\xa0which a new smartphone carries over the user’s contacts and histories.\\xa0\\xa0\\nEthics cuts across all layers of the AI application stack, which positions the planning stage at the top, followed by decision support, modeling, machine learning, massive data management and the device layer or platform at the bottom.\\xa0\\xa0\\n“I am advocating that we think of the stack as a core infrastructure and a way for applications to be deployed and not to be siloed in our approach,” he said. “We need to create a development environment for a globally-distributed workforce.”\\xa0\\xa0\\xa0\\nThe Army has been working on a Common Operating Environment Software (Coes) platform, first announced in 2017, a design for DOD work that is scalable, agile, modular, portable and open. “It is suitable for a broad range of AI projects,” Faber said. For executing the effort, “The devil is in the details,” he said.\\xa0\\xa0\\xa0\\nThe Army is working with CMU and private companies on a prototype platform, including with\\xa0Visimo\\xa0of Coraopolis, Pa., which offers AI development services. Faber said he prefers to collaborate and coordinate with private industry rather than buying products off the shelf. “The problem with that is, you are stuck with the value you are being provided by that one vendor, which is usually not designed for the challenges of DOD networks,” he said.\\xa0\\xa0\\nArmy Trains a Range of Tech Teams in AI\\xa0\\nThe Army engages in AI workforce development efforts for several teams, including:\\xa0 leadership, professionals with graduate degrees; technical staff, which is put through training to get certified; and AI users.\\xa0\\xa0\\xa0\\nTech teams in the Army have different areas of focus include: general purpose software development, operational data science, deployment which includes analytics, and a machine learning operations team, such as a large team required to build a computer vision system. “As folks come through the workforce, they need a place to collaborate, build and share,” Faber said.\\xa0\\xa0\\xa0\\nTypes of projects include diagnostic, which might be combining streams of historical data, predictive and prescriptive, which recommends a course of action based on a prediction. “At the far end is AI; you don’t start with that,” said Faber. The developer has to solve three problems: data engineering, the AI development platform, which he called “the green bubble,” and the deployment platform, which he called “the red bubble.”\\xa0\\xa0\\xa0\\n“These are mutually exclusive and all interconnected. Those teams of different people need to programmatically coordinate. Usually a good project team will have people from each of those bubble areas,” he said. “If you have not done this yet, do not try to solve the green bubble problem. It makes no sense to pursue AI until you have an operational need.”\\xa0\\xa0\\xa0\\nAsked by a participant which group is the most difficult to reach and train, Faber said without hesitation, “The hardest to reach are the executives. They need to learn what the value is to be provided by the AI ecosystem. The biggest challenge is how to communicate that value,” he said.\\xa0\\xa0\\xa0\\nPanel Discusses AI Use Cases with the Most Potential\\xa0\\xa0\\nIn a panel on Foundations of Emerging AI, moderator Curt Savoie, program director, Global Smart Cities Strategies for IDC, the market research firm, asked what emerging AI use case has the most potential.\\xa0\\xa0\\nJean-Charles Lede, autonomy tech advisor for the US Air Force, Office of Scientific Research, said,” I would point to decision advantages at the edge, supporting pilots and operators, and decisions at the back, for mission and resource planning.”\\xa0\\xa0\\xa0\\nKrista Kinnard, Chief of Emerging Technology for the Department of Labor\\nKrista Kinnard, Chief of Emerging Technology for the Department of Labor, said, “Natural language processing is an opportunity to open the doors to AI in the Department of Labor,” she said. “Ultimately, we are dealing with data on people, programs,\\xa0and organizations.”\\xa0\\xa0\\xa0\\xa0\\nSavoie asked what are the big risks and dangers the panelists see when implementing AI.\\xa0\\xa0\\xa0\\nAnil Chaudhry, Director of Federal AI Implementations for the General Services Administration (GSA), said in a typical IT organization using traditional software development, the impact of a decision by a developer only goes so far. With AI, “You have to consider the impact on a whole class of people, constituents,\\xa0and stakeholders. With a simple change in algorithms, you could be delaying benefits to millions of people or making incorrect inferences at scale. That’s the most important risk,” he said.\\xa0\\xa0\\nHe said he asks his contract partners to have “humans in the loop and humans on the loop.”\\xa0\\xa0\\xa0\\nKinnard seconded this, saying, “We have no intention of removing humans from the loop. It’s really about empowering people to make better decisions.”\\xa0\\xa0\\xa0\\nShe emphasized the importance of monitoring the AI models after they are deployed. “Models can drift as the data underlying the changes,” she said. “So you need a level of critical thinking to not only do the task, but to assess whether what the AI model is doing is acceptable.”\\xa0\\xa0\\xa0\\nShe added, “We have built out use cases and partnerships across the government to make sure we’re implementing responsible AI. We will never replace people with algorithms.”\\xa0\\xa0\\nLede of the Air Force said, “We often have use cases where the data does not exist. We cannot explore 50 years of war data, so we use simulation. The risk is in teaching an algorithm that you have a ‘simulation to real gap’ that is a real risk. You are not sure how the algorithms will map to the real world.”\\xa0\\xa0\\nChaudhry emphasized the importance of a testing strategy for AI systems. He warned of developers “who get enamored with a tool and forget the purpose of the exercise.” He recommended the development manager design in independent verification and validation strategy. “Your testing, that is where you have to focus your energy as a leader. The leader needs an idea in mind, before committing resources, on how they will justify whether the investment was a success.”\\xa0\\xa0\\xa0\\nLede of the Air Force talked about the importance of\\xa0explainability. “I am a technologist. I don’t\\xa0do laws. The ability for the AI function to explain in a way a human can interact with, is important. The AI is a partner that we have a dialogue with, instead of the AI coming up with a conclusion that we have no way of verifying,” he said.\\xa0\\xa0\\nLearn more at\\xa0AI World Government.\\xa0'},\n",
       " {'title': 'Advance Trustworthy AI and ML, and Identify Best Practices for Scaling AI',\n",
       "  'content': 'By John P. Desmond, AI Trends Editor\\xa0\\xa0\\nAdvancing trustworthy AI and machine learning to mitigate agency risk is a priority for the US Department of Energy (DOE), and identifying best practices for implementing AI at scale is a priority for the US General Services Administration (GSA).\\xa0\\xa0\\nThat’s what attendees learned in two sessions at the\\xa0AI World Government\\xa0live and virtual event held in Alexandria, Va. last week.\\xa0\\xa0\\xa0\\nPamela Isom, Director of the AI and Technology Office, DOE\\nPamela Isom, Director of the AI and Technology Office at the DOE, who spoke on Advancing Trustworthy AI and ML Techniques for Mitigating Agency Risks, has been involved in proliferating the use of AI across the agency for several years. With an emphasis on applied AI and data science, she oversees risk mitigation policies and standards and has been involved with applying AI to save lives, fight fraud, and strengthen the cybersecurity infrastructure.\\xa0\\xa0\\nShe emphasized the need for the AI project effort to be part of a strategic portfolio. “My office is there to drive a holistic view on AI and to mitigate risk by bringing us together to address challenges,” she said. The effort is assisted by the DOE’s AI and Technology Office, which is focused on transforming the DOE into a world-leading AI enterprise by accelerating research, development, delivery and the adoption of AI.\\xa0\\xa0\\n“I am telling my organization to be mindful of the fact that you can have tons and tons of data, but it might not be representative,” she said. Her team looks at examples from international partners, industry, academia and other agencies for outcomes “we can trust” from systems incorporating AI.\\xa0\\xa0\\n“We know that AI is disruptive, in trying to do what humans do and do it better,” she said. “It is beyond human capability; it goes beyond data in spreadsheets; it can tell me what I’m going to do next before I contemplate it myself. It’s that powerful,” she said.\\xa0\\xa0\\nAs a result, close attention must be paid to data sources. “AI is vital to the economy and our national security. We need precision; we need algorithms we can trust; we need accuracy. We don’t need biases,” Isom said, adding, “And don’t forget that you need to monitor the output of the models long after they have been deployed.”\\xa0\\xa0\\xa0\\nExecutive Orders Guide GSA AI Work\\xa0\\nExecutive Order 14028, a detailed set of actions to address the cybersecurity of government agencies, issued in May of this year, and Executive Order 13960, promoting the use of trustworthy AI in the Federal government, issued in December 2020, provide valuable guides to her work.\\xa0\\xa0\\xa0\\nTo help manage the risk of AI development and deployment, Isom has produced the AI Risk Management Playbook, which provides guidance around system features and mitigation techniques. It also has a filter for ethical and trustworthy principles which are considered throughout AI lifecycle stages and risk types. Plus, the playbook ties to relevant Executive Orders.\\xa0\\xa0\\nAnd it provides examples, such as your results came in at 80%\\xa0accuracy, but you wanted 90%. “Something is wrong there,” Isom said, adding, “The playbook helps you look at these types of problems and what you can do to mitigate risk, and what factors you should weigh as you design and build your project.”\\xa0\\xa0\\nWhile internal to DOE at present, the agency is looking into next steps for an external version. “We will share it with other federal agencies soon,” she said.\\xa0\\xa0\\xa0\\nGSA Best Practices for Scaling AI Projects Outlined\\xa0\\xa0\\nAnil Chaudhry, Director of Federal AI Implementations, AI Center of Excellence (CoE), GSA\\nAnil Chaudhry, Director of Federal AI Implementations for the AI Center of Excellence (CoE) of the GSA, who spoke on Best Practices for Implementing AI at Scale, has over 20 years of experience in technology delivery, operations and program management in the defense, intelligence and national security sectors.\\xa0\\xa0\\xa0\\nThe mission of the\\xa0CoE\\xa0is to accelerate technology modernization across the government, improve the public experience and increase operational efficiency. “Our business model is to partner with industry subject matter experts to solve problems,” Chaudhry said, adding, “We are not in the business of recreating industry solutions and duplicating them.”\\xa0\\xa0\\xa0\\nThe\\xa0CoE\\xa0is providing recommendations to partner agencies and working with them to implement AI systems as the federal government engages heavily in AI development. “For AI, the government landscape is vast. Every federal agency has some sort of AI project going on right now,” he said, and the maturity of AI experience varies widely across agencies.\\xa0\\xa0\\nTypical use cases he is seeing include having AI focus on increasing speed and efficiency, on cost savings and cost avoidance, on improved response time and increased quality and compliance. As one best practice, he recommended the agencies\\xa0vet their commercial experience\\xa0with the large datasets they will encounter in government.\\xa0\\xa0\\xa0\\n“We’re talking petabytes and exabytes here, of structured and unstructured data,” Chaudhry said.\\xa0[Ed. Note: A petabyte is 1,000 terabytes.]\\xa0“Also ask industry partners about their strategies and processes on how they do macro and micro trend analysis, and what their experience has been in the deployment of bots such as in Robotic Process Automation, and how they demonstrate sustainability as a result of drift of data.”\\xa0\\xa0\\xa0\\nHe also asks potential industry partners to\\xa0describe the AI talent on their team\\xa0or what talent they can access. If the company is weak on AI talent, Chaudhry would ask, “If you buy something, how will you know you got what you wanted when you have no way of evaluating it?”\\xa0\\xa0\\nHe added, “A best practice in implementing AI is defining how you train your workforce to leverage AI tools, techniques and practices, and to define how you grow and mature your workforce. Access to talent leads to either success or failure in AI projects, especially when it comes to scaling a pilot up to a fully deployed system.”\\xa0\\xa0\\nIn another best practice, Chaudhry recommended examining the industry partner’s\\xa0access to financial capital.\\xa0“AI is a field where the flow of capital is highly volatile. “You cannot predict or project that you will spend X amount of dollars this year to get where you want to be,” he said, because an AI development team may need to explore another hypothesis, or clean up some data that may not be transparent or is potentially biased. “If you don’t have access to funding, it is a risk your project will fail,” he said.\\xa0\\xa0\\nAnother best practice is\\xa0access to logistical capital, such as the data\\xa0 that sensors collect for an AI IoT system. “AI requires an enormous amount of data that is authoritative and timely. Direct access to that data is critical,” Chaudhry said. He recommended that data sharing agreements\\xa0 be in place with organizations relevant to the AI system. “You might not need it right away, but having access to the data, so you could immediately use it and to have thought through the privacy issues before you need the data, is a good practice for scaling AI programs,” he said.\\xa0\\xa0\\xa0\\nA final best practice is planning of\\xa0physical infrastructure,\\xa0such as data center space. “When you are in a pilot, you need to know how much capacity you need to reserve at your data center, and how many end points you need to manage” when the application scales up, Chaudhry said, adding, “This all ties back to access to capital and all the other best practices.“\\xa0\\nLearn more at\\xa0AI World Government.\\xa0'},\n",
       " {'title': 'Promise and Perils of Using AI for Hiring: Guard Against Data Bias',\n",
       "  'content': 'By AI Trends Staff\\xa0\\xa0\\nWhile\\xa0AI in hiring is now widely used for writing job descriptions, screening candidates, and automating interviews, it poses a risk of wide discrimination if not implemented carefully.\\xa0\\nKeith Sonderling, Commissioner, US Equal Opportunity Commission\\nThat was the message from Keith Sonderling, Commissioner with the US Equal Opportunity Commision, speaking at the\\xa0AI World Government\\xa0event held live and virtually in Alexandria, Va., last week. Sonderling is responsible for enforcing federal laws that prohibit discrimination against job applicants because of race, color, religion, sex, national origin, age or\\xa0disability.\\xa0\\xa0\\xa0\\n“The thought that AI would become mainstream in HR departments was closer to science fiction two year ago, but the pandemic has accelerated the rate at which AI is being used by employers,” he said. “Virtual recruiting is now here to stay.”\\xa0\\xa0\\nIt’s a busy time for HR professionals. “The great resignation is leading to the great rehiring, and AI will play a role in that like we have not seen before,” Sonderling said.\\xa0\\xa0\\nAI has been employed for years in hiring—“It did not happen overnight.”—for tasks including chatting with applications, predicting whether a candidate would take the job, projecting what type of employee they would be and mapping out upskilling and reskilling opportunities. “In short, AI is now making all the decisions once made by HR personnel,” which he did not characterize as good or bad.\\xa0\\xa0\\xa0\\n“Carefully designed and properly used, AI has the potential to make the workplace more fair,” Sonderling said. “But carelessly implemented, AI could discriminate on a scale we have never seen before by an HR professional.”\\xa0\\xa0\\nTraining Datasets for AI Models Used for Hiring Need to Reflect Diversity\\xa0\\xa0\\nThis is because AI models rely on training data. If the company’s current workforce is used as the basis for training, “It will replicate the status quo. If it’s one gender or one race primarily, it will replicate that,” he said. Conversely, AI can help mitigate risks of hiring bias by race, ethnic background, or disability status. “I want to see AI improve on workplace discrimination,” he said.\\xa0\\xa0\\nAmazon began building a hiring application in 2014, and found over time that it discriminated against women in its recommendations, because the AI model was trained on a dataset of the company’s own hiring record for the previous 10 years, which was primarily of males. Amazon developers tried to correct it but ultimately scrapped the system in 2017.\\xa0\\xa0\\xa0\\nFacebook has recently agreed to pay $14.25 million to settle civil claims by the US government that the social media company discriminated against American workers and violated federal recruitment rules, according to an account from\\xa0Reuters. The case centered on Facebook’s use of what it called its PERM program for labor certification. The government found that Facebook refused to hire American workers for jobs that had been reserved for temporary visa holders under the PERM program.\\xa0\\xa0\\xa0\\n“Excluding people from the hiring pool is a violation,” Sonderling said.\\xa0 If the AI program “withholds the existence of the job opportunity to that\\xa0class, so\\xa0they cannot exercise their rights, or if it downgrades a protected class, it is within our domain,” he said.\\xa0\\xa0\\xa0\\nEmployment assessments, which became more common after World War II, have provided\\xa0 high value to HR managers and with help from AI they have the potential to minimize bias in hiring. “At the same time, they are vulnerable to claims of discrimination, so\\xa0employers\\xa0need to be careful and cannot take a hands-off approach,” Sonderling said. “Inaccurate data will amplify bias in decision-making. Employers must be vigilant against discriminatory outcomes.”\\xa0\\xa0\\nHe recommended researching solutions from vendors who vet data for risks of bias on the basis of race, sex, and other factors.\\xa0\\xa0\\xa0\\nOne example is from\\xa0HireVue\\xa0of South Jordan, Utah, which has\\xa0built a\\xa0hiring platform predicated on the US Equal Opportunity Commission’s Uniform Guidelines, designed specifically to mitigate unfair hiring practices, according to an account from\\xa0allWork.\\xa0\\xa0\\xa0\\nA post on AI ethical principles on its website states in part, “Because\\xa0HireVue\\xa0uses AI technology in our products, we actively work to prevent the introduction or\\xa0propagation\\xa0of bias against any group or individual. We will continue to carefully review the datasets we use in our work and ensure that they are as accurate and diverse as possible. We also continue to advance our abilities to monitor, detect, and mitigate bias. We strive to build teams from diverse backgrounds with diverse knowledge, experiences, and perspectives to best represent the people our systems serve.”\\xa0\\xa0\\nAlso, “Our data scientists and IO psychologists build\\xa0HireVue\\xa0Assessment algorithms in a way that removes data from consideration by the algorithm that contributes to adverse impact without significantly impacting the assessment’s predictive accuracy. The result is a highly valid, bias-mitigated assessment that helps to enhance human decision making while actively promoting diversity and equal opportunity regardless of gender, ethnicity, age, or disability status.”\\xa0\\xa0\\nDr. Ed\\xa0Ikeguchi, CEO,\\xa0AiCure\\nThe issue of bias in datasets used to train AI models is not confined to hiring. Dr. Ed\\xa0Ikeguchi, CEO of\\xa0AiCure, an AI analytics company working in the life sciences industry, stated in a recent account in\\xa0HealthcareITNews, “AI is only as strong as the data it’s fed, and lately that data backbone’s credibility is being increasingly called into question. Today’s AI developers lack access to large, diverse data sets on which to train and validate new tools.”\\xa0\\xa0\\nHe added, “They often need to leverage open-source datasets, but many of these were trained using computer programmer volunteers, which is a predominantly white population. Because algorithms are often trained on single-origin data samples with limited diversity, when applied in real-world scenarios to a broader population of different races, genders, ages, and more, tech that appeared highly accurate in research may prove unreliable.”\\xa0\\nAlso, “There needs to be an element of governance and peer review for all algorithms, as even the most solid and tested algorithm is bound to have unexpected results arise. An algorithm is never done learning—it must be constantly developed and fed more data to improve.”\\xa0\\nAnd, “As an industry, we need to become more skeptical of AI’s conclusions and encourage transparency in the industry. Companies should readily answer basic questions, such as ‘How was the algorithm trained? On what basis did it draw this conclusion?”\\xa0\\nRead the source articles and information at\\xa0AI World Government, from\\xa0Reuters\\xa0and from\\xa0HealthcareITNews.\\xa0'},\n",
       " {'title': 'Predictive Maintenance Proving Out as Successful AI Use Case',\n",
       "  'content': 'By John P. Desmond, AI Trends Editor\\xa0\\xa0\\nMore companies are successfully exploiting predictive maintenance systems that combine AI and IoT sensors to collect data that anticipates breakdowns and recommends preventive action before break or machines fail, in a demonstration of an AI use case with proven value.\\xa0\\xa0\\nThis growth is reflected in optimistic market forecasts. The predictive maintenance market is sized at $6.9 billion today and is projected to grow to $28.2 billion by 2026, according to a report from\\xa0IoT Analytics\\xa0of Hamburg, Germany. The firm\\xa0counts over 280 vendors offering solutions in the market today, projected to grow to over 500 by 2026.\\xa0\\xa0\\nFernando Bruegge, Analyst, IoT Analytics, Hamburg, Germany\\n“This research is a wake-up call to those that claim IoT is failing,” stated analyst Fernando Bruegge, author of the report, adding, “For companies that own industrial assets or sell equipment, now is the time to invest in predictive maintenance-type solutions.” And, “Enterprise technology firms need to prepare to integrate predictive maintenance solutions into their offerings,” Bruegge suggested.\\xa0\\xa0\\nHere is a review of some specific experience with predictive maintenance systems that combine AI and IoT sensors.\\xa0\\nAircraft engine manufacturer\\xa0Rolls-Royce\\xa0is\\xa0deploying predictive analytics\\xa0to help reduce the amount of carbon its engines produce, while also optimizing maintenance to help customers keep planes in the air longer, according to a recent account in\\xa0CIO.\\xa0\\nRolls-Royce built an Intelligent Engine platform to monitor engine flight, gathering data on weather conditions and how pilots are flying. Machine learning is applied to the data to customize maintenance regimes for individual engines.\\xa0\\nStuart Hughes, chief information and digital officer, Rolls-Royce\\n“We’re tailoring our maintenance regimes to make sure that we’re optimizing for the life an engine has, not the life the manual says it should have,” stated Stuart Hughes, chief information and digital officer at Rolls-Royce. “It’s truly variable service, looking at each engine as an individual engine.”\\xa0\\nCustomers are seeing less service interruption. “Rolls-Royce has been monitoring engines and charging per hour for at least 20 years,” Hughes stated. “That part of the business isn’t new. But as we’ve evolved, we’ve begun to treat the engine as a singular engine. It’s much more about the personalization of that engine.”\\xa0\\xa0\\nPredictive analytics is being applied in healthcare as well as in the manufacturing industry. Kaiser Permanente, the integrated managed care consortium based in Oakland, Calif. Is using predictive analytics to identify non-intensive care unit (ICU) patients at risk of rapid deterioration.\\xa0\\xa0\\xa0\\nWhile non-ICU patients that require unexpected transfers to the ICU constitute less than\\xa04%\\xa0of the total hospital population, they account for 20%\\xa0of all hospital deaths, according to Dr. Gabriel Escobar, research scientist, Division of Research, and regional director, Hospital Operations Research, Kaiser Permanente Northern California.\\xa0\\nKaiser Permanente Practicing Predictive Maintenance in Healthcare\\xa0\\nKaiser Permanente developed the Advanced Alert Monitor (AAM) system, leveraging three predictive analytic models to analyze more than 70 factors in a given patient’s electronic health record to generate a composite risk score.\\xa0\\n“The AAM system synthesizes and analyzes vital statistics, lab results, and other variables to generate hourly deterioration risk scores for adult hospital patients in the medical-surgical and transitional care units,” stated Dick Daniels, executive vice president and CIO of Kaiser Permanente in the CIO account. “Remote hospital teams evaluate the risk scores every hour and notify rapid response teams in the hospital when potential deterioration is detected. The rapid response team conducts bedside evaluation of the patient and calibrates the course treatment with the hospitalist.”\\xa0\\nIn advice to other practitioners, Daniels recommended a focus on how the tool will be fit into the workflow of health care teams. “It took us about five years to perform the initial mapping of the electronic medical record backend and develop the predictive models,” Daniels stated. “It then took us another two to three years to transition these models into a live web services application that could be used operationally.”\\xa0\\nIn an example from the food industry, a PepsiCo Frito-Lay plant in Fayetteville, Tenn. is using predictive maintenance successfully, with year-to-date equipment downtime at 0.75%\\xa0and unplanned downtime at 2.88%, according to Carlos Calloway, the site’s reliability engineering manager, in an account in\\xa0PlantServices.\\xa0\\nExamples of monitoring include: vibration readings confirmed by ultrasound helped to prevent a PC combustion blower motor from failing and shutting down the whole potato chip department; infrared analysis of the main pole for the plant’s GES automated warehouse detected a hot fuse holder, which helped to avoid a shutdown of the entire warehouse; and increased acid levels were detected in oil samples from a baked extruder gearbox, indicating oil degradation, which enabled prevention of a shutdown of Cheetos Puffs production.\\xa0\\nThe Frito-Lay plant produces more than 150 million pounds of product per year, including Lays, Ruffles, Cheetos, Doritos, Fritos, and Tostitos.\\xa0\\xa0\\nThe types of monitoring include vibration analysis, used on mechanical applications, which is processed with the help of a third-party company which sends alerts to the plant for investigation and resolution.\\xa0Another service partner performs quarterly vibration monitoring on selected equipment. All motor control center rooms and electrical panels are monitored with quarterly infrared analysis, which is also used on electrical equipment, some rotating equipment, and heat exchangers. In addition, the plant has done ultrasonic monitoring for more than 15 years, and it is “kind of like the pride and joy of our site from a predictive standpoint,” stated Calloway.\\xa0\\xa0\\nThe plan has a number of products in place from UE Systems of Elmsford, NY, supplier of ultrasonic instruments, hardware and software, and training for predictive maintenance.\\xa0\\xa0\\xa0\\nLouisiana Alumina Plant Automating Bearing Maintenance\\xa0\\xa0\\xa0\\nBearings, which wear over time under varying conditions of weather and temperature in the case of automobiles, are a leading candidate for IoT monitoring and predictive maintenance with AI. The\\xa0Noranda Alumina\\xa0plant in Gramercy, La. is finding a big payoff from its investment in a system to improve the lubrication of bearings in its production equipment.\\xa0\\xa0\\nThe system has resulted in a 60%\\xa0decline in bearing changes in the second year of using the new lubrication system, translating to some $900,000 in savings on bearings that did not need to be replaced and avoided downtime.\\xa0\\xa0\\n“Four hours of downtime is about $1 million dollars’ worth of lost production,” stated Russell Goodwin, a reliability engineer and millwright instructor at Noranda Alumina, in the\\xa0PlantServices\\xa0account, which was based on presentations at the Leading Reliability 2021 event.\\xa0\\nThe Noranda Alumina plant is the only alumina plant operating in the US.\\xa0“If we shut down, you’ll need to import it,” stated Goodwin. The plant experiences pervasive dust, dirt, and caustic substances, which complicate efforts at improved reliability and maintenance practices.\\xa0\\xa0\\nNoranda Alumina tracks all motors and gearboxes at 1,500 rpm and higher with vibration readings, and most below 1,500 with ultrasound. Ultrasonic monitoring, of sound in ranges beyond human hearing, was introduced to the plant after Goodwin joined the company in 2019. At the time, grease monitoring had room for improvement. “If grease was not visibly coming out of the seal, the mechanical supervisor did not count the round as complete,” stated Goodwin.\\xa0\\xa0\\nAfter introducing automation, the greasing system has improved dramatically, he stated. The system was also able to detect bearings in a belt whose bearings were wearing out too quickly due to contamination. “Tool-enabled tracking helped to prove that it wasn’t improper greasing, but rather the bearing was made improperly,” stated Goodwin.\\xa0\\xa0\\nRead the source articles and information in\\xa0\\xa0IoT Analytics,\\xa0in\\xa0CIO\\xa0and in\\xa0PlantServices.\\xa0'},\n",
       " {'title': 'Novelty In The Game Of Go Provides Bright Insights For AI And Autonomous Vehicles',\n",
       "  'content': 'By Lance Eliot, the AI Trends Insider\\xa0\\xa0\\nWe already\\xa0expect that humans to exhibit flashes of brilliance.\\xa0It might not happen all the time, but the act itself is welcomed and not altogether disturbing when\\xa0it occurs.\\xa0\\xa0\\xa0\\nWhat about when Artificial Intelligence (AI) seems to display an act of novelty?\\xa0Any such instance is bound to get our attention; questions arise right away.\\xa0\\xa0\\xa0\\nHow did the AI come up with the apparent out-of-the-blue insight or novel indication? Was it a mistake, or did it fit within the parameters of what the AI was expected to produce? There is also the immediate consideration of whether the AI somehow is slipping toward the precipice of becoming sentient.\\xa0\\xa0\\xa0\\nPlease be aware that no AI system in existence is anywhere close to reaching sentience, despite the\\xa0claims and falsehoods tossed around in the media. As such, if today’s AI seems to do something that appears to be a novel act, you should not leap to the conclusion that this is a sign of human insight within technology or the emergence of human ingenuity among AI.\\xa0\\xa0\\xa0\\nThat’s an anthropomorphic bridge too far.\\xa0\\xa0\\xa0\\nThe reality is that any such AI “insightful” novelties are based on various concrete computational algorithms and tangible data-based pattern matching.\\xa0\\xa0\\xa0\\nIn today’s column, we’ll be taking a close look at an example of an AI-powered novel act, illustrated via the game of Go, and relate these facets to the advent of AI-based true self-driving cars as a means of understanding the AI-versus-human related ramifications.\\xa0\\nRealize that the capacity to spot or suggest a novelty is being done methodically by an AI system, while, in contrast, no one can say for sure how humans can devise novel thoughts or intuitions.\\xa0\\nPerhaps we too are bound by some internal mechanistic-like facets, or maybe there is something else going on. Someday, hopefully, we will crack open the secret inner workings of the mind and finally know how we think. I suppose it might undercut the mystery and magical aura that oftentimes goes along with those of us that have moments of outside-the-box visions, though I’d trade that enigma to know how the cups-and-balls trickery truly functions (going behind the curtain, as it were).\\xa0\\xa0\\xa0\\nSpeaking of novelty, a famous game match involving the playing of Go can provide useful illumination on this overall topic.\\xa0\\xa0\\xa0\\nGo is a popular board game\\xa0in the same complexity category as chess. Arguments are made about which is tougher, chess or Go, but I’m not going to get mired into that morass. For the sake of civil discussion, the key point is that Go is highly complex and requires intense mental concentration especially\\xa0at the\\xa0tournament level.\\xa0\\xa0\\xa0\\nGenerally, Go consists of trying to capture territory on a standard Go board, consisting of a 19 by 19 grid of intersecting lines. For those of you that have never tried playing Go, the closest similar kind of game might be the connect-the-dots that you played in childhood, which involves grabbing up territory, though Go is magnitudes more involved.\\xa0\\xa0\\xa0\\xa0\\nThere is no need for you to know anything in particular about Go to get the gist of what will be discussed next regarding the act of human novelty and the act of AI novelty.\\xa0\\xa0\\xa0\\nA famous Go competition took place about four years ago that pitted one of the world’s top professional Go players, Lee Sedol, against an AI program that had been crafted to play Go, coined as AlphaGo. There is a riveting documentary about the contest and plenty of write-ups and online videos that have in detail covered the match, including post-game analysis.\\xa0\\xa0\\xa0\\nPut yourself back in time to 2016 and relive what happened.\\xa0\\nMost AI developers did not anticipate that the AI of that time would be proficient enough to beat a top Go player. Sure, AI had already been able to best some top chess players, and thus offered a glimmer of expectation that Go would eventually be equally undertaken, but there weren’t any Go programs that had been able to compete at the pinnacle levels of human Go players. Most expected that it would probably be around the year 2020 or so before the capabilities of AI would be sufficient to compete in world-class Go tournaments.\\xa0\\xa0\\nDeepMind Created AlphaGo Using Deep Learning, Machine Learning\\xa0\\xa0\\xa0\\nA small-sized tech company named DeepMind Technologies devised the AlphaGo AI playing system (the firm was later acquired by Google). Using techniques from Machine Learning and Deep Learning, the AlphaGo program was being revamped and adjusted right up to the actual tournament, a typical kind of last-ditch developer contortions that many of us have done when trying to get the last bit of added edge into something that is about to be demonstrated.\\xa0\\xa0\\xa0\\nThis was a monumental competition that had garnered global interest.\\xa0\\xa0\\xa0\\nHuman players of Go were doubtful that the AlphaGo program would win. Many AI techies were doubtful that AlphaGo would win. Even the AlphaGo developers were unsure of how well the program would do, including the stay-awake-at-night fears that the AlphaGo program would hit a bug or go into a kind of delusional mode and make outright mistakes and play foolishly.\\xa0\\xa0\\xa0\\nA million dollars in prize money was put into the pot for the competition. There would be five Go games played, one per day, along with associated rules about taking breaks, etc. Some predicted that Sedol would handily win all five games, doing so without cracking a sweat. AI pundits were clinging to the hope that AlphaGo would win at least one of the five games, and otherwise, present itself as a respectable level of Go player throughout the contest.\\xa0\\nIn the first match, AlphaGo won.\\xa0\\xa0\\xa0\\nThis was pretty much a worldwide shocker. Sedol was taken aback. Lots of Go players were surprised that a computer program could compete and beat someone at Sedol’s level of play. Everyone began to give some street cred to the AlphaGo program and the efforts by the AI developers.\\xa0\\xa0\\xa0\\nTension grew for the next match.\\xa0\\xa0\\xa0\\nFor the second game, it was anticipated that Sedol might significantly change his approach to the contest. Perhaps he had been overconfident coming into the competition, some harshly asserted, and the loss of the first game would awaken him to the importance of putting all his concentration into the tournament. Or, possibly he had played as though he was competing with a lesser capable player and thus was not pulling out all the stops to try and win the match.\\xa0\\xa0\\xa0\\nWhat happened in the second game?\\xa0\\nTurns out that AlphaGo prevailed, again, and also did something that was seemingly remarkable for those that avidly play Go. On the 37th\\xa0move of the match, the AlphaGo program opted to make placement onto the Go board in a spot that nobody especially anticipated. It was a surprise move, coming partway through a match that otherwise was relatively conventional in the nature of the moves being made by both Sedol and AlphaGo.\\xa0\\xa0\\xa0\\nAt the time, in real-time, rampant speculation was that the move was an utter gaffe on the part of the AlphaGo program.\\xa0\\xa0\\xa0\\nInstead, it became famous as a novel move, known now as “Move 37” and heralded in Go and used colloquially overall to suggest any instance when AI does something of a novel or unexpected manner.\\xa0\\xa0\\xa0\\nIn the third match, AlphaGo won again, now having successfully beaten Sedol in a 3-out-of-5 winner competition. They\\xa0continued though to\\xa0play a fourth and a fifth game.\\xa0\\xa0\\xa0\\nDuring the fourth game, things were tight as usual and the match play was going head-to-head (well, head versus AI). Put yourself into the shoes of Sedol. In one sense, he wasn’t just a Go player, he was somehow representing all of humanity (an unfair and misguided viewpoint, but pervasive anyway), and the pressure was on him to win at least one game. Just even one game would be something to hang your hat on, and bolster faith in mankind (again, a nonsensical way to look at it).\\xa0\\xa0\\xa0\\nAt the seventy-eighth move of the fourth game, Sedol made a so-called “wedge” play that was not conventional and surprised onlookers. The next move by AlphaGo was rotten and diminished the likelihood of a win by the AI system. After additional play, ultimately AlphaGo tossed in the towel and resigned from the match, thus Sedol finally had a win against the AI in his belt. He ended-up losing the fifth game, so AlphaGo won four games, Sedol won one). His move also became famous, generally known as “Move 78” in the lore of Go playing.\\xa0\\nSomething else that is worthwhile to know about involves the overarching strategy that AlphaGo was crafted to utilize.\\xa0\\xa0\\xa0\\nWhen you play a game, let’s say connect-the-dots, you can aim to grab as many squares at each moment of play, doing so under the belief that inevitably you will then win by the accumulation of those tactically-oriented successes. Human players of Go are often apt to play that way, as it can be said too of chess players, and nearly any kind of game playing altogether.\\xa0\\xa0\\xa0\\nAnother approach involves playing to win, even if only by the thinnest of margins, as long as you win. In that case, you might not be motivated for each tactical move to gain near-term territory or score immediate points, and be willing instead to play a larger scope game per se. The proverbial mantra is that if you are shortsighted, you might win some of the battles, but could eventually lose the war. Therefore, it might be a better strategy to keep your eye on the prize, winning the war, albeit if it means that there are battles and skirmishes to be lost along the way.\\xa0\\xa0\\xa0\\nThe AI developers devised AlphaGo with that kind of macro-perspective underlying how the AI system functioned.\\xa0\\xa0\\xa0\\nHumans can have an especially hard time choosing at the moment to make a move that might look bad or ill-advised, such as giving up territory, finding themselves to be unable to grit their teeth, and taking a lump or two during play. The embarrassment at the instant is difficult to offset by betting that it is going to ultimately be okay, and you will prevail in the end.\\xa0\\xa0\\xa0\\nFor an AI system, there is no semblance of that kind of sentiment involved, and it is all about calculated odds and probabilities.\\xa0\\xa0\\xa0\\nNow that we’ve covered the legendary Go match, let’s consider some lessons learned about novelty.\\xa0\\xa0\\xa0\\nThe “Move 38” made by the AI system was not magical. It was an interesting move, for sure, and the AI developers later indicated that the move was one that the AI had calculated would rarely be undertaken by a human player.\\xa0\\xa0\\xa0\\nThis can be interpreted in two ways (at least).\\xa0\\xa0\\xa0\\nOne interpretation is that a human player would not make that move because humans are right and know that it would be a lousy move.\\xa0\\xa0\\xa0\\nAnother interpretation is that humans would not make that move due to a belief that the move is unwise, but this could be a result of the humans insufficiently assessing the ultimate value of the move, in the long-run, and getting caught up in a shorter time frame semblance of play.\\xa0\\nIn this instance, it turned out to be a good move—maybe a brilliant move—and turned the course of the game to the advantage of the AI. Thus, what looked like brilliance was in fact a calculated move that few humans would have imagined as valuable and for which jostled humans to rethink how they think about such matters.\\xa0\\xa0\\xa0\\nSome useful recap lessons:\\xa0\\xa0\\xa0\\nShowcasing Human Self-Limited Insight.\\xa0When the AI does something seemingly novel, it might be viewed as novel simply because humans have already predetermined what is customary and anything beyond that is blunted by the assumption that it is unworthy or mistaken. You could say that we are mentally trapped by our own drawing of the lines of what is considered as inside versus outside the box.\\xa0\\xa0\\xa0\\nHumans Exploiting AI For Added Insight. Humans can gainfully assess an AI-powered novelty to potentially re-calibrate human thinking on a given topic, enlarging our understanding via leveraging something that the AI, via its vast calculative capacity, might detect or spot that we have not yet so ascertained. Thus, besides admiring the novelty, we ought to seek to improve our mental prowess by whatever source shines brightly including an AI system.\\xa0\\xa0\\xa0\\nAI Novelty Is A Dual-Edged Sword.\\xa0We need to be mindful of all AI systems and their possibility of acting in a novel way, which could be good or could be bad. In the Go game, it worked out well. In other circumstances, the AI exploiting the novelty route might go off the tracks, as it were.\\xa0\\xa0\\xa0\\nLet’s see how this can be made tangible via exploring the advent of AI-based true self-driving cars.\\xa0\\xa0\\xa0\\nFor my framework about AI autonomous cars, see the link here:\\xa0https://aitrends.com/ai-insider/framework-ai-self-driving-driverless-cars-big-picture/\\xa0\\xa0\\xa0\\nWhy this is a moonshot effort, see my explanation here:\\xa0https://aitrends.com/ai-insider/self-driving-car-mother-ai-projects-moonshot/\\xa0\\xa0\\xa0\\nFor more about the levels as a type of Richter scale, see my discussion here:\\xa0https://aitrends.com/ai-insider/richter-scale-levels-self-driving-cars/\\xa0\\xa0\\xa0\\nFor the argument about bifurcating the levels, see my explanation here:\\xa0https://aitrends.com/ai-insider/reframing-ai-levels-for-self-driving-cars-bifurcation-of-autonomy/\\xa0\\xa0\\xa0\\nUnderstanding The Levels Of Self-Driving Cars\\xa0\\nAs a clarification, true self-driving cars are ones where the AI drives the car entirely on its own and there isn’t any human assistance during the driving task.\\xa0\\nThese driverless vehicles are considered a Level 4 and Level 5, while a car that requires a human driver to co-share the driving effort is usually considered at a Level 2 or Level 3. The cars that co-share the driving task are described as being semi-autonomous, and typically contain a variety of automated add-on’s that are referred to as ADAS (Advanced Driver-Assistance Systems).\\xa0\\xa0\\xa0\\nThere is not yet a true self-driving car at Level 5, which we don’t yet even know if this will be possible to achieve, and nor how long it will take to get there.\\xa0\\xa0\\xa0\\nMeanwhile, the Level 4 efforts are gradually trying to get some traction by undergoing very narrow and selective public roadway trials, though there is controversy over whether this testing should be allowed per se (we are all life-or-death guinea pigs in an experiment taking place on our highways and byways, some contend).\\xa0\\xa0\\xa0\\nFor why remote piloting or operating of self-driving cars is generally eschewed, see my explanation here:\\xa0https://aitrends.com/ai-insider/remote-piloting-is-a-self-driving-car-crutch/\\xa0\\xa0\\xa0\\nTo be wary of fake news about self-driving cars, see my tips here:\\xa0https://aitrends.com/ai-insider/ai-fake-news-about-self-driving-cars/\\xa0\\nThe ethical implications of AI driving systems are significant, see my indication here:\\xa0http://aitrends.com/selfdrivingcars/ethically-ambiguous-self-driving-cars/\\xa0\\xa0\\xa0\\nBe aware of the pitfalls of normalization of deviance when it comes to self-driving cars, here’s my call to arms:\\xa0https://aitrends.com/ai-insider/normalization-of-deviance-endangers-ai-self-driving-cars/\\xa0\\nSelf-Driving Cars And Acts Of Novelty\\xa0\\xa0\\xa0\\nFor Level 4 and Level 5 true self-driving vehicles, there won’t be a human driver involved in the driving task.\\xa0All occupants will be passengers; the AI is doing the driving.\\xa0\\xa0\\xa0\\nYou could say that the AI is playing a game, a driving game, requiring tactical decision-making and strategic planning, akin to when playing Go or chess, though in this case involving life-or-death matters driving a multi-ton car on our public roadways.\\xa0\\xa0\\xa0\\nOur base assumption is that the AI driving system is going to always take a tried-and-true approach to any driving decisions. This assumption is somewhat shaped around a notion that AI is a type of robot or automata that is bereft of any human biases or human foibles.\\xa0\\xa0\\xa0\\nIn reality, there is no reason to make this kind of assumption. Yes, we can generally rule out the aspect that the AI is not going to display the emotion of a human ilk, and we also know that the AI will not be drunk or DUI in its driving efforts. Nonetheless, if the AI has been trained using Machine Learning (ML) and Deep Learning (DL), it can pick up subtleties of human behavioral patterns in the data about human driving, out of which it will likewise utilize or mimic in choosing its driving actions (for example, see my column postings involving an analysis of potential racial biases in AI and the possibility of gender biases).\\xa0\\xa0\\xa0\\nTurning back to the topic of novelty, let’s ponder a specific use case.\\xa0\\xa0\\xa0\\nA few years ago, I was driving on an open highway, going at the prevailing speed of around 65 miles per hour, and something nearly unimaginable occurred. A car coming toward me in the opposing lane, and likely traveling at around 60 to 70 miles per hour, suddenly and unexpectedly veered into my lane. It was one of those moments that you cannot anticipate.\\xa0\\xa0\\xa0\\nThere did not appear to be any reason for the other driver to be headed toward me, in my lane of traffic, and coming at me for an imminent and bone-chillingly terrifying head-on collision. If there had been debris on the other lane, it might have been a clue that perhaps this other driver was simply trying to swing around the obstruction. No debris. If there was a slower moving car, the driver might have wanted to do a fast end-around to get past it. Nope, there was absolutely no discernible basis for this radical and life-threatening maneuver.\\xa0\\nWhat would you do?\\xa0\\nCome on, hurry, the clock is ticking, and you have just a handful of split seconds to make a life-or-death driving decision.\\xa0\\xa0\\xa0\\nYou could stay in your lane and hope that the other driver realizes the error of their ways, opting to veer back into their lane at the last moment. Or, you could proactively go into the opposing lane, giving the other driver a clear path in your lane, but this could be a chancy game of chicken whereby the other driver chooses to go back into their lane (plus, there was other traffic further behind that driver, so going into the opposing lane was quite dicey).\\xa0\\xa0\\xa0\\nOkay, so do you stay in your lane or veer away into the opposing lane?\\xa0\\xa0\\xa0\\nI dare say that most people would be torn between those two options. Neither one is palatable.\\xa0\\nSuppose the AI of a self-driving car was faced with the same circumstance.\\xa0\\xa0\\xa0\\nWhat would the AI do?\\xa0\\xa0\\xa0\\nThe odds are that even if the AI had been fed with thousands upon thousands of miles of driving via a database about human driving while undergoing the ML/DL training, there might not be any instances of a head-to-head nature and thus no prior pattern to utilize for making this onerous decision.\\xa0\\xa0\\xa0\\nAnyway, here’s a twist.\\xa0\\xa0\\xa0\\nImagine that the AI calculated the probabilities involving which way to go, and in some computational manner came to the conclusion that the self-driving car should go into the ditch that was at the right of the roadway. This was intended to avoid entirely a collision with the other car (the AI estimated that a head-on collision would be near-certain death for the occupants). The AI estimated that going into the ditch at such high speed would indisputably wreck the car and cause great bodily injury to the occupants, but the odds of assured death were (let’s say) calculated as lower than the head-on option possibilities (this is a variant of the infamous Trolley Problem, as covered in my columns).\\xa0\\xa0\\xa0\\nI’m betting that you would concede that most humans would be relatively unwilling to aim purposely into that ditch, which they know for sure is going to be a wreck and potential death, while instead willing (reluctantly) to take a hoped-for chance of either veering into the other lane or staying on course and wishing for the best.\\xa0\\xa0\\xa0\\nIn some sense, the AI might seem to have made a novel choice. It is one that (we’ll assume) few humans would have given any explicit thought toward.\\xa0\\xa0\\xa0\\nReturning to the earlier recap of the points about AI novelty, you could suggest that in this example, the AI has exceeded a human self-imposed limitation by the AI having considered otherwise “unthinkable” options. From this, perhaps we can learn to broaden our view for options that otherwise don’t seem apparent.\\xa0\\xa0\\xa0\\nThe other recap element was that the AI novelty can be a dual-edged sword.\\xa0\\xa0\\xa0\\nIf the AI did react by driving into the ditch, and you were inside the self-driving car, and you got badly injured, would you later believe that the AI acted in a novel manner or that it acted mistakenly or adversely?\\xa0\\xa0\\xa0\\nSome might say that if you lived to ask that question, apparently the AI made the right choice. The counter-argument is that if the AI had gone with one of the other choices, perhaps you would have sailed right past the other car and not gotten a single scratch.\\xa0\\xa0\\xa0\\nFor more details about ODDs, see my indication at this link here:\\xa0https://www.aitrends.com/ai-insider/amalgamating-of-operational-design-domains-odds-for-ai-self-driving-cars/\\xa0\\nOn the topic of off-road self-driving cars, here’s my details elicitation:\\xa0https://www.aitrends.com/ai-insider/off-roading-as-a-challenging-use-case-for-ai-autonomous-cars/\\xa0\\nI’ve urged that there must be a Chief Safety Officer at self-driving car makers, here’s the scoop:\\xa0https://www.aitrends.com/ai-insider/chief-safety-officers-needed-in-ai-the-case-of-ai-self-driving-cars/\\xa0\\nExpect that lawsuits are going to gradually become a significant part of the self-driving car industry, see my explanatory details here:\\xa0http://aitrends.com/selfdrivingcars/self-driving-car-lawsuits-bonanza-ahead/\\xa0\\nConclusion\\xa0\\xa0\\xa0\\nFor those of you wondering what actually did happen, my lucky stars were looking over me that day, and I survived with nothing more than a close call. I decided to remain in my lane, though it was tempting to veer into the opposing lane, and by some miracle, the other driver suddenly went back into the opposing lane.\\xa0\\xa0\\xa0\\nWhen I tell the story, my heart still gets pumping, and I begin to sweat.\\xa0\\xa0\\xa0\\nOverall, AI that appears to engage in novel approaches to problems can be advantageous and in some circumstances such as playing a board game can be right or wrong, for which being wrong does not especially put human lives at stake.\\xa0\\xa0\\xa0\\nFor AI-based true self-driving cars, lives are at stake.\\xa0\\xa0\\xa0\\nWe’ll need to proceed mindfully and with our eyes wide open about how we want AI driving systems to operate, including calculating odds and deriving choices while at the wheel of the vehicle.\\xa0\\xa0\\nCopyright 2021 Dr. Lance Eliot\\xa0\\xa0\\nhttp://ai-selfdriving-cars.libsyn.com/website\\xa0'},\n",
       " {'title': 'Getting Government AI Engineers to Tune into AI Ethics Seen as Challenge',\n",
       "  'content': 'By John P. Desmond, AI Trends Editor\\xa0\\xa0\\nEngineers tend to see things in unambiguous terms, which some may call Black and White terms, such as a choice between right or wrong and good and bad.\\xa0The consideration of ethics in AI is highly nuanced, with vast gray areas, making it\\xa0 challenging for AI software engineers to apply it in their work.\\xa0\\xa0\\nThat was a takeaway from a session on the Future of Standards and Ethical AI at the\\xa0AI World Government\\xa0conference held in-person and virtually in Alexandria, Va. this week.\\xa0\\xa0\\xa0\\nAn overall impression from the conference is that the discussion of AI and ethics is happening in virtually every quarter of AI in the vast enterprise of the federal government, and the consistency of points being made across all these different and independent efforts stood out.\\xa0\\xa0\\nBeth-Ann Schuelke-Leech, associate professor, engineering management, University of Windsor\\n“We engineers often think of ethics as a fuzzy thing that no one has really explained,” stated Beth-Anne Schuelke-Leech, an associate professor, Engineering Management and Entrepreneurship at the University of Windsor, Ontario, Canada, speaking at the Future of Ethical AI session. “It can be difficult for engineers looking for solid constraints to be told to be ethical. That becomes really complicated because we don’t know what it really means.”\\xa0\\xa0\\nSchuelke-Leech started her career as an engineer, then decided to pursue a PhD in public policy, a background which\\xa0enables\\xa0her to see things as an engineer and as a social scientist. “I got a PhD in social science, and have been pulled back into the engineering world where I am involved in AI projects, but based in a mechanical engineering faculty,” she said.\\xa0\\xa0\\xa0\\nAn engineering project has a goal, which describes the purpose, a set of needed features and functions, and a set of constraints, such as budget and timeline “The standards and regulations become part of the constraints,” she said. “If I know I have to comply with it, I will do that. But if you tell me it’s a good thing to do, I may or may not adopt that.”\\xa0\\xa0\\nSchuelke-Leech also serves as chair of the IEEE Society’s Committee on the Social Implications of Technology Standards. She commented, “Voluntary compliance standards such as from the IEEE are essential from people in the industry getting together to say this is what we think we should do as an industry.”\\xa0\\xa0\\nSome standards, such as around interoperability, do not have the force of law but engineers comply with them, so their systems will work. Other standards are described as good practices, but are not required to be followed. “Whether it helps me to achieve my goal or hinders me getting to the objective, is how the engineer looks at it,” she said.\\xa0\\xa0\\xa0\\nThe Pursuit of AI Ethics Described as “Messy and Difficult”\\xa0\\xa0\\nSara Jordan, senior counsel, Future of Privacy Forum\\nSara Jordan, senior counsel with the Future of Privacy Forum, in the session with Schuelke-Leech, works on the ethical challenges of AI and machine learning and is an active member of the IEEE Global Initiative on Ethics and Autonomous and Intelligent Systems. “Ethics is messy and difficult, and is context-laden. We have a proliferation of theories, frameworks and constructs,” she said, adding, “The practice of ethical AI will require repeatable, rigorous thinking in context.”\\xa0\\xa0\\nSchuelke-Leech offered, “Ethics is not an end outcome. It is the process being followed. But I’m also looking for someone to tell me what I need to do to do my job, to tell me how to be ethical, what rules I’m supposed to follow, to take away the ambiguity.”\\xa0\\xa0\\n“Engineers shut down when you get into funny words that they don’t understand, like ‘ontological,’ They’ve been taking math and science since they were 13-years-old,” she said.\\xa0\\xa0\\nShe has found it difficult to get engineers involved in attempts to draft standards for ethical AI. “Engineers are missing from the table,” she said. “The debates about whether we can get to 100%\\xa0ethical are\\xa0conversations\\xa0engineers do not have.”\\xa0\\xa0\\nShe concluded, “If their managers tell them to figure it out, they will do so. We need to help the engineers cross the bridge halfway. It is essential that social scientists and engineers don’t give up on this.”\\xa0\\xa0\\nLeader’s Panel Described Integration of Ethics into AI Development Practices\\xa0\\xa0\\nThe topic of ethics in AI is coming up more in the curriculum of the US Naval War College of Newport, R.I., which was established to provide advanced study for US Navy officers and now educates leaders from all services. Ross Coffey, a military professor of National Security Affairs at the institution, participated in a Leader’s Panel on AI, Ethics and Smart Policy at AI World Government.\\xa0\\xa0\\n“The ethical literacy of students increases over time as they are working with these ethical issues, which is why it is an urgent matter because it will take a long time,” Coffey said.\\xa0\\xa0\\nPanel member Carole Smith, a senior research scientist with Carnegie Mellon University who\\xa0studies human-machine interaction, has been involved in integrating ethics into AI systems development since 2015. She cited the importance of “demystifying” AI.\\xa0\\xa0\\xa0\\xa0\\n“My interest is in understanding what kind of interactions we can create where the human is appropriately trusting the system they are working with, not over- or under-trusting it,” she said, adding, “In general, people have higher expectations than they should for the systems.”\\xa0\\xa0\\nAs an example, she cited the Tesla Autopilot features, which implement self-driving car capability to a degree but not completely. “People assume the system can do a much broader set of activities than it was designed to do. Helping people understand the limitations of a system is important. Everyone needs to understand the expected outcomes of a system and what some of the mitigating circumstances might be,” she said.\\xa0\\xa0\\xa0\\nPanel member Taka Ariga, the first chief data scientist appointed to the US Government Accountability Office and director of the GAO’s Innovation Lab, sees a gap in AI literacy for the young workforce coming into the federal government.\\xa0 “Data scientist training does not always include ethics. Accountable AI is a laudable construct, but I’m not sure everyone buys into it. We need their responsibility to go beyond technical aspects and be accountable to the end user we are trying to serve,” he said.\\xa0\\xa0\\nPanel moderator Alison Brooks, PhD, research VP of Smart Cities and Communities at the IDC market research firm, asked whether principles of ethical AI can be shared across the boundaries of nations.\\xa0\\xa0\\xa0\\n“We will have a limited ability for every nation to align on the same exact approach, but we will have to align in some ways on what we will not allow AI to do, and what people will also be responsible for,” stated Smith of CMU.\\xa0\\xa0\\xa0\\nThe panelists credited the European Commission for being out front on these issues of ethics, especially in the enforcement realm.\\xa0\\nRoss of the Naval War Colleges acknowledged the importance of finding common ground around AI ethics. “From a military perspective, our interoperability needs to go to a whole new level. We need to find common ground with our partners and our allies on what we will allow AI to do and what we will not allow AI to do.” Unfortunately, “I don’t know if that discussion is happening,” he said.\\xa0\\xa0\\nDiscussion on AI ethics could perhaps be pursued as part of certain existing treaties, Smith suggested\\xa0\\xa0\\nThe many AI ethics principles, frameworks,\\xa0and road maps being offered in many federal agencies can be challenging to follow and be made consistent. Take said, “I am hopeful that over the next year or two, we will see a\\xa0coalescing.”\\xa0\\xa0\\nFor more information and access to recorded sessions, go to\\xa0AI World Government.\\xa0'},\n",
       " {'title': 'Digital Natives Seen Having Advantages as Part of Government AI Engineering Teams',\n",
       "  'content': 'By John P. Desmond, AI Trends Editor\\xa0\\xa0\\nAI is more accessible to young people in the workforce who grew up as ‘digital natives’ with Alexa and self-driving cars as part of the landscape, giving them expectations grounded in their experience of what is possible.\\xa0\\xa0\\nThat idea set the foundation for a panel discussion at\\xa0AI World Government\\xa0on Mindset Needs and Skill Set Myths for AI engineering teams, held this week virtually and in-person in Alexandria, Va.\\xa0\\xa0\\nDorothy Aronson, CIO and Chief Data Officer, National Science Foundation\\n“People feel that AI is within their grasp because the technology is available, but the technology is ahead of our cultural maturity,” said panel member Dorothy Aronson, CIO and Chief Data Officer for the National Science Foundation. “It’s like giving a sharp object to a child. We might have access to big data, but it might not be the right thing to do,” to work with it in all cases.\\xa0\\xa0\\xa0\\nThings are accelerating, which is raising expectations. When panel member Vivek Rao, lecturer and researcher at the University of California at Berkeley, was working on his PhD, a paper on natural language processing might be a master’s thesis. “Now we assign it as a homework assignment with a two-day turnaround. We have an enormous amount of compute power that was not available even two years ago,”\\xa0he said of his students, who he described as “digital natives”\\xa0with high expectations of what AI makes possible.\\xa0\\xa0\\nRachel\\xa0Dzombak, digital transformation lead, Software Engineering Institute, Carnegie Mellon University\\nPanel moderator Rachel\\xa0Dzombak, digital transformation lead at the\\xa0Software Engineering Institute\\xa0of Carnegie Mellon University, asked the panelists what is unique about working on AI in the government.\\xa0\\xa0\\xa0\\nAronson said the government cannot get too far ahead with the technology, or the users will not know how to interact with it. “We’re not building iPhones,” she said. “We have experimentation going on, and we are always looking ahead, anticipating the future, so we can make the most cost-effective decisions. In the government right now, we are seeing the convergence of the emerging generation and the close-to-retiring generation, who we also have to serve.”\\xa0\\xa0\\xa0\\nEarly in her career, Aronson did not want to work in the government. “I thought it meant you were either in the armed services or the Peace Corps,” she said. “But what I learned after a while is what motivates federal employees is service to larger, problem-solving institutions. We are trying to solve really big problems of equity and diversity, and getting food to people and keeping people safe. People that work for the government are dedicated to those missions.”\\xa0\\xa0\\xa0\\nShe referred to her two children in their 20s, who like the idea of service, but in “tiny chunks,” meaning, “They don’t look at the government as a place where they have freedom, and they can do whatever they want. They see it as a lockdown situation. But it’s really not.”\\xa0\\xa0\\xa0\\nBerkeley Students Learn About Role of Government in Disaster Response\\xa0\\xa0\\nRao of Berkeley said his students are seeing wildfires in California and asking who is working on the challenge of doing something about them. When he tells them it is almost always local, state and federal government entities, “Students are generally surprised to find that out.”\\xa0\\xa0\\xa0\\nIn one example, he developed a course on innovation in disaster response, in collaboration with CMU and the Department of Defense, the Army Futures Lab and Coast Guard search and rescue. “This was eye-opening for students,” he said. At the outset, two of 35 students expressed interest in a federal government career. By the end of the course, 10 of the 35 students were expressing interest. One of them was hired by the Naval Surface Warfare Center outside Corona, Calif. as a software engineer, Rao said.\\xa0\\xa0\\nAronson described the process of bringing on new federal employees as a “heavy lift,” suggesting, “if we could prepare in advance, it would move a lot faster.”\\xa0\\nBryan Lane, director of Data &\\xa0AI,\\xa0General\\xa0Services Administration\\nAsked by\\xa0Dzombak\\xa0what skill sets and mindsets are seen as essential to AI engineering teams, panel member Bryan Lane, director of Data & AI at the General Services Administration (who announced during the session that he is taking on a new role at FDIC), said resiliency is a necessary quality.\\xa0\\xa0\\nLane is a technology executive within the GSA IT Modernization Centers of Excellence (CoE) with over 15 years of experience leading advanced analytics and technology initiatives. He has led the GSA partnership with the DoD Joint Artificial Intelligence Center (JAIC).\\xa0[Ed. Note: Known as “the Jake.”]\\xa0Lane also is the founder of\\xa0DATA XD.\\xa0He also has experience in industry, managing acquisition portfolios.\\xa0\\xa0\\xa0\\n“The most important thing about resilient teams going on an AI journey is that you need to be ready for the unexpected, and the mission persists,” he said.\\xa0“If you are all aligned on the importance of the mission, the team can be held together.”\\xa0\\xa0\\nGood Sign that Team Members Acknowledge Having “Never Done This Before”\\xa0\\xa0\\nRegarding mindset, he said more of his team members are coming to him and saying, “I’ve never done this before.” He sees that as a good sign that offers an opportunity to talk about risk and alternative solutions. “When your team has the psychological safety to say that they don’t know something,” Lane sees it as positive. “The focus is always on what you have done and what you have delivered. Rarely is the focus on what you have not done before and what you want to grow into,” he said,\\xa0\\xa0\\nAronson has found it challenging to get AI projects off the ground. “It’s hard to tell management that you have a use case or problem to solve and want to go at it, and there is a 50-50 chance it will get done, and you don’t know how much it’s going to cost,” she said. “It comes down to articulating the rationale and convincing others it’s the right thing to do to move forward.”\\xa0\\xa0\\nRao said he talks to students about experimentation and having an experimental mindset. “AI tools can be easily accessible, but they can mask the challenges you can encounter. When you apply the vision API, for example in the context of challenges in your business or government agency, things may not be smooth,” he said.\\xa0\\xa0\\nModerator\\xa0Dzombak\\xa0asked the panelists how they build teams. Arson said, “You need a mix of people.” She has tried “communities of practice” around solving specific problems, where people can come and go. “You bring people together around a problem and not a tool,” she said.\\xa0\\xa0\\nLane seconded this. “I really have stopped focusing on tools in general,” he said. He ran experiments at JAIC in accounting, finance and other areas. “We found it’s not really about the tools. It’s about getting the right people together to understand the problems, then looking at the tools available,” he said.\\xa0\\xa0\\nLane said he sets up “cross-functional teams” that are “a little more formal than a community of interest.”\\xa0He has found them to be effective for working together on a problem for maybe 45 days. He also likes working with customers of the needed services inside the organization, and has seen customers learn about data management and AI as a result. “We will pick up one or two along the way who become advocates for accelerating AI throughout the organization,” Lane said.\\xa0\\xa0\\nLane sees it taking five years to work out proven methods of thinking, working,\\xa0and best practices for developing AI systems to serve the government. He mentioned\\xa0The Opportunity Project\\xa0(TOP) of the US Census Bureau, begun in 2016 to work on challenges such as ocean plastic pollution, COVID-19 economic recovery and disaster response. TOP has engaged in over 135 public-facing projects in that time, and has over 1,300 alumni including developers, designers, community leaders, data and policy experts, students and government agencies.\\xa0\\xa0\\xa0\\n“It’s based on a way of thinking and how to organize work,” Lane said. “We have to scale the model of delivery, but five years from now, we will have enough proof of concept to know what works and what does not.”\\xa0\\nLearn more at\\xa0AI World Government, at the\\xa0Software Engineering Institute, at\\xa0DATA XD\\xa0and at\\xa0The Opportunity Project.\\xa0'},\n",
       " {'title': 'How Accountability Practices\\xa0Are\\xa0Pursued by AI Engineers in the Federal Government',\n",
       "  'content': 'By\\xa0John P. Desmond,\\xa0AI Trends\\xa0Editor\\xa0\\xa0\\xa0\\nTwo experiences of how AI developers within the federal government are pursuing AI accountability practices were outlined at the\\xa0AI World Government\\xa0event held virtually and in-person this week in Alexandria, Va.\\xa0\\nTaka Ariga, chief data scientist and director, US\\xa0Government Accountability Office\\nTaka Ariga, chief data scientist and director at the US\\xa0Government Accountability Office,\\xa0described an AI accountability framework he uses within his agency and plans to make available to others.\\xa0\\xa0\\nAnd Bryce\\xa0Goodman, chief strategist for AI and machine learning at the\\xa0Defense Innovation Unit\\xa0(DIU), a unit of the Department of Defense founded to help the US military make faster use of emerging commercial technologies, described work in his unit to apply principles of AI development to terminology that an engineer can apply.\\xa0\\xa0\\nAriga, the first chief data scientist appointed to the US Government Accountability Office and director of the GAO’s Innovation Lab, discussed an\\xa0AI Accountability Framework\\xa0he helped to develop by convening a forum of experts in the government, industry, nonprofits, as well as federal inspector general officials and AI experts.\\xa0\\xa0\\xa0\\n“We are adopting an auditor’s perspective on the AI accountability framework,” Ariga said. “GAO is in the business of verification.”\\xa0\\xa0\\nThe effort to produce a formal framework began in September 2020 and included 60%\\xa0women, 40%\\xa0of whom were underrepresented minorities, to discuss over two days. The effort was spurred by a desire to ground the AI accountability framework in the reality of an engineer’s day-to-day work. The resulting framework was first published in June as what Ariga described as “version 1.0.”\\xa0\\xa0\\nSeeking to Bring a “High-Altitude Posture” Down to Earth\\xa0\\xa0\\n“We found the AI accountability framework had a very high-altitude posture,” Ariga said. “These are laudable ideals and aspirations, but what do they mean to the day-to-day AI practitioner? There is a gap, while we see AI proliferating across the government.”\\xa0\\xa0\\n“We landed on a lifecycle approach,” which steps through stages of design, development, deployment and continuous monitoring. The development effort stands on four “pillars” of Governance, Data, Monitoring and Performance.\\xa0\\xa0\\nGovernance reviews what the organization has put in place to oversee the AI efforts. “The chief AI officer might be in place, but what does it mean? Can the person make changes? Is it multidisciplinary?”\\xa0 At a system level within this pillar, the team will review individual AI models to see if they were “purposely deliberated.”\\xa0\\xa0\\nFor the Data pillar,\\xa0his team will examine how the training data was evaluated, how representative it is, and is it functioning as intended.\\xa0\\xa0\\nFor the Performance pillar, the team will consider the “societal impact” the AI system will have in deployment, including whether it risks a violation of the Civil Rights Act. “Auditors have a long-standing track record of evaluating equity. We grounded the evaluation of AI to a proven system,” Ariga said.\\xa0\\xa0\\xa0\\nEmphasizing the importance of continuous monitoring, he said, “AI is not a technology you deploy and forget.” he said. “We are preparing to continually monitor for model drift and the fragility of algorithms, and we are scaling the AI appropriately.” The evaluations will determine whether the AI system continues to meet the need “or whether a sunset is more appropriate,” Ariga said.\\xa0\\xa0\\nHe is part of the discussion with NIST on an overall government AI accountability framework. “We don’t want an ecosystem of confusion,” Ariga said. “We want a whole-government approach. We feel that this is a useful first step in pushing high-level ideas down to an altitude meaningful to the practitioners of AI.”\\xa0\\xa0\\nDIU Assesses Whether Proposed Projects Meet Ethical AI Guidelines\\xa0\\xa0\\nBryce Goodman, chief strategist for AI and machine learning, the\\xa0Defense Innovation Unit\\nAt the DIU, Goodman is involved in a similar effort to develop guidelines for developers of AI projects within the government.\\xa0\\xa0\\xa0\\nProjects Goodman has been involved with implementation of AI for humanitarian assistance and disaster response, predictive maintenance,\\xa0to\\xa0counter-disinformation,\\xa0and predictive health. He heads the Responsible AI Working Group. He is a faculty member of Singularity University, has a wide range of consulting clients from inside and outside the government, and holds a PhD in AI and Philosophy from the University of Oxford.\\xa0\\xa0\\nThe DOD in February 2020 adopted five areas of\\xa0Ethical Principles for AI\\xa0after 15 months of consulting with AI experts in commercial industry, government academia and the American public.\\xa0 These areas are: Responsible, Equitable, Traceable, Reliable and Governable.\\xa0\\xa0\\xa0\\n“Those are well-conceived, but it’s not obvious to an engineer how to translate them into a specific project requirement,” Good said in a presentation on Responsible AI Guidelines at the AI World Government event. “That’s the gap we are trying to fill.”\\xa0\\nBefore the DIU even considers a project, they run through the ethical principles to see if it passes muster. Not all projects do. “There needs to be an option to say the technology is not there or the problem is not compatible with AI,” he said.\\xa0\\xa0\\xa0\\nAll project stakeholders, including from commercial vendors and within the government, need to be able to test and validate and go beyond minimum legal requirements to meet the principles. “The law is not moving as fast as AI, which is why these principles are important,” he said.\\xa0\\xa0\\nAlso, collaboration is going on across the government to ensure values are being preserved and maintained. “Our intention with these guidelines is not to try to achieve perfection, but to avoid catastrophic consequences,” Goodman said. “It can be difficult to get a group to agree on what the best outcome is, but it’s easier to get the group to agree on what the worst-case outcome is.”\\xa0\\xa0\\nThe DIU guidelines along with case studies and supplemental materials will be published on the DIU website “soon,” Goodman said, to help others leverage the experience.\\xa0\\xa0\\nHere are Questions DIU Asks Before Development Starts\\xa0\\xa0\\nThe first step in the guidelines is to define the task.\\xa0 “That’s the single most important question,” he said. “Only if there is an advantage, should you use AI.”\\xa0\\nNext is a benchmark, which needs to be set up front to know if the project\\xa0has delivered.\\xa0\\xa0\\xa0\\nNext, he evaluates ownership of the candidate data. “Data is critical to the AI system and is the place where a lot of problems can exist.” Goodman said. “We need a certain contract on who owns the data. If ambiguous, this can lead to problems.”\\xa0\\xa0\\nNext, Goodman’s team wants a sample of data to evaluate. Then, they need to know how and why the information was collected. “If consent was given for one purpose, we cannot use it for another purpose without re-obtaining consent,” he said.\\xa0\\xa0\\nNext, the team asks if the responsible stakeholders are identified, such as pilots who could be affected if a component fails.\\xa0\\xa0\\xa0\\nNext, the responsible mission-holders must be identified. “We need a single individual for this,” Goodman said. “Often we have a tradeoff between the performance of an algorithm and its\\xa0explainability. We might have to decide between the two. Those kinds of decisions have an ethical component and an operational component. So we need to have someone who is accountable for those decisions, which is consistent with the chain of command in the DOD.”\\xa0\\xa0\\xa0\\nFinally, the DIU team requires a process for rolling back if things go wrong. “We need to be cautious about abandoning the previous system,” he said.\\xa0\\xa0\\xa0\\nOnce all these questions are answered in a satisfactory way, the team moves on to the development phase.\\xa0\\xa0\\nIn lessons learned, Goodman said, “Metrics are key. And simply measuring accuracy might not be adequate. We need to be able to measure success.”\\xa0\\nAlso, fit the technology to the task. “High risk applications require low-risk technology. And when potential harm is significant, we need to have high confidence in the technology,” he said.\\xa0\\xa0\\nAnother lesson learned is to set expectations with commercial vendors. “We need vendors to be transparent,” he said. ”When someone says they have a proprietary algorithm they cannot tell us about, we are very wary. We view the relationship as a collaboration. It’s the only way we can ensure\\xa0that the AI\\xa0is developed responsibly.”\\xa0\\xa0\\nLastly, “AI is not magic. It will not solve everything. It should only be used when necessary and only when we can prove it will provide an advantage.”\\xa0\\xa0\\nLearn more at\\xa0AI World Government, at the\\xa0Government Accountability Office,\\xa0at the\\xa0AI Accountability Framework\\xa0and at the\\xa0Defense Innovation Unit\\xa0site.\\xa0'},\n",
       " {'title': 'Startup:\\xa0AssemblyAI\\xa0Represents New Generation Speech Recognition',\n",
       "  'content': 'By AI Trends Staff\\xa0\\xa0\\nAdvances in the AI behind speech recognition are driving growth in the market, attracting venture capital and funding startups, posing challenges to established players.\\xa0\\xa0\\nThe growing acceptance and use of speech recognition devices are driving the market, which according to an estimate by Meticulous Research is expected to reach $26.8 billion globally by 2025, according to a recent account in\\xa0Analytics Insight. Better speed and accuracy are among the benefits of the evolving technology.\\xa0\\nDylan Fox, CEO and Founder,\\xa0AssemblyAI\\nOne company in the throes of this new growth,\\xa0AssemblyAI\\xa0of San Francisco, is offering an API for speech recognition capable of transcribing videos, podcasts, phone calls,\\xa0and remote meetings. The company was founded by CEO Dylan Fox in 2017 and has received backing from Y Combinator, a startup accelerator, as well as NVIDIA.\\xa0\\xa0\\xa0\\nFox has an unusual background for a high tech entrepreneur. He is a graduate of George Washington University with a degree in business administration, business economics,\\xa0and public policy. He got a job as a software engineer for machine learning in the emerging product lab of Cisco in San Francisco, working on deep neural networks and machine learning. He got the idea for\\xa0AssemblyAi\\xa0and attracted capital from Y Combinator, which enabled him to hire data scientists and data engineers to get the technology off the ground.\\xa0\\xa0\\xa0\\nAsked in an interview with\\xa0AI Trends\\xa0how he made this transition from undergrad in business administration and economics to high-tech entrepreneur, Fox said, “I taught myself how to program, which led me to a path of machine learning. I was looking for a harder software challenge, which led to natural language processing, which took me to Cisco.” They were working on Siri for the Enterprise for Apple at the time,\\xa0\\nTo speed up the work, Cisco was looking to acquire speech recognition software; Fox was in the catbird’s seat for the search. “We looked at Nuance,” for example, acknowledged as a market leader and owner of more speech recognition software than its competitors. (The acquisition of Nuance by Microsoft for $19.6 billion is expected to be finalized by year-end.) The young, budding entrepreneur was not impressed. “It was crazy how bad all the options were from an accuracy and a developer point of view,” he stated.\\xa0\\xa0\\nHe was impressed by Twilio, a San Francisco-based company founded in 2008, which that year released the Twilio Voice API to make and receive phone calls hosted in the cloud. The company has since raised $103 million in venture capital. “They were setting new standards for a good API for developers,” Fox said.\\xa0\\xa0\\nFox’s idea was to use AI and machine learning to achieve “super accurate results, and make it easy for developers to incorporate the API into their products. One customer is\\xa0CallRail, offering call tracking and marketing analytics software, which plans to incorporate\\xa0AssembyAI’s\\xa0API to gain insight into why people are calling. Other customers include NBC and the Wall Street\\xa0Journal, using the product to transcribe content and interviews, and provide closed captioning.\\xa0\\xa0\\n“We’ve been working on building as close to human speech recognition quality as possible. It’s been a lot of work” Fox said. He expects to reach that plateau in 2022.\\xa0\\xa0\\nHe targets companies incorporating speech recognition into their products and makes it easy to buy. Customers pay on a usage basis; for every second of audio transcribed,\\xa0AssemblyAI\\xa0charges a fraction of a penny. Clients get billed monthly. If a customer uses 10 hours a month, it costs about nine dollars. If a customer uses a million hours a month, it costs about $900,000.\\xa0\\xa0\\xa0\\xa0\\nVoice recognition is a hot market. “Many new startups are being launched,” Fox said, providing opportunity. “Many interesting new businesses are being built on voice data.”\\xa0\\xa0\\xa0\\nAssemblyAI’s\\xa0product can detect sensitive topics such as hate speech and profanity, so customers can save on human content moderation.\\xa0\\nAsked to describe what differentiates his technology, Fox said, “We are an experienced team of deep learning researchers,” with experience from companies including BMW, Apple,\\xa0and Facebook. “We build very large, very accurate deep learning models that have recognition results far more accurate than a traditional machine learning approach. We build really large models using advanced neural network technologies.” He compared the approach to what\\xa0OpenAI\\xa0uses to develop its GPT-3 large language model.\\xa0\\xa0\\nIn addition, they build AI features on top of the transcriptions, to provide summaries of audio and video content, which can be searched and indexed. “It goes beyond just transcription,” Fox said.\\xa0\\xa0\\xa0\\nThe company currently has 25 employees and expects to double in about four months. Business has been good. “There is an explosion of audio and video data online and customers want to be able to take advantage of it, so we see a lot of demand,” Fox said.\\xa0\\nLearn more at\\xa0AssemblyAI.\\xa0'},\n",
       " {'title': 'Pursuit of Autonomous Cars May Pose Risk of AI Tapping Forbidden Knowledge',\n",
       "  'content': 'By Lance Eliot, the AI Trends Insider\\xa0\\xa0\\xa0\\xa0\\nAre there things that we must not know?\\xa0\\xa0\\xa0\\nThis is an age-old question.\\xa0Some assert that there is the potential for knowledge that ought to not be known. In other words, there are ideas, concepts, or mental formulations that should we become aware of that knowledge it could be our downfall. The discovery or invention of some new innovation or way of thinking could be unduly dangerous. It would be best to not go there, as it were, and avoid ever landing on such knowledge: forbidden knowledge.\\xa0\\xa0\\xa0\\nThe typical basis for wanting to forbid the discovery or emergence of forbidden knowledge is that the adverse consequences are overwhelming.\\xa0The end result is so devastating and undercutting that the bad side outweighs the good that could be derived from the knowledge.\\xa0\\xa0\\xa0\\nIt is conceivable that there might be knowledge that is so bad that it has no good possibilities at all. Thus, rather than trying to balance or weigh the good versus the bad, the knowledge has no counterbalancing effects. It is just plain bad.\\xa0\\xa0\\xa0\\xa0\\nWe are usually faced with the matter of knowledge that has both the good and the bad as to how it might be utilized or employed. This then leads to a dogged debate about whether the bad is so bad that it outweighs the good. On top of this, there is the unrealized bad and the unrealized good, which could be differentiated from the realized bad and the realized good (in essence, the knowledge might be said to be either good or bad, though this is purely conceptual and not put into real-world conditions to attest or become realized as such).\\xa0\\xa0\\xa0\\nThe most familiar reference to forbidden knowledge is likely evoked via the Garden of Eden and the essence of forbidden fruit.\\xa0\\nA contemporary down-to-earth example often discussed about forbidden knowledge consists of the atomic bomb. Some suggest that the knowledge devised or invented to ultimately produce a nuclear bomb provides a quite visible and overt\\xa0exemplar\\xa0of the problems associated with knowledge. Had the knowledge about being able to attain an atomic bomb never been achieved, there presumably would not be any such device. In debates about the topic, it is feasible to take a resolute position favoring the attainment of an atomic bomb and there are equally counterbalancing contentions sternly disfavoring this attainment.\\xa0\\xa0\\xa0\\nOne perplexing problem about forbidden knowledge encompasses knowing beforehand the kind of knowledge that might end up in the forbidden category. This is a bit of a Catch-22 or circular type of puzzle. You might discover knowledge and then ascertain it ought to be forbidden, but the cat is kind of out of the bag due to the knowledge having been already uncovered or rendered. Oopsie, you should have in advance decided to not go there and therefore have avoided falling into the forbidden knowledge zone.\\xa0\\xa0\\xa0\\nOn a related twist, suppose that we could beforehand declare what type of knowledge is to be averted because it is predetermined as forbidden. Some people might accidentally discover the knowledge, doing so by happenstance, and now they’ve again potentially opened Pandora’s box. Meanwhile, there might be others that, regardless of being instructed to not derive any such stated forbidden knowledge, do so anyway.\\xa0\\xa0\\xa0\\nThis then takes us to a frequently used retort about forbidden knowledge, namely, if you don’t seek the forbidden knowledge there is a chance that someone else will, and you’ll be left in the dust because they got there first. In that preemptive viewpoint, the claim is that it is better to go ahead and forage for the forbidden knowledge and not get caught behind the eight-ball when someone else beats you to the punch.\\xa0\\xa0\\xa0\\nRound and round we can go.\\xa0\\xa0\\xa0\\nThe main thing that most would agree to is that knowledge is power.\\xa0\\nThe alluded to power could be devastating and destroy others, possibly even leading to the self-destruction of the wielder of the knowledge. Yet there is also the potential for knowledge to be advantageous and save humanity from other ills.\\xa0\\xa0\\xa0\\nMaybe we ought to say that knowledge is powerful. Despite that perhaps obvious proclamation, we might also add that knowledge can decay and gradually become outdated or less potent. Furthermore, since we are immersing ourselves herein into the cauldron of the love-it or hate-it knowledge conundrum, knowledge can be known and yet undervalued, perhaps only becoming valuable at a later time and in a different light.\\xa0\\xa0\\xa0\\nThere is a case to be made that humankind has a seemingly irresistible allure toward more and more knowledge. Some philosophers suggest you are unlikely to be able to bottle up or stop this quest for knowledge. If that’s the manner of how humanity will be, this implies that you must find ways to control or contain knowledge and give up on the belief that we can altogether avoid landing into forbidden knowledge.\\xa0\\xa0\\xa0\\nThere is a relatively new venue prompting a lot of anxious hand wringing pertaining to forbidden knowledge, namely the advent of Artificial Intelligence (AI).\\xa0\\xa0\\xa0\\nHere’s the rub.\\xa0\\xa0\\xa0\\nSuppose that we are able to craft AI systems that make use of knowledge about how humans can think. There are two major potential gotchas.\\xa0\\xa0\\xa0\\nFirst, the AI systems themselves might end up doing good things, and they also might end up doing bad things. If the bad outweighs the good, maybe we are shooting our own foot by allowing AI to be put into use.\\xa0\\nSecondly, perhaps this could be averted entirely by deciding that there is forbidden knowledge about how humans think, and we ought to not discover or reveal those mental mechanisms. It is the classic stepwise logic that step A axiomatically leads to step B. We won’t need to worry about AI systems (step B), if we never allow the achievement of step A (figuring out how humans think and then imparting that into computers), since the attainment of AI would presumably not arise.\\xa0\\xa0\\xa0\\nIn any case, there is inarguably a growing concern about AI.\\xa0\\xa0\\xa0\\nPlenty of efforts are underway to promulgate a semblance of AI Ethics, meaning that those developers and indeed all stakeholders that are conceiving of, building, and putting into use an AI system needs to consider the ethical aspects of their efforts. AI systems have been unveiled and placed into use replete with all sorts of notable concerns, including incorporating unsavory biases and other problems.\\xa0\\xa0\\xa0\\nAll told, one bold and somewhat stark argument is that the pursuit of AI is being underpinned or stoked by the discovery and then exploitation of forbidden knowledge.\\xa0\\xa0\\xa0\\nBe aware that many would scoff at this allegation.\\xa0\\xa0\\xa0\\nThere are those deeply immersed in the field of AI who would laugh that there is anything in the entirety of AI to date that constitutes potential forbidden knowledge. The technology and technological elements are relatively ho-hum, they would argue. You would be hard-pressed to pinpoint what AI-related knowledge that is already known comes anywhere near the ballpark of forbidden knowledge.\\xa0\\nFor those that concur with that posture, there is the reply that it might be future knowledge that we have not yet attained that is the upcoming forbidden kind, and for which we are heading pell-mell down that path. Thus, they would concede that we haven’t arrived at forbidden knowledge at this juncture, but this is an insidious distractor due to the aspect that it masks or belies our qualms entailing the possibility that it lays in wait at the next turn.\\xa0\\nOne area where AI is being actively used is to create Autonomous Vehicles (AVs).\\xa0\\nWe are gradually seeing the emergence of self-driving cars and can expect self-driving trucks, self-driving motorcycles, self-driving drones, self-driving planes, self-driving ships, self-driving submersibles, etc.\\xa0\\xa0\\xa0\\nToday’s conventional cars are eventually going to give way to the advent of AI-based, true self-driving cars. Self-driving cars are driven via an AI driving system. There isn’t a need for a human driver at the wheel, and nor is there a provision for a human to drive the vehicle.\\xa0\\xa0\\xa0\\nHere’s an intriguing question that has arisen:\\xa0Might the crafting of AI-based true self-driving cars take us into the realm of discovering forbidden knowledge, and if so, what should be done about this?\\xa0\\xa0\\xa0\\nBefore jumping into the details, I’d like to clarify what is meant when referring to true self-driving cars.\\xa0\\xa0\\xa0\\nFor my framework about AI autonomous cars, see the link here:\\xa0https://aitrends.com/ai-insider/framework-ai-self-driving-driverless-cars-big-picture/\\xa0\\xa0\\xa0\\nWhy this is a moonshot effort, see my explanation here:\\xa0https://aitrends.com/ai-insider/self-driving-car-mother-ai-projects-moonshot/\\xa0\\xa0\\xa0\\nFor more about the levels as a type of Richter scale, see my discussion here:\\xa0https://aitrends.com/ai-insider/richter-scale-levels-self-driving-cars/\\xa0\\xa0\\xa0\\nFor the argument about bifurcating the levels, see my explanation here:\\xa0https://aitrends.com/ai-insider/reframing-ai-levels-for-self-driving-cars-bifurcation-of-autonomy/\\xa0\\xa0\\xa0\\nUnderstanding The Levels Of Self-Driving Cars\\xa0\\xa0\\xa0\\nAs a clarification, true self-driving cars are ones where the AI drives the car entirely on its own and there isn’t any human assistance during the driving task.\\xa0\\xa0\\xa0\\nThese driverless vehicles are considered Level 4 and Level 5, while a car that requires a human driver to co-share the driving effort is usually considered at Level 2 or Level 3. The cars that co-share the driving task are described as being semi-autonomous, and typically contain a variety of automated add-on’s that are referred to as ADAS (Advanced Driver-Assistance Systems).\\xa0\\nThere is not yet a true self-driving car at Level 5, which we don’t yet even know if this will be possible to achieve, and nor how long it will take to get there.\\xa0\\nMeanwhile, the Level 4 efforts are gradually trying to get some traction by undergoing very narrow and selective public roadway trials, though there is controversy over whether this testing should be allowed per se (we are all life-or-death guinea pigs in an experiment taking place on our highways and byways, some contend).\\xa0\\xa0\\xa0\\nSince semi-autonomous cars require a human driver, the adoption of those types of cars won’t be markedly different from driving conventional vehicles, so there’s not much new per se to cover about them on this topic (though, as you’ll see in a moment, the points next made are generally applicable).\\xa0\\xa0\\nFor semi-autonomous cars, it is important that the public needs to be forewarned about a disturbing aspect that’s been arising lately, namely that despite those human drivers that keep posting videos of themselves falling asleep at the wheel of a Level 2 or Level 3 car, we all need to avoid being misled into believing that the driver can take away their attention from the driving task while driving a semi-autonomous car.\\xa0\\xa0\\xa0\\nYou are the responsible party for the driving actions of the vehicle, regardless of how much automation might be tossed into a Level 2 or Level 3.\\xa0\\xa0\\xa0\\nFor why remote piloting or operating of self-driving cars is generally eschewed, see my explanation here:\\xa0https://aitrends.com/ai-insider/remote-piloting-is-a-self-driving-car-crutch/\\xa0\\xa0\\xa0\\nTo be wary of fake news about self-driving cars, see my tips here:\\xa0https://aitrends.com/ai-insider/ai-fake-news-about-self-driving-cars/\\xa0\\nThe ethical implications of AI driving systems are significant, see my indication here:\\xa0http://aitrends.com/selfdrivingcars/ethically-ambiguous-self-driving-cars/\\xa0\\xa0\\xa0\\nBe aware of the pitfalls of normalization of deviance when it comes to self-driving cars, here’s my call to arms:\\xa0https://aitrends.com/ai-insider/normalization-of-deviance-endangers-ai-self-driving-cars/\\xa0\\xa0\\xa0\\nSelf-Driving Cars And Forbidden Knowledge\\xa0\\xa0\\xa0\\nFor Level 4 and Level 5 true self-driving vehicles, there won’t be a human driver involved in the driving task.\\xa0\\xa0\\xa0\\nAll occupants will be passengers.\\xa0\\xa0\\xa0\\nThe AI is doing the driving.\\xa0\\xa0\\xa0\\nOne aspect to immediately discuss entails the fact that the AI involved in today’s AI driving systems is not sentient. In other words, the AI is altogether a collective of computer-based programming and algorithms, and most assuredly not able to reason in the same manner that humans can.\\xa0\\xa0\\xa0\\nWhy this added emphasis about the AI not being sentient?\\xa0\\xa0\\xa0\\nBecause I want to underscore that when discussing the role of the AI driving system, I am not ascribing human qualities to the AI. Please be aware that there is an ongoing and dangerous tendency these days to anthropomorphize AI. In essence, people are assigning human-like sentience to today’s AI, despite the undeniable and inarguable fact that no such AI exists as yet.\\xa0\\nWith that clarification, you can envision that the AI driving system won’t natively somehow “know” about the facets of driving. Driving and all that it entails will need to be programmed as part of the hardware and software of the self-driving car.\\xa0\\xa0\\xa0\\nLet’s dive into the myriad of aspects that come to play on this topic.\\xa0\\xa0\\xa0\\nThe crux here is whether there is forbidden knowledge lurking within the existing and ongoing efforts to achieve AI-based true self-driving cars. We’ll begin by considering the status of the existent efforts and then shift into speculation about the future of such efforts.\\xa0\\xa0\\xa0\\nPer the earlier discussion about whether there is forbidden knowledge that has already perchance been revealed or discovered via the efforts toward today’s AI systems all told, the odds seem stacked against such a notion at this time, and likewise the same could be said about the pursuit of self-driving cars. Essentially, there doesn’t seem to be any forbidden knowledge per se that has been discovered or revealed during the self-driving cars development journey so far, at least with respect to the conventional wisdom about what forbidden knowledge might entail.\\xa0\\xa0\\xa0\\nOne could try to argue that it is premature to reach such a conclusion and that we might, later on, realize that forbidden knowledge was indeed uncovered or invented, and we just didn’t realize it. That is a rabbit hole that we’ll not go down for now, though you are welcome to keep that presumption at hand if so desired.\\xa0\\xa0\\xa0\\nThat covers the present, and ergo we can turn our attention to the future.\\xa0\\xa0\\xa0\\nGenerally, the efforts underway today have been primarily aimed at achieving Level 4, and the hope is that someday we will go beyond Level 4 and attain Level 5. To get to a robust Level 4, most would likely say that we can continue the existing approaches.\\xa0\\xa0\\xa0\\nNot everyone would agree with that assumption. Some believe that we will get stymied within Level 4. Furthermore, the inability to produce a robust Level 4 will ostensibly preclude us from being able to attain Level 5. There is a contingent that suggests we need to start over and set aside the existing AI approaches, which otherwise are taking us down a dead-end or blind alley. An entirely new way of devising AI for autonomous vehicles is needed, they would vehemently argue.\\xa0\\xa0\\xa0\\nThere is also a contingent that asserts the Level 4 itself is a type of dead-end. In brief, those proponents would say that we will achieve a robust Level 4, though this will do little good towards attaining Level 5. Once again, their view is similar to the preceding remark that we will need to come up with some radically new understandings about AI and the nature of cognitive acumen in order to get self-driving cars into the Level 5 realm.\\xa0\\xa0\\xa0\\nAha, it is within that scope of having to dramatically revisit and revamp what AI is and how we can advance significantly in the pursuit of AI that the forbidden knowledge question can reside. In theory, perhaps the only means of attaining Level 5 will be to strike upon some knowledge that we do not yet know and that for which bodes for falling within the realm of forbidden knowledge.\\xa0\\nTo some, this seems\\xa0farfetched.\\xa0\\nThey would emphatically ask; just what kind of knowledge are you even talking about?\\xa0\\xa0\\xa0\\nHere’s their logic. Humans are able to drive cars. Humans do not seem to need or possess forbidden knowledge as it relates to the act of driving a car. Therefore, it seems ridiculous on the face of things to claim or contend that the only means to get AI-based true self-driving cars, for which they would be driven on an equal basis as human drivers can drive, would require the discovery or invention of whatever might be construed as forbidden knowledge.\\xa0\\xa0\\xa0\\nSeems like pretty ironclad logic.\\xa0\\xa0\\xa0\\nThe retort is that humans have common-sense reasoning. With common-sense reasoning, we seem to know all sorts of things about the world around us. When we drive a car, we intrinsically make use of our common-sense reasoning. We take for granted that we do have a common-sense reasoning capacity, and similarly, we take for granted that it integrally comes to the fore when driving a car.\\xa0\\xa0\\xa0\\nAttempts to create AI that can exhibit the equivalent of human common-sense reasoning have made ostensibly modest or some would say minimal progress (to clarify, those pursuing this line of inquiry are to be lauded, it’s just that no earth-shattering breakthroughs seem to have been reached and none seem on the immediate horizon). Yes, there are some quite fascinating and exciting efforts underway, but when you measure those against the everyday common-sense reasoning of humans, there is no comparison. They are night and day. If this were a contest, the humans win hands down, no doubt about it, and the AI experimental efforts encompassing common-sense reasoning are mere playthings in contrast.\\xa0\\xa0\\xa0\\nYou might have gleaned where this line of thought is headed.\\xa0\\xa0\\xa0\\nThe belief by some is that until we crack open the enigma of common-sense reasoning, there is little chance of achieving a Level 5, and perhaps also this will hold back the Level 4 too. It could be that a secret ingredient of sorts for autonomous vehicles is the need to figure out and include common-sense reasoning into AI-based driving and piloting systems.\\xa0\\xa0\\xa0\\nIf you buy into that logic, the added assertion is that maybe within the confines of how common-sense reasoning takes place is a semblance of forbidden knowledge. On the surface, you would certainly assume that if we knew entirely how common-sense reasoning works, there would not appear to be any cause for alarm or concern. The act of employing common-sense reasoning does not seem to necessarily embody forbidden knowledge.\\xa0\\xa0\\xa0\\nThe twist is that perhaps the underlying cognitive means that gives rise to the advent of common-sense reasoning is where there is forbidden knowledge. Some deep-rooted elements in the nature of human thought and how we form common sense and undertake common-sense reasoning are possibly a type of knowledge that will be shown as crucial and a forbidden knowledge formulation.\\xa0\\xa0\\xa0\\nFor more details about ODDs, see my indication at this link here:\\xa0https://www.aitrends.com/ai-insider/amalgamating-of-operational-design-domains-odds-for-ai-self-driving-cars/\\xa0\\nOn the topic of off-road self-driving cars, here’s my details elicitation:\\xa0https://www.aitrends.com/ai-insider/off-roading-as-a-challenging-use-case-for-ai-autonomous-cars/\\xa0\\nI’ve urged that there must be a Chief Safety Officer at self-driving car makers, here’s the scoop:\\xa0https://www.aitrends.com/ai-insider/chief-safety-officers-needed-in-ai-the-case-of-ai-self-driving-cars/\\xa0\\nExpect that lawsuits are going to gradually become a significant part of the self-driving car industry, see my explanatory details here:\\xa0http://aitrends.com/selfdrivingcars/self-driving-car-lawsuits-bonanza-ahead/\\xa0\\xa0\\xa0\\nConclusion\\xa0\\xa0\\xa0\\nWow, that’s quite a bit of pondering, contemplation, and (some would say) wild thinking.\\xa0\\xa0\\xa0\\nMaybe so, but it is a consideration that some would wish that we gave at least some credence toward and devoted attention to. There is the angst that we might find ourselves by happenstance stumbling into forbidden knowledge on these voracious self-driving cars quests.\\xa0\\xa0\\xa0\\nFor however you might emphasize that having AI-based true self-driving cars will be a potential blessing, proffering mobility-for-all and leading to reducing the number of car crash-related fatalities, there is a sneaking suspicion that it will not be all-good. The catch or trap could be that there is some kind of forbidden knowledge that will get brought to the eye and we will inevitably kick ourselves that we didn’t see it coming.\\xa0\\xa0\\xa0\\nThe next time you are munching on a delicious apple, give some thought to whether self-driving cars might be forbidden fruit.\\xa0\\xa0\\xa0\\nWe are on the path to taking a big bite, and we’ll have to see where that takes us.\\xa0\\nCopyright 2021 Dr. Lance Eliot\\xa0\\xa0\\nhttp://ai-selfdriving-cars.libsyn.com/website\\xa0'}]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blog_articles = json.load(open(\"articles.json\")) # read the articles from the JSON file\n",
    "blog_articles"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "641b5220",
   "metadata": {},
   "source": [
    "**8. Word cloud**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "9e750554",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Best Practices for Building the AI Development Platform in Government\\nBy John P. Desmond, AI Trends Editor\\xa0\\nThe AI stack defined by Carnegie Mellon University is fundamental to the approach being taken by the US Army for its AI development platform efforts, according to Isaac Faber, Chief Data Scientist at the US Army AI Integration Center, speaking at the\\xa0AI World Government\\xa0event held in-person and virtually from Alexandria, Va., last week.\\xa0\\xa0\\nIsaac Faber, Chief Data Scientist, US Army AI Integration Center\\n“If we want to move the Army from legacy systems through digital modernization, one of the biggest issues I have found is the difficulty in abstracting away the differences in applications,” he said. “The most important part of digital transformation is the middle layer, the platform that makes it easier to be on the cloud or on a local computer.” The desire is to be able to move your software platform to another platform, with the same ease\\xa0with\\xa0which a new smartphone carries over the user’s contacts and histories.\\xa0\\xa0\\nEthics cuts across all layers of the AI application stack, which positions the planning stage at the top, followed by decision support, modeling, machine learning, massive data management and the device layer or platform at the bottom.\\xa0\\xa0\\n“I am advocating that we think of the stack as a core infrastructure and a way for applications to be deployed and not to be siloed in our approach,” he said. “We need to create a development environment for a globally-distributed workforce.”\\xa0\\xa0\\xa0\\nThe Army has been working on a Common Operating Environment Software (Coes) platform, first announced in 2017, a design for DOD work that is scalable, agile, modular, portable and open. “It is suitable for a broad range of AI projects,” Faber said. For executing the effort, “The devil is in the details,” he said.\\xa0\\xa0\\xa0\\nThe Army is working with CMU and private companies on a prototype platform, including with\\xa0Visimo\\xa0of Coraopolis, Pa., which offers AI development services. Faber said he prefers to collaborate and coordinate with private industry rather than buying products off the shelf. “The problem with that is, you are stuck with the value you are being provided by that one vendor, which is usually not designed for the challenges of DOD networks,” he said.\\xa0\\xa0\\nArmy Trains a Range of Tech Teams in AI\\xa0\\nThe Army engages in AI workforce development efforts for several teams, including:\\xa0 leadership, professionals with graduate degrees; technical staff, which is put through training to get certified; and AI users.\\xa0\\xa0\\xa0\\nTech teams in the Army have different areas of focus include: general purpose software development, operational data science, deployment which includes analytics, and a machine learning operations team, such as a large team required to build a computer vision system. “As folks come through the workforce, they need a place to collaborate, build and share,” Faber said.\\xa0\\xa0\\xa0\\nTypes of projects include diagnostic, which might be combining streams of historical data, predictive and prescriptive, which recommends a course of action based on a prediction. “At the far end is AI; you don’t start with that,” said Faber. The developer has to solve three problems: data engineering, the AI development platform, which he called “the green bubble,” and the deployment platform, which he called “the red bubble.”\\xa0\\xa0\\xa0\\n“These are mutually exclusive and all interconnected. Those teams of different people need to programmatically coordinate. Usually a good project team will have people from each of those bubble areas,” he said. “If you have not done this yet, do not try to solve the green bubble problem. It makes no sense to pursue AI until you have an operational need.”\\xa0\\xa0\\xa0\\nAsked by a participant which group is the most difficult to reach and train, Faber said without hesitation, “The hardest to reach are the executives. They need to learn what the value is to be provided by the AI ecosystem. The biggest challenge is how to communicate that value,” he said.\\xa0\\xa0\\xa0\\nPanel Discusses AI Use Cases with the Most Potential\\xa0\\xa0\\nIn a panel on Foundations of Emerging AI, moderator Curt Savoie, program director, Global Smart Cities Strategies for IDC, the market research firm, asked what emerging AI use case has the most potential.\\xa0\\xa0\\nJean-Charles Lede, autonomy tech advisor for the US Air Force, Office of Scientific Research, said,” I would point to decision advantages at the edge, supporting pilots and operators, and decisions at the back, for mission and resource planning.”\\xa0\\xa0\\xa0\\nKrista Kinnard, Chief of Emerging Technology for the Department of Labor\\nKrista Kinnard, Chief of Emerging Technology for the Department of Labor, said, “Natural language processing is an opportunity to open the doors to AI in the Department of Labor,” she said. “Ultimately, we are dealing with data on people, programs,\\xa0and organizations.”\\xa0\\xa0\\xa0\\xa0\\nSavoie asked what are the big risks and dangers the panelists see when implementing AI.\\xa0\\xa0\\xa0\\nAnil Chaudhry, Director of Federal AI Implementations for the General Services Administration (GSA), said in a typical IT organization using traditional software development, the impact of a decision by a developer only goes so far. With AI, “You have to consider the impact on a whole class of people, constituents,\\xa0and stakeholders. With a simple change in algorithms, you could be delaying benefits to millions of people or making incorrect inferences at scale. That’s the most important risk,” he said.\\xa0\\xa0\\nHe said he asks his contract partners to have “humans in the loop and humans on the loop.”\\xa0\\xa0\\xa0\\nKinnard seconded this, saying, “We have no intention of removing humans from the loop. It’s really about empowering people to make better decisions.”\\xa0\\xa0\\xa0\\nShe emphasized the importance of monitoring the AI models after they are deployed. “Models can drift as the data underlying the changes,” she said. “So you need a level of critical thinking to not only do the task, but to assess whether what the AI model is doing is acceptable.”\\xa0\\xa0\\xa0\\nShe added, “We have built out use cases and partnerships across the government to make sure we’re implementing responsible AI. We will never replace people with algorithms.”\\xa0\\xa0\\nLede of the Air Force said, “We often have use cases where the data does not exist. We cannot explore 50 years of war data, so we use simulation. The risk is in teaching an algorithm that you have a ‘simulation to real gap’ that is a real risk. You are not sure how the algorithms will map to the real world.”\\xa0\\xa0\\nChaudhry emphasized the importance of a testing strategy for AI systems. He warned of developers “who get enamored with a tool and forget the purpose of the exercise.” He recommended the development manager design in independent verification and validation strategy. “Your testing, that is where you have to focus your energy as a leader. The leader needs an idea in mind, before committing resources, on how they will justify whether the investment was a success.”\\xa0\\xa0\\xa0\\nLede of the Air Force talked about the importance of\\xa0explainability. “I am a technologist. I don’t\\xa0do laws. The ability for the AI function to explain in a way a human can interact with, is important. The AI is a partner that we have a dialogue with, instead of the AI coming up with a conclusion that we have no way of verifying,” he said.\\xa0\\xa0\\nLearn more at\\xa0AI World Government.\\xa0\\nAdvance Trustworthy AI and ML, and Identify Best Practices for Scaling AI\\nBy John P. Desmond, AI Trends Editor\\xa0\\xa0\\nAdvancing trustworthy AI and machine learning to mitigate agency risk is a priority for the US Department of Energy (DOE), and identifying best practices for implementing AI at scale is a priority for the US General Services Administration (GSA).\\xa0\\xa0\\nThat’s what attendees learned in two sessions at the\\xa0AI World Government\\xa0live and virtual event held in Alexandria, Va. last week.\\xa0\\xa0\\xa0\\nPamela Isom, Director of the AI and Technology Office, DOE\\nPamela Isom, Director of the AI and Technology Office at the DOE, who spoke on Advancing Trustworthy AI and ML Techniques for Mitigating Agency Risks, has been involved in proliferating the use of AI across the agency for several years. With an emphasis on applied AI and data science, she oversees risk mitigation policies and standards and has been involved with applying AI to save lives, fight fraud, and strengthen the cybersecurity infrastructure.\\xa0\\xa0\\nShe emphasized the need for the AI project effort to be part of a strategic portfolio. “My office is there to drive a holistic view on AI and to mitigate risk by bringing us together to address challenges,” she said. The effort is assisted by the DOE’s AI and Technology Office, which is focused on transforming the DOE into a world-leading AI enterprise by accelerating research, development, delivery and the adoption of AI.\\xa0\\xa0\\n“I am telling my organization to be mindful of the fact that you can have tons and tons of data, but it might not be representative,” she said. Her team looks at examples from international partners, industry, academia and other agencies for outcomes “we can trust” from systems incorporating AI.\\xa0\\xa0\\n“We know that AI is disruptive, in trying to do what humans do and do it better,” she said. “It is beyond human capability; it goes beyond data in spreadsheets; it can tell me what I’m going to do next before I contemplate it myself. It’s that powerful,” she said.\\xa0\\xa0\\nAs a result, close attention must be paid to data sources. “AI is vital to the economy and our national security. We need precision; we need algorithms we can trust; we need accuracy. We don’t need biases,” Isom said, adding, “And don’t forget that you need to monitor the output of the models long after they have been deployed.”\\xa0\\xa0\\xa0\\nExecutive Orders Guide GSA AI Work\\xa0\\nExecutive Order 14028, a detailed set of actions to address the cybersecurity of government agencies, issued in May of this year, and Executive Order 13960, promoting the use of trustworthy AI in the Federal government, issued in December 2020, provide valuable guides to her work.\\xa0\\xa0\\xa0\\nTo help manage the risk of AI development and deployment, Isom has produced the AI Risk Management Playbook, which provides guidance around system features and mitigation techniques. It also has a filter for ethical and trustworthy principles which are considered throughout AI lifecycle stages and risk types. Plus, the playbook ties to relevant Executive Orders.\\xa0\\xa0\\nAnd it provides examples, such as your results came in at 80%\\xa0accuracy, but you wanted 90%. “Something is wrong there,” Isom said, adding, “The playbook helps you look at these types of problems and what you can do to mitigate risk, and what factors you should weigh as you design and build your project.”\\xa0\\xa0\\nWhile internal to DOE at present, the agency is looking into next steps for an external version. “We will share it with other federal agencies soon,” she said.\\xa0\\xa0\\xa0\\nGSA Best Practices for Scaling AI Projects Outlined\\xa0\\xa0\\nAnil Chaudhry, Director of Federal AI Implementations, AI Center of Excellence (CoE), GSA\\nAnil Chaudhry, Director of Federal AI Implementations for the AI Center of Excellence (CoE) of the GSA, who spoke on Best Practices for Implementing AI at Scale, has over 20 years of experience in technology delivery, operations and program management in the defense, intelligence and national security sectors.\\xa0\\xa0\\xa0\\nThe mission of the\\xa0CoE\\xa0is to accelerate technology modernization across the government, improve the public experience and increase operational efficiency. “Our business model is to partner with industry subject matter experts to solve problems,” Chaudhry said, adding, “We are not in the business of recreating industry solutions and duplicating them.”\\xa0\\xa0\\xa0\\nThe\\xa0CoE\\xa0is providing recommendations to partner agencies and working with them to implement AI systems as the federal government engages heavily in AI development. “For AI, the government landscape is vast. Every federal agency has some sort of AI project going on right now,” he said, and the maturity of AI experience varies widely across agencies.\\xa0\\xa0\\nTypical use cases he is seeing include having AI focus on increasing speed and efficiency, on cost savings and cost avoidance, on improved response time and increased quality and compliance. As one best practice, he recommended the agencies\\xa0vet their commercial experience\\xa0with the large datasets they will encounter in government.\\xa0\\xa0\\xa0\\n“We’re talking petabytes and exabytes here, of structured and unstructured data,” Chaudhry said.\\xa0[Ed. Note: A petabyte is 1,000 terabytes.]\\xa0“Also ask industry partners about their strategies and processes on how they do macro and micro trend analysis, and what their experience has been in the deployment of bots such as in Robotic Process Automation, and how they demonstrate sustainability as a result of drift of data.”\\xa0\\xa0\\xa0\\nHe also asks potential industry partners to\\xa0describe the AI talent on their team\\xa0or what talent they can access. If the company is weak on AI talent, Chaudhry would ask, “If you buy something, how will you know you got what you wanted when you have no way of evaluating it?”\\xa0\\xa0\\nHe added, “A best practice in implementing AI is defining how you train your workforce to leverage AI tools, techniques and practices, and to define how you grow and mature your workforce. Access to talent leads to either success or failure in AI projects, especially when it comes to scaling a pilot up to a fully deployed system.”\\xa0\\xa0\\nIn another best practice, Chaudhry recommended examining the industry partner’s\\xa0access to financial capital.\\xa0“AI is a field where the flow of capital is highly volatile. “You cannot predict or project that you will spend X amount of dollars this year to get where you want to be,” he said, because an AI development team may need to explore another hypothesis, or clean up some data that may not be transparent or is potentially biased. “If you don’t have access to funding, it is a risk your project will fail,” he said.\\xa0\\xa0\\nAnother best practice is\\xa0access to logistical capital, such as the data\\xa0 that sensors collect for an AI IoT system. “AI requires an enormous amount of data that is authoritative and timely. Direct access to that data is critical,” Chaudhry said. He recommended that data sharing agreements\\xa0 be in place with organizations relevant to the AI system. “You might not need it right away, but having access to the data, so you could immediately use it and to have thought through the privacy issues before you need the data, is a good practice for scaling AI programs,” he said.\\xa0\\xa0\\xa0\\nA final best practice is planning of\\xa0physical infrastructure,\\xa0such as data center space. “When you are in a pilot, you need to know how much capacity you need to reserve at your data center, and how many end points you need to manage” when the application scales up, Chaudhry said, adding, “This all ties back to access to capital and all the other best practices.“\\xa0\\nLearn more at\\xa0AI World Government.\\xa0\\nPromise and Perils of Using AI for Hiring: Guard Against Data Bias\\nBy AI Trends Staff\\xa0\\xa0\\nWhile\\xa0AI in hiring is now widely used for writing job descriptions, screening candidates, and automating interviews, it poses a risk of wide discrimination if not implemented carefully.\\xa0\\nKeith Sonderling, Commissioner, US Equal Opportunity Commission\\nThat was the message from Keith Sonderling, Commissioner with the US Equal Opportunity Commision, speaking at the\\xa0AI World Government\\xa0event held live and virtually in Alexandria, Va., last week. Sonderling is responsible for enforcing federal laws that prohibit discrimination against job applicants because of race, color, religion, sex, national origin, age or\\xa0disability.\\xa0\\xa0\\xa0\\n“The thought that AI would become mainstream in HR departments was closer to science fiction two year ago, but the pandemic has accelerated the rate at which AI is being used by employers,” he said. “Virtual recruiting is now here to stay.”\\xa0\\xa0\\nIt’s a busy time for HR professionals. “The great resignation is leading to the great rehiring, and AI will play a role in that like we have not seen before,” Sonderling said.\\xa0\\xa0\\nAI has been employed for years in hiring—“It did not happen overnight.”—for tasks including chatting with applications, predicting whether a candidate would take the job, projecting what type of employee they would be and mapping out upskilling and reskilling opportunities. “In short, AI is now making all the decisions once made by HR personnel,” which he did not characterize as good or bad.\\xa0\\xa0\\xa0\\n“Carefully designed and properly used, AI has the potential to make the workplace more fair,” Sonderling said. “But carelessly implemented, AI could discriminate on a scale we have never seen before by an HR professional.”\\xa0\\xa0\\nTraining Datasets for AI Models Used for Hiring Need to Reflect Diversity\\xa0\\xa0\\nThis is because AI models rely on training data. If the company’s current workforce is used as the basis for training, “It will replicate the status quo. If it’s one gender or one race primarily, it will replicate that,” he said. Conversely, AI can help mitigate risks of hiring bias by race, ethnic background, or disability status. “I want to see AI improve on workplace discrimination,” he said.\\xa0\\xa0\\nAmazon began building a hiring application in 2014, and found over time that it discriminated against women in its recommendations, because the AI model was trained on a dataset of the company’s own hiring record for the previous 10 years, which was primarily of males. Amazon developers tried to correct it but ultimately scrapped the system in 2017.\\xa0\\xa0\\xa0\\nFacebook has recently agreed to pay $14.25 million to settle civil claims by the US government that the social media company discriminated against American workers and violated federal recruitment rules, according to an account from\\xa0Reuters. The case centered on Facebook’s use of what it called its PERM program for labor certification. The government found that Facebook refused to hire American workers for jobs that had been reserved for temporary visa holders under the PERM program.\\xa0\\xa0\\xa0\\n“Excluding people from the hiring pool is a violation,” Sonderling said.\\xa0 If the AI program “withholds the existence of the job opportunity to that\\xa0class, so\\xa0they cannot exercise their rights, or if it downgrades a protected class, it is within our domain,” he said.\\xa0\\xa0\\xa0\\nEmployment assessments, which became more common after World War II, have provided\\xa0 high value to HR managers and with help from AI they have the potential to minimize bias in hiring. “At the same time, they are vulnerable to claims of discrimination, so\\xa0employers\\xa0need to be careful and cannot take a hands-off approach,” Sonderling said. “Inaccurate data will amplify bias in decision-making. Employers must be vigilant against discriminatory outcomes.”\\xa0\\xa0\\nHe recommended researching solutions from vendors who vet data for risks of bias on the basis of race, sex, and other factors.\\xa0\\xa0\\xa0\\nOne example is from\\xa0HireVue\\xa0of South Jordan, Utah, which has\\xa0built a\\xa0hiring platform predicated on the US Equal Opportunity Commission’s Uniform Guidelines, designed specifically to mitigate unfair hiring practices, according to an account from\\xa0allWork.\\xa0\\xa0\\xa0\\nA post on AI ethical principles on its website states in part, “Because\\xa0HireVue\\xa0uses AI technology in our products, we actively work to prevent the introduction or\\xa0propagation\\xa0of bias against any group or individual. We will continue to carefully review the datasets we use in our work and ensure that they are as accurate and diverse as possible. We also continue to advance our abilities to monitor, detect, and mitigate bias. We strive to build teams from diverse backgrounds with diverse knowledge, experiences, and perspectives to best represent the people our systems serve.”\\xa0\\xa0\\nAlso, “Our data scientists and IO psychologists build\\xa0HireVue\\xa0Assessment algorithms in a way that removes data from consideration by the algorithm that contributes to adverse impact without significantly impacting the assessment’s predictive accuracy. The result is a highly valid, bias-mitigated assessment that helps to enhance human decision making while actively promoting diversity and equal opportunity regardless of gender, ethnicity, age, or disability status.”\\xa0\\xa0\\nDr. Ed\\xa0Ikeguchi, CEO,\\xa0AiCure\\nThe issue of bias in datasets used to train AI models is not confined to hiring. Dr. Ed\\xa0Ikeguchi, CEO of\\xa0AiCure, an AI analytics company working in the life sciences industry, stated in a recent account in\\xa0HealthcareITNews, “AI is only as strong as the data it’s fed, and lately that data backbone’s credibility is being increasingly called into question. Today’s AI developers lack access to large, diverse data sets on which to train and validate new tools.”\\xa0\\xa0\\nHe added, “They often need to leverage open-source datasets, but many of these were trained using computer programmer volunteers, which is a predominantly white population. Because algorithms are often trained on single-origin data samples with limited diversity, when applied in real-world scenarios to a broader population of different races, genders, ages, and more, tech that appeared highly accurate in research may prove unreliable.”\\xa0\\nAlso, “There needs to be an element of governance and peer review for all algorithms, as even the most solid and tested algorithm is bound to have unexpected results arise. An algorithm is never done learning—it must be constantly developed and fed more data to improve.”\\xa0\\nAnd, “As an industry, we need to become more skeptical of AI’s conclusions and encourage transparency in the industry. Companies should readily answer basic questions, such as ‘How was the algorithm trained? On what basis did it draw this conclusion?”\\xa0\\nRead the source articles and information at\\xa0AI World Government, from\\xa0Reuters\\xa0and from\\xa0HealthcareITNews.\\xa0\\nPredictive Maintenance Proving Out as Successful AI Use Case\\nBy John P. Desmond, AI Trends Editor\\xa0\\xa0\\nMore companies are successfully exploiting predictive maintenance systems that combine AI and IoT sensors to collect data that anticipates breakdowns and recommends preventive action before break or machines fail, in a demonstration of an AI use case with proven value.\\xa0\\xa0\\nThis growth is reflected in optimistic market forecasts. The predictive maintenance market is sized at $6.9 billion today and is projected to grow to $28.2 billion by 2026, according to a report from\\xa0IoT Analytics\\xa0of Hamburg, Germany. The firm\\xa0counts over 280 vendors offering solutions in the market today, projected to grow to over 500 by 2026.\\xa0\\xa0\\nFernando Bruegge, Analyst, IoT Analytics, Hamburg, Germany\\n“This research is a wake-up call to those that claim IoT is failing,” stated analyst Fernando Bruegge, author of the report, adding, “For companies that own industrial assets or sell equipment, now is the time to invest in predictive maintenance-type solutions.” And, “Enterprise technology firms need to prepare to integrate predictive maintenance solutions into their offerings,” Bruegge suggested.\\xa0\\xa0\\nHere is a review of some specific experience with predictive maintenance systems that combine AI and IoT sensors.\\xa0\\nAircraft engine manufacturer\\xa0Rolls-Royce\\xa0is\\xa0deploying predictive analytics\\xa0to help reduce the amount of carbon its engines produce, while also optimizing maintenance to help customers keep planes in the air longer, according to a recent account in\\xa0CIO.\\xa0\\nRolls-Royce built an Intelligent Engine platform to monitor engine flight, gathering data on weather conditions and how pilots are flying. Machine learning is applied to the data to customize maintenance regimes for individual engines.\\xa0\\nStuart Hughes, chief information and digital officer, Rolls-Royce\\n“We’re tailoring our maintenance regimes to make sure that we’re optimizing for the life an engine has, not the life the manual says it should have,” stated Stuart Hughes, chief information and digital officer at Rolls-Royce. “It’s truly variable service, looking at each engine as an individual engine.”\\xa0\\nCustomers are seeing less service interruption. “Rolls-Royce has been monitoring engines and charging per hour for at least 20 years,” Hughes stated. “That part of the business isn’t new. But as we’ve evolved, we’ve begun to treat the engine as a singular engine. It’s much more about the personalization of that engine.”\\xa0\\xa0\\nPredictive analytics is being applied in healthcare as well as in the manufacturing industry. Kaiser Permanente, the integrated managed care consortium based in Oakland, Calif. Is using predictive analytics to identify non-intensive care unit (ICU) patients at risk of rapid deterioration.\\xa0\\xa0\\xa0\\nWhile non-ICU patients that require unexpected transfers to the ICU constitute less than\\xa04%\\xa0of the total hospital population, they account for 20%\\xa0of all hospital deaths, according to Dr. Gabriel Escobar, research scientist, Division of Research, and regional director, Hospital Operations Research, Kaiser Permanente Northern California.\\xa0\\nKaiser Permanente Practicing Predictive Maintenance in Healthcare\\xa0\\nKaiser Permanente developed the Advanced Alert Monitor (AAM) system, leveraging three predictive analytic models to analyze more than 70 factors in a given patient’s electronic health record to generate a composite risk score.\\xa0\\n“The AAM system synthesizes and analyzes vital statistics, lab results, and other variables to generate hourly deterioration risk scores for adult hospital patients in the medical-surgical and transitional care units,” stated Dick Daniels, executive vice president and CIO of Kaiser Permanente in the CIO account. “Remote hospital teams evaluate the risk scores every hour and notify rapid response teams in the hospital when potential deterioration is detected. The rapid response team conducts bedside evaluation of the patient and calibrates the course treatment with the hospitalist.”\\xa0\\nIn advice to other practitioners, Daniels recommended a focus on how the tool will be fit into the workflow of health care teams. “It took us about five years to perform the initial mapping of the electronic medical record backend and develop the predictive models,” Daniels stated. “It then took us another two to three years to transition these models into a live web services application that could be used operationally.”\\xa0\\nIn an example from the food industry, a PepsiCo Frito-Lay plant in Fayetteville, Tenn. is using predictive maintenance successfully, with year-to-date equipment downtime at 0.75%\\xa0and unplanned downtime at 2.88%, according to Carlos Calloway, the site’s reliability engineering manager, in an account in\\xa0PlantServices.\\xa0\\nExamples of monitoring include: vibration readings confirmed by ultrasound helped to prevent a PC combustion blower motor from failing and shutting down the whole potato chip department; infrared analysis of the main pole for the plant’s GES automated warehouse detected a hot fuse holder, which helped to avoid a shutdown of the entire warehouse; and increased acid levels were detected in oil samples from a baked extruder gearbox, indicating oil degradation, which enabled prevention of a shutdown of Cheetos Puffs production.\\xa0\\nThe Frito-Lay plant produces more than 150 million pounds of product per year, including Lays, Ruffles, Cheetos, Doritos, Fritos, and Tostitos.\\xa0\\xa0\\nThe types of monitoring include vibration analysis, used on mechanical applications, which is processed with the help of a third-party company which sends alerts to the plant for investigation and resolution.\\xa0Another service partner performs quarterly vibration monitoring on selected equipment. All motor control center rooms and electrical panels are monitored with quarterly infrared analysis, which is also used on electrical equipment, some rotating equipment, and heat exchangers. In addition, the plant has done ultrasonic monitoring for more than 15 years, and it is “kind of like the pride and joy of our site from a predictive standpoint,” stated Calloway.\\xa0\\xa0\\nThe plan has a number of products in place from UE Systems of Elmsford, NY, supplier of ultrasonic instruments, hardware and software, and training for predictive maintenance.\\xa0\\xa0\\xa0\\nLouisiana Alumina Plant Automating Bearing Maintenance\\xa0\\xa0\\xa0\\nBearings, which wear over time under varying conditions of weather and temperature in the case of automobiles, are a leading candidate for IoT monitoring and predictive maintenance with AI. The\\xa0Noranda Alumina\\xa0plant in Gramercy, La. is finding a big payoff from its investment in a system to improve the lubrication of bearings in its production equipment.\\xa0\\xa0\\nThe system has resulted in a 60%\\xa0decline in bearing changes in the second year of using the new lubrication system, translating to some $900,000 in savings on bearings that did not need to be replaced and avoided downtime.\\xa0\\xa0\\n“Four hours of downtime is about $1 million dollars’ worth of lost production,” stated Russell Goodwin, a reliability engineer and millwright instructor at Noranda Alumina, in the\\xa0PlantServices\\xa0account, which was based on presentations at the Leading Reliability 2021 event.\\xa0\\nThe Noranda Alumina plant is the only alumina plant operating in the US.\\xa0“If we shut down, you’ll need to import it,” stated Goodwin. The plant experiences pervasive dust, dirt, and caustic substances, which complicate efforts at improved reliability and maintenance practices.\\xa0\\xa0\\nNoranda Alumina tracks all motors and gearboxes at 1,500 rpm and higher with vibration readings, and most below 1,500 with ultrasound. Ultrasonic monitoring, of sound in ranges beyond human hearing, was introduced to the plant after Goodwin joined the company in 2019. At the time, grease monitoring had room for improvement. “If grease was not visibly coming out of the seal, the mechanical supervisor did not count the round as complete,” stated Goodwin.\\xa0\\xa0\\nAfter introducing automation, the greasing system has improved dramatically, he stated. The system was also able to detect bearings in a belt whose bearings were wearing out too quickly due to contamination. “Tool-enabled tracking helped to prove that it wasn’t improper greasing, but rather the bearing was made improperly,” stated Goodwin.\\xa0\\xa0\\nRead the source articles and information in\\xa0\\xa0IoT Analytics,\\xa0in\\xa0CIO\\xa0and in\\xa0PlantServices.\\xa0\\nNovelty In The Game Of Go Provides Bright Insights For AI And Autonomous Vehicles\\nBy Lance Eliot, the AI Trends Insider\\xa0\\xa0\\nWe already\\xa0expect that humans to exhibit flashes of brilliance.\\xa0It might not happen all the time, but the act itself is welcomed and not altogether disturbing when\\xa0it occurs.\\xa0\\xa0\\xa0\\nWhat about when Artificial Intelligence (AI) seems to display an act of novelty?\\xa0Any such instance is bound to get our attention; questions arise right away.\\xa0\\xa0\\xa0\\nHow did the AI come up with the apparent out-of-the-blue insight or novel indication? Was it a mistake, or did it fit within the parameters of what the AI was expected to produce? There is also the immediate consideration of whether the AI somehow is slipping toward the precipice of becoming sentient.\\xa0\\xa0\\xa0\\nPlease be aware that no AI system in existence is anywhere close to reaching sentience, despite the\\xa0claims and falsehoods tossed around in the media. As such, if today’s AI seems to do something that appears to be a novel act, you should not leap to the conclusion that this is a sign of human insight within technology or the emergence of human ingenuity among AI.\\xa0\\xa0\\xa0\\nThat’s an anthropomorphic bridge too far.\\xa0\\xa0\\xa0\\nThe reality is that any such AI “insightful” novelties are based on various concrete computational algorithms and tangible data-based pattern matching.\\xa0\\xa0\\xa0\\nIn today’s column, we’ll be taking a close look at an example of an AI-powered novel act, illustrated via the game of Go, and relate these facets to the advent of AI-based true self-driving cars as a means of understanding the AI-versus-human related ramifications.\\xa0\\nRealize that the capacity to spot or suggest a novelty is being done methodically by an AI system, while, in contrast, no one can say for sure how humans can devise novel thoughts or intuitions.\\xa0\\nPerhaps we too are bound by some internal mechanistic-like facets, or maybe there is something else going on. Someday, hopefully, we will crack open the secret inner workings of the mind and finally know how we think. I suppose it might undercut the mystery and magical aura that oftentimes goes along with those of us that have moments of outside-the-box visions, though I’d trade that enigma to know how the cups-and-balls trickery truly functions (going behind the curtain, as it were).\\xa0\\xa0\\xa0\\nSpeaking of novelty, a famous game match involving the playing of Go can provide useful illumination on this overall topic.\\xa0\\xa0\\xa0\\nGo is a popular board game\\xa0in the same complexity category as chess. Arguments are made about which is tougher, chess or Go, but I’m not going to get mired into that morass. For the sake of civil discussion, the key point is that Go is highly complex and requires intense mental concentration especially\\xa0at the\\xa0tournament level.\\xa0\\xa0\\xa0\\nGenerally, Go consists of trying to capture territory on a standard Go board, consisting of a 19 by 19 grid of intersecting lines. For those of you that have never tried playing Go, the closest similar kind of game might be the connect-the-dots that you played in childhood, which involves grabbing up territory, though Go is magnitudes more involved.\\xa0\\xa0\\xa0\\xa0\\nThere is no need for you to know anything in particular about Go to get the gist of what will be discussed next regarding the act of human novelty and the act of AI novelty.\\xa0\\xa0\\xa0\\nA famous Go competition took place about four years ago that pitted one of the world’s top professional Go players, Lee Sedol, against an AI program that had been crafted to play Go, coined as AlphaGo. There is a riveting documentary about the contest and plenty of write-ups and online videos that have in detail covered the match, including post-game analysis.\\xa0\\xa0\\xa0\\nPut yourself back in time to 2016 and relive what happened.\\xa0\\nMost AI developers did not anticipate that the AI of that time would be proficient enough to beat a top Go player. Sure, AI had already been able to best some top chess players, and thus offered a glimmer of expectation that Go would eventually be equally undertaken, but there weren’t any Go programs that had been able to compete at the pinnacle levels of human Go players. Most expected that it would probably be around the year 2020 or so before the capabilities of AI would be sufficient to compete in world-class Go tournaments.\\xa0\\xa0\\nDeepMind Created AlphaGo Using Deep Learning, Machine Learning\\xa0\\xa0\\xa0\\nA small-sized tech company named DeepMind Technologies devised the AlphaGo AI playing system (the firm was later acquired by Google). Using techniques from Machine Learning and Deep Learning, the AlphaGo program was being revamped and adjusted right up to the actual tournament, a typical kind of last-ditch developer contortions that many of us have done when trying to get the last bit of added edge into something that is about to be demonstrated.\\xa0\\xa0\\xa0\\nThis was a monumental competition that had garnered global interest.\\xa0\\xa0\\xa0\\nHuman players of Go were doubtful that the AlphaGo program would win. Many AI techies were doubtful that AlphaGo would win. Even the AlphaGo developers were unsure of how well the program would do, including the stay-awake-at-night fears that the AlphaGo program would hit a bug or go into a kind of delusional mode and make outright mistakes and play foolishly.\\xa0\\xa0\\xa0\\nA million dollars in prize money was put into the pot for the competition. There would be five Go games played, one per day, along with associated rules about taking breaks, etc. Some predicted that Sedol would handily win all five games, doing so without cracking a sweat. AI pundits were clinging to the hope that AlphaGo would win at least one of the five games, and otherwise, present itself as a respectable level of Go player throughout the contest.\\xa0\\nIn the first match, AlphaGo won.\\xa0\\xa0\\xa0\\nThis was pretty much a worldwide shocker. Sedol was taken aback. Lots of Go players were surprised that a computer program could compete and beat someone at Sedol’s level of play. Everyone began to give some street cred to the AlphaGo program and the efforts by the AI developers.\\xa0\\xa0\\xa0\\nTension grew for the next match.\\xa0\\xa0\\xa0\\nFor the second game, it was anticipated that Sedol might significantly change his approach to the contest. Perhaps he had been overconfident coming into the competition, some harshly asserted, and the loss of the first game would awaken him to the importance of putting all his concentration into the tournament. Or, possibly he had played as though he was competing with a lesser capable player and thus was not pulling out all the stops to try and win the match.\\xa0\\xa0\\xa0\\nWhat happened in the second game?\\xa0\\nTurns out that AlphaGo prevailed, again, and also did something that was seemingly remarkable for those that avidly play Go. On the 37th\\xa0move of the match, the AlphaGo program opted to make placement onto the Go board in a spot that nobody especially anticipated. It was a surprise move, coming partway through a match that otherwise was relatively conventional in the nature of the moves being made by both Sedol and AlphaGo.\\xa0\\xa0\\xa0\\nAt the time, in real-time, rampant speculation was that the move was an utter gaffe on the part of the AlphaGo program.\\xa0\\xa0\\xa0\\nInstead, it became famous as a novel move, known now as “Move 37” and heralded in Go and used colloquially overall to suggest any instance when AI does something of a novel or unexpected manner.\\xa0\\xa0\\xa0\\nIn the third match, AlphaGo won again, now having successfully beaten Sedol in a 3-out-of-5 winner competition. They\\xa0continued though to\\xa0play a fourth and a fifth game.\\xa0\\xa0\\xa0\\nDuring the fourth game, things were tight as usual and the match play was going head-to-head (well, head versus AI). Put yourself into the shoes of Sedol. In one sense, he wasn’t just a Go player, he was somehow representing all of humanity (an unfair and misguided viewpoint, but pervasive anyway), and the pressure was on him to win at least one game. Just even one game would be something to hang your hat on, and bolster faith in mankind (again, a nonsensical way to look at it).\\xa0\\xa0\\xa0\\nAt the seventy-eighth move of the fourth game, Sedol made a so-called “wedge” play that was not conventional and surprised onlookers. The next move by AlphaGo was rotten and diminished the likelihood of a win by the AI system. After additional play, ultimately AlphaGo tossed in the towel and resigned from the match, thus Sedol finally had a win against the AI in his belt. He ended-up losing the fifth game, so AlphaGo won four games, Sedol won one). His move also became famous, generally known as “Move 78” in the lore of Go playing.\\xa0\\nSomething else that is worthwhile to know about involves the overarching strategy that AlphaGo was crafted to utilize.\\xa0\\xa0\\xa0\\nWhen you play a game, let’s say connect-the-dots, you can aim to grab as many squares at each moment of play, doing so under the belief that inevitably you will then win by the accumulation of those tactically-oriented successes. Human players of Go are often apt to play that way, as it can be said too of chess players, and nearly any kind of game playing altogether.\\xa0\\xa0\\xa0\\nAnother approach involves playing to win, even if only by the thinnest of margins, as long as you win. In that case, you might not be motivated for each tactical move to gain near-term territory or score immediate points, and be willing instead to play a larger scope game per se. The proverbial mantra is that if you are shortsighted, you might win some of the battles, but could eventually lose the war. Therefore, it might be a better strategy to keep your eye on the prize, winning the war, albeit if it means that there are battles and skirmishes to be lost along the way.\\xa0\\xa0\\xa0\\nThe AI developers devised AlphaGo with that kind of macro-perspective underlying how the AI system functioned.\\xa0\\xa0\\xa0\\nHumans can have an especially hard time choosing at the moment to make a move that might look bad or ill-advised, such as giving up territory, finding themselves to be unable to grit their teeth, and taking a lump or two during play. The embarrassment at the instant is difficult to offset by betting that it is going to ultimately be okay, and you will prevail in the end.\\xa0\\xa0\\xa0\\nFor an AI system, there is no semblance of that kind of sentiment involved, and it is all about calculated odds and probabilities.\\xa0\\xa0\\xa0\\nNow that we’ve covered the legendary Go match, let’s consider some lessons learned about novelty.\\xa0\\xa0\\xa0\\nThe “Move 38” made by the AI system was not magical. It was an interesting move, for sure, and the AI developers later indicated that the move was one that the AI had calculated would rarely be undertaken by a human player.\\xa0\\xa0\\xa0\\nThis can be interpreted in two ways (at least).\\xa0\\xa0\\xa0\\nOne interpretation is that a human player would not make that move because humans are right and know that it would be a lousy move.\\xa0\\xa0\\xa0\\nAnother interpretation is that humans would not make that move due to a belief that the move is unwise, but this could be a result of the humans insufficiently assessing the ultimate value of the move, in the long-run, and getting caught up in a shorter time frame semblance of play.\\xa0\\nIn this instance, it turned out to be a good move—maybe a brilliant move—and turned the course of the game to the advantage of the AI. Thus, what looked like brilliance was in fact a calculated move that few humans would have imagined as valuable and for which jostled humans to rethink how they think about such matters.\\xa0\\xa0\\xa0\\nSome useful recap lessons:\\xa0\\xa0\\xa0\\nShowcasing Human Self-Limited Insight.\\xa0When the AI does something seemingly novel, it might be viewed as novel simply because humans have already predetermined what is customary and anything beyond that is blunted by the assumption that it is unworthy or mistaken. You could say that we are mentally trapped by our own drawing of the lines of what is considered as inside versus outside the box.\\xa0\\xa0\\xa0\\nHumans Exploiting AI For Added Insight. Humans can gainfully assess an AI-powered novelty to potentially re-calibrate human thinking on a given topic, enlarging our understanding via leveraging something that the AI, via its vast calculative capacity, might detect or spot that we have not yet so ascertained. Thus, besides admiring the novelty, we ought to seek to improve our mental prowess by whatever source shines brightly including an AI system.\\xa0\\xa0\\xa0\\nAI Novelty Is A Dual-Edged Sword.\\xa0We need to be mindful of all AI systems and their possibility of acting in a novel way, which could be good or could be bad. In the Go game, it worked out well. In other circumstances, the AI exploiting the novelty route might go off the tracks, as it were.\\xa0\\xa0\\xa0\\nLet’s see how this can be made tangible via exploring the advent of AI-based true self-driving cars.\\xa0\\xa0\\xa0\\nFor my framework about AI autonomous cars, see the link here:\\xa0https://aitrends.com/ai-insider/framework-ai-self-driving-driverless-cars-big-picture/\\xa0\\xa0\\xa0\\nWhy this is a moonshot effort, see my explanation here:\\xa0https://aitrends.com/ai-insider/self-driving-car-mother-ai-projects-moonshot/\\xa0\\xa0\\xa0\\nFor more about the levels as a type of Richter scale, see my discussion here:\\xa0https://aitrends.com/ai-insider/richter-scale-levels-self-driving-cars/\\xa0\\xa0\\xa0\\nFor the argument about bifurcating the levels, see my explanation here:\\xa0https://aitrends.com/ai-insider/reframing-ai-levels-for-self-driving-cars-bifurcation-of-autonomy/\\xa0\\xa0\\xa0\\nUnderstanding The Levels Of Self-Driving Cars\\xa0\\nAs a clarification, true self-driving cars are ones where the AI drives the car entirely on its own and there isn’t any human assistance during the driving task.\\xa0\\nThese driverless vehicles are considered a Level 4 and Level 5, while a car that requires a human driver to co-share the driving effort is usually considered at a Level 2 or Level 3. The cars that co-share the driving task are described as being semi-autonomous, and typically contain a variety of automated add-on’s that are referred to as ADAS (Advanced Driver-Assistance Systems).\\xa0\\xa0\\xa0\\nThere is not yet a true self-driving car at Level 5, which we don’t yet even know if this will be possible to achieve, and nor how long it will take to get there.\\xa0\\xa0\\xa0\\nMeanwhile, the Level 4 efforts are gradually trying to get some traction by undergoing very narrow and selective public roadway trials, though there is controversy over whether this testing should be allowed per se (we are all life-or-death guinea pigs in an experiment taking place on our highways and byways, some contend).\\xa0\\xa0\\xa0\\nFor why remote piloting or operating of self-driving cars is generally eschewed, see my explanation here:\\xa0https://aitrends.com/ai-insider/remote-piloting-is-a-self-driving-car-crutch/\\xa0\\xa0\\xa0\\nTo be wary of fake news about self-driving cars, see my tips here:\\xa0https://aitrends.com/ai-insider/ai-fake-news-about-self-driving-cars/\\xa0\\nThe ethical implications of AI driving systems are significant, see my indication here:\\xa0http://aitrends.com/selfdrivingcars/ethically-ambiguous-self-driving-cars/\\xa0\\xa0\\xa0\\nBe aware of the pitfalls of normalization of deviance when it comes to self-driving cars, here’s my call to arms:\\xa0https://aitrends.com/ai-insider/normalization-of-deviance-endangers-ai-self-driving-cars/\\xa0\\nSelf-Driving Cars And Acts Of Novelty\\xa0\\xa0\\xa0\\nFor Level 4 and Level 5 true self-driving vehicles, there won’t be a human driver involved in the driving task.\\xa0All occupants will be passengers; the AI is doing the driving.\\xa0\\xa0\\xa0\\nYou could say that the AI is playing a game, a driving game, requiring tactical decision-making and strategic planning, akin to when playing Go or chess, though in this case involving life-or-death matters driving a multi-ton car on our public roadways.\\xa0\\xa0\\xa0\\nOur base assumption is that the AI driving system is going to always take a tried-and-true approach to any driving decisions. This assumption is somewhat shaped around a notion that AI is a type of robot or automata that is bereft of any human biases or human foibles.\\xa0\\xa0\\xa0\\nIn reality, there is no reason to make this kind of assumption. Yes, we can generally rule out the aspect that the AI is not going to display the emotion of a human ilk, and we also know that the AI will not be drunk or DUI in its driving efforts. Nonetheless, if the AI has been trained using Machine Learning (ML) and Deep Learning (DL), it can pick up subtleties of human behavioral patterns in the data about human driving, out of which it will likewise utilize or mimic in choosing its driving actions (for example, see my column postings involving an analysis of potential racial biases in AI and the possibility of gender biases).\\xa0\\xa0\\xa0\\nTurning back to the topic of novelty, let’s ponder a specific use case.\\xa0\\xa0\\xa0\\nA few years ago, I was driving on an open highway, going at the prevailing speed of around 65 miles per hour, and something nearly unimaginable occurred. A car coming toward me in the opposing lane, and likely traveling at around 60 to 70 miles per hour, suddenly and unexpectedly veered into my lane. It was one of those moments that you cannot anticipate.\\xa0\\xa0\\xa0\\nThere did not appear to be any reason for the other driver to be headed toward me, in my lane of traffic, and coming at me for an imminent and bone-chillingly terrifying head-on collision. If there had been debris on the other lane, it might have been a clue that perhaps this other driver was simply trying to swing around the obstruction. No debris. If there was a slower moving car, the driver might have wanted to do a fast end-around to get past it. Nope, there was absolutely no discernible basis for this radical and life-threatening maneuver.\\xa0\\nWhat would you do?\\xa0\\nCome on, hurry, the clock is ticking, and you have just a handful of split seconds to make a life-or-death driving decision.\\xa0\\xa0\\xa0\\nYou could stay in your lane and hope that the other driver realizes the error of their ways, opting to veer back into their lane at the last moment. Or, you could proactively go into the opposing lane, giving the other driver a clear path in your lane, but this could be a chancy game of chicken whereby the other driver chooses to go back into their lane (plus, there was other traffic further behind that driver, so going into the opposing lane was quite dicey).\\xa0\\xa0\\xa0\\nOkay, so do you stay in your lane or veer away into the opposing lane?\\xa0\\xa0\\xa0\\nI dare say that most people would be torn between those two options. Neither one is palatable.\\xa0\\nSuppose the AI of a self-driving car was faced with the same circumstance.\\xa0\\xa0\\xa0\\nWhat would the AI do?\\xa0\\xa0\\xa0\\nThe odds are that even if the AI had been fed with thousands upon thousands of miles of driving via a database about human driving while undergoing the ML/DL training, there might not be any instances of a head-to-head nature and thus no prior pattern to utilize for making this onerous decision.\\xa0\\xa0\\xa0\\nAnyway, here’s a twist.\\xa0\\xa0\\xa0\\nImagine that the AI calculated the probabilities involving which way to go, and in some computational manner came to the conclusion that the self-driving car should go into the ditch that was at the right of the roadway. This was intended to avoid entirely a collision with the other car (the AI estimated that a head-on collision would be near-certain death for the occupants). The AI estimated that going into the ditch at such high speed would indisputably wreck the car and cause great bodily injury to the occupants, but the odds of assured death were (let’s say) calculated as lower than the head-on option possibilities (this is a variant of the infamous Trolley Problem, as covered in my columns).\\xa0\\xa0\\xa0\\nI’m betting that you would concede that most humans would be relatively unwilling to aim purposely into that ditch, which they know for sure is going to be a wreck and potential death, while instead willing (reluctantly) to take a hoped-for chance of either veering into the other lane or staying on course and wishing for the best.\\xa0\\xa0\\xa0\\nIn some sense, the AI might seem to have made a novel choice. It is one that (we’ll assume) few humans would have given any explicit thought toward.\\xa0\\xa0\\xa0\\nReturning to the earlier recap of the points about AI novelty, you could suggest that in this example, the AI has exceeded a human self-imposed limitation by the AI having considered otherwise “unthinkable” options. From this, perhaps we can learn to broaden our view for options that otherwise don’t seem apparent.\\xa0\\xa0\\xa0\\nThe other recap element was that the AI novelty can be a dual-edged sword.\\xa0\\xa0\\xa0\\nIf the AI did react by driving into the ditch, and you were inside the self-driving car, and you got badly injured, would you later believe that the AI acted in a novel manner or that it acted mistakenly or adversely?\\xa0\\xa0\\xa0\\nSome might say that if you lived to ask that question, apparently the AI made the right choice. The counter-argument is that if the AI had gone with one of the other choices, perhaps you would have sailed right past the other car and not gotten a single scratch.\\xa0\\xa0\\xa0\\nFor more details about ODDs, see my indication at this link here:\\xa0https://www.aitrends.com/ai-insider/amalgamating-of-operational-design-domains-odds-for-ai-self-driving-cars/\\xa0\\nOn the topic of off-road self-driving cars, here’s my details elicitation:\\xa0https://www.aitrends.com/ai-insider/off-roading-as-a-challenging-use-case-for-ai-autonomous-cars/\\xa0\\nI’ve urged that there must be a Chief Safety Officer at self-driving car makers, here’s the scoop:\\xa0https://www.aitrends.com/ai-insider/chief-safety-officers-needed-in-ai-the-case-of-ai-self-driving-cars/\\xa0\\nExpect that lawsuits are going to gradually become a significant part of the self-driving car industry, see my explanatory details here:\\xa0http://aitrends.com/selfdrivingcars/self-driving-car-lawsuits-bonanza-ahead/\\xa0\\nConclusion\\xa0\\xa0\\xa0\\nFor those of you wondering what actually did happen, my lucky stars were looking over me that day, and I survived with nothing more than a close call. I decided to remain in my lane, though it was tempting to veer into the opposing lane, and by some miracle, the other driver suddenly went back into the opposing lane.\\xa0\\xa0\\xa0\\nWhen I tell the story, my heart still gets pumping, and I begin to sweat.\\xa0\\xa0\\xa0\\nOverall, AI that appears to engage in novel approaches to problems can be advantageous and in some circumstances such as playing a board game can be right or wrong, for which being wrong does not especially put human lives at stake.\\xa0\\xa0\\xa0\\nFor AI-based true self-driving cars, lives are at stake.\\xa0\\xa0\\xa0\\nWe’ll need to proceed mindfully and with our eyes wide open about how we want AI driving systems to operate, including calculating odds and deriving choices while at the wheel of the vehicle.\\xa0\\xa0\\nCopyright 2021 Dr. Lance Eliot\\xa0\\xa0\\nhttp://ai-selfdriving-cars.libsyn.com/website\\xa0\\nGetting Government AI Engineers to Tune into AI Ethics Seen as Challenge\\nBy John P. Desmond, AI Trends Editor\\xa0\\xa0\\nEngineers tend to see things in unambiguous terms, which some may call Black and White terms, such as a choice between right or wrong and good and bad.\\xa0The consideration of ethics in AI is highly nuanced, with vast gray areas, making it\\xa0 challenging for AI software engineers to apply it in their work.\\xa0\\xa0\\nThat was a takeaway from a session on the Future of Standards and Ethical AI at the\\xa0AI World Government\\xa0conference held in-person and virtually in Alexandria, Va. this week.\\xa0\\xa0\\xa0\\nAn overall impression from the conference is that the discussion of AI and ethics is happening in virtually every quarter of AI in the vast enterprise of the federal government, and the consistency of points being made across all these different and independent efforts stood out.\\xa0\\xa0\\nBeth-Ann Schuelke-Leech, associate professor, engineering management, University of Windsor\\n“We engineers often think of ethics as a fuzzy thing that no one has really explained,” stated Beth-Anne Schuelke-Leech, an associate professor, Engineering Management and Entrepreneurship at the University of Windsor, Ontario, Canada, speaking at the Future of Ethical AI session. “It can be difficult for engineers looking for solid constraints to be told to be ethical. That becomes really complicated because we don’t know what it really means.”\\xa0\\xa0\\nSchuelke-Leech started her career as an engineer, then decided to pursue a PhD in public policy, a background which\\xa0enables\\xa0her to see things as an engineer and as a social scientist. “I got a PhD in social science, and have been pulled back into the engineering world where I am involved in AI projects, but based in a mechanical engineering faculty,” she said.\\xa0\\xa0\\xa0\\nAn engineering project has a goal, which describes the purpose, a set of needed features and functions, and a set of constraints, such as budget and timeline “The standards and regulations become part of the constraints,” she said. “If I know I have to comply with it, I will do that. But if you tell me it’s a good thing to do, I may or may not adopt that.”\\xa0\\xa0\\nSchuelke-Leech also serves as chair of the IEEE Society’s Committee on the Social Implications of Technology Standards. She commented, “Voluntary compliance standards such as from the IEEE are essential from people in the industry getting together to say this is what we think we should do as an industry.”\\xa0\\xa0\\nSome standards, such as around interoperability, do not have the force of law but engineers comply with them, so their systems will work. Other standards are described as good practices, but are not required to be followed. “Whether it helps me to achieve my goal or hinders me getting to the objective, is how the engineer looks at it,” she said.\\xa0\\xa0\\xa0\\nThe Pursuit of AI Ethics Described as “Messy and Difficult”\\xa0\\xa0\\nSara Jordan, senior counsel, Future of Privacy Forum\\nSara Jordan, senior counsel with the Future of Privacy Forum, in the session with Schuelke-Leech, works on the ethical challenges of AI and machine learning and is an active member of the IEEE Global Initiative on Ethics and Autonomous and Intelligent Systems. “Ethics is messy and difficult, and is context-laden. We have a proliferation of theories, frameworks and constructs,” she said, adding, “The practice of ethical AI will require repeatable, rigorous thinking in context.”\\xa0\\xa0\\nSchuelke-Leech offered, “Ethics is not an end outcome. It is the process being followed. But I’m also looking for someone to tell me what I need to do to do my job, to tell me how to be ethical, what rules I’m supposed to follow, to take away the ambiguity.”\\xa0\\xa0\\n“Engineers shut down when you get into funny words that they don’t understand, like ‘ontological,’ They’ve been taking math and science since they were 13-years-old,” she said.\\xa0\\xa0\\nShe has found it difficult to get engineers involved in attempts to draft standards for ethical AI. “Engineers are missing from the table,” she said. “The debates about whether we can get to 100%\\xa0ethical are\\xa0conversations\\xa0engineers do not have.”\\xa0\\xa0\\nShe concluded, “If their managers tell them to figure it out, they will do so. We need to help the engineers cross the bridge halfway. It is essential that social scientists and engineers don’t give up on this.”\\xa0\\xa0\\nLeader’s Panel Described Integration of Ethics into AI Development Practices\\xa0\\xa0\\nThe topic of ethics in AI is coming up more in the curriculum of the US Naval War College of Newport, R.I., which was established to provide advanced study for US Navy officers and now educates leaders from all services. Ross Coffey, a military professor of National Security Affairs at the institution, participated in a Leader’s Panel on AI, Ethics and Smart Policy at AI World Government.\\xa0\\xa0\\n“The ethical literacy of students increases over time as they are working with these ethical issues, which is why it is an urgent matter because it will take a long time,” Coffey said.\\xa0\\xa0\\nPanel member Carole Smith, a senior research scientist with Carnegie Mellon University who\\xa0studies human-machine interaction, has been involved in integrating ethics into AI systems development since 2015. She cited the importance of “demystifying” AI.\\xa0\\xa0\\xa0\\xa0\\n“My interest is in understanding what kind of interactions we can create where the human is appropriately trusting the system they are working with, not over- or under-trusting it,” she said, adding, “In general, people have higher expectations than they should for the systems.”\\xa0\\xa0\\nAs an example, she cited the Tesla Autopilot features, which implement self-driving car capability to a degree but not completely. “People assume the system can do a much broader set of activities than it was designed to do. Helping people understand the limitations of a system is important. Everyone needs to understand the expected outcomes of a system and what some of the mitigating circumstances might be,” she said.\\xa0\\xa0\\xa0\\nPanel member Taka Ariga, the first chief data scientist appointed to the US Government Accountability Office and director of the GAO’s Innovation Lab, sees a gap in AI literacy for the young workforce coming into the federal government.\\xa0 “Data scientist training does not always include ethics. Accountable AI is a laudable construct, but I’m not sure everyone buys into it. We need their responsibility to go beyond technical aspects and be accountable to the end user we are trying to serve,” he said.\\xa0\\xa0\\nPanel moderator Alison Brooks, PhD, research VP of Smart Cities and Communities at the IDC market research firm, asked whether principles of ethical AI can be shared across the boundaries of nations.\\xa0\\xa0\\xa0\\n“We will have a limited ability for every nation to align on the same exact approach, but we will have to align in some ways on what we will not allow AI to do, and what people will also be responsible for,” stated Smith of CMU.\\xa0\\xa0\\xa0\\nThe panelists credited the European Commission for being out front on these issues of ethics, especially in the enforcement realm.\\xa0\\nRoss of the Naval War Colleges acknowledged the importance of finding common ground around AI ethics. “From a military perspective, our interoperability needs to go to a whole new level. We need to find common ground with our partners and our allies on what we will allow AI to do and what we will not allow AI to do.” Unfortunately, “I don’t know if that discussion is happening,” he said.\\xa0\\xa0\\nDiscussion on AI ethics could perhaps be pursued as part of certain existing treaties, Smith suggested\\xa0\\xa0\\nThe many AI ethics principles, frameworks,\\xa0and road maps being offered in many federal agencies can be challenging to follow and be made consistent. Take said, “I am hopeful that over the next year or two, we will see a\\xa0coalescing.”\\xa0\\xa0\\nFor more information and access to recorded sessions, go to\\xa0AI World Government.\\xa0\\nDigital Natives Seen Having Advantages as Part of Government AI Engineering Teams\\nBy John P. Desmond, AI Trends Editor\\xa0\\xa0\\nAI is more accessible to young people in the workforce who grew up as ‘digital natives’ with Alexa and self-driving cars as part of the landscape, giving them expectations grounded in their experience of what is possible.\\xa0\\xa0\\nThat idea set the foundation for a panel discussion at\\xa0AI World Government\\xa0on Mindset Needs and Skill Set Myths for AI engineering teams, held this week virtually and in-person in Alexandria, Va.\\xa0\\xa0\\nDorothy Aronson, CIO and Chief Data Officer, National Science Foundation\\n“People feel that AI is within their grasp because the technology is available, but the technology is ahead of our cultural maturity,” said panel member Dorothy Aronson, CIO and Chief Data Officer for the National Science Foundation. “It’s like giving a sharp object to a child. We might have access to big data, but it might not be the right thing to do,” to work with it in all cases.\\xa0\\xa0\\xa0\\nThings are accelerating, which is raising expectations. When panel member Vivek Rao, lecturer and researcher at the University of California at Berkeley, was working on his PhD, a paper on natural language processing might be a master’s thesis. “Now we assign it as a homework assignment with a two-day turnaround. We have an enormous amount of compute power that was not available even two years ago,”\\xa0he said of his students, who he described as “digital natives”\\xa0with high expectations of what AI makes possible.\\xa0\\xa0\\nRachel\\xa0Dzombak, digital transformation lead, Software Engineering Institute, Carnegie Mellon University\\nPanel moderator Rachel\\xa0Dzombak, digital transformation lead at the\\xa0Software Engineering Institute\\xa0of Carnegie Mellon University, asked the panelists what is unique about working on AI in the government.\\xa0\\xa0\\xa0\\nAronson said the government cannot get too far ahead with the technology, or the users will not know how to interact with it. “We’re not building iPhones,” she said. “We have experimentation going on, and we are always looking ahead, anticipating the future, so we can make the most cost-effective decisions. In the government right now, we are seeing the convergence of the emerging generation and the close-to-retiring generation, who we also have to serve.”\\xa0\\xa0\\xa0\\nEarly in her career, Aronson did not want to work in the government. “I thought it meant you were either in the armed services or the Peace Corps,” she said. “But what I learned after a while is what motivates federal employees is service to larger, problem-solving institutions. We are trying to solve really big problems of equity and diversity, and getting food to people and keeping people safe. People that work for the government are dedicated to those missions.”\\xa0\\xa0\\xa0\\nShe referred to her two children in their 20s, who like the idea of service, but in “tiny chunks,” meaning, “They don’t look at the government as a place where they have freedom, and they can do whatever they want. They see it as a lockdown situation. But it’s really not.”\\xa0\\xa0\\xa0\\nBerkeley Students Learn About Role of Government in Disaster Response\\xa0\\xa0\\nRao of Berkeley said his students are seeing wildfires in California and asking who is working on the challenge of doing something about them. When he tells them it is almost always local, state and federal government entities, “Students are generally surprised to find that out.”\\xa0\\xa0\\xa0\\nIn one example, he developed a course on innovation in disaster response, in collaboration with CMU and the Department of Defense, the Army Futures Lab and Coast Guard search and rescue. “This was eye-opening for students,” he said. At the outset, two of 35 students expressed interest in a federal government career. By the end of the course, 10 of the 35 students were expressing interest. One of them was hired by the Naval Surface Warfare Center outside Corona, Calif. as a software engineer, Rao said.\\xa0\\xa0\\nAronson described the process of bringing on new federal employees as a “heavy lift,” suggesting, “if we could prepare in advance, it would move a lot faster.”\\xa0\\nBryan Lane, director of Data &\\xa0AI,\\xa0General\\xa0Services Administration\\nAsked by\\xa0Dzombak\\xa0what skill sets and mindsets are seen as essential to AI engineering teams, panel member Bryan Lane, director of Data & AI at the General Services Administration (who announced during the session that he is taking on a new role at FDIC), said resiliency is a necessary quality.\\xa0\\xa0\\nLane is a technology executive within the GSA IT Modernization Centers of Excellence (CoE) with over 15 years of experience leading advanced analytics and technology initiatives. He has led the GSA partnership with the DoD Joint Artificial Intelligence Center (JAIC).\\xa0[Ed. Note: Known as “the Jake.”]\\xa0Lane also is the founder of\\xa0DATA XD.\\xa0He also has experience in industry, managing acquisition portfolios.\\xa0\\xa0\\xa0\\n“The most important thing about resilient teams going on an AI journey is that you need to be ready for the unexpected, and the mission persists,” he said.\\xa0“If you are all aligned on the importance of the mission, the team can be held together.”\\xa0\\xa0\\nGood Sign that Team Members Acknowledge Having “Never Done This Before”\\xa0\\xa0\\nRegarding mindset, he said more of his team members are coming to him and saying, “I’ve never done this before.” He sees that as a good sign that offers an opportunity to talk about risk and alternative solutions. “When your team has the psychological safety to say that they don’t know something,” Lane sees it as positive. “The focus is always on what you have done and what you have delivered. Rarely is the focus on what you have not done before and what you want to grow into,” he said,\\xa0\\xa0\\nAronson has found it challenging to get AI projects off the ground. “It’s hard to tell management that you have a use case or problem to solve and want to go at it, and there is a 50-50 chance it will get done, and you don’t know how much it’s going to cost,” she said. “It comes down to articulating the rationale and convincing others it’s the right thing to do to move forward.”\\xa0\\xa0\\nRao said he talks to students about experimentation and having an experimental mindset. “AI tools can be easily accessible, but they can mask the challenges you can encounter. When you apply the vision API, for example in the context of challenges in your business or government agency, things may not be smooth,” he said.\\xa0\\xa0\\nModerator\\xa0Dzombak\\xa0asked the panelists how they build teams. Arson said, “You need a mix of people.” She has tried “communities of practice” around solving specific problems, where people can come and go. “You bring people together around a problem and not a tool,” she said.\\xa0\\xa0\\nLane seconded this. “I really have stopped focusing on tools in general,” he said. He ran experiments at JAIC in accounting, finance and other areas. “We found it’s not really about the tools. It’s about getting the right people together to understand the problems, then looking at the tools available,” he said.\\xa0\\xa0\\nLane said he sets up “cross-functional teams” that are “a little more formal than a community of interest.”\\xa0He has found them to be effective for working together on a problem for maybe 45 days. He also likes working with customers of the needed services inside the organization, and has seen customers learn about data management and AI as a result. “We will pick up one or two along the way who become advocates for accelerating AI throughout the organization,” Lane said.\\xa0\\xa0\\nLane sees it taking five years to work out proven methods of thinking, working,\\xa0and best practices for developing AI systems to serve the government. He mentioned\\xa0The Opportunity Project\\xa0(TOP) of the US Census Bureau, begun in 2016 to work on challenges such as ocean plastic pollution, COVID-19 economic recovery and disaster response. TOP has engaged in over 135 public-facing projects in that time, and has over 1,300 alumni including developers, designers, community leaders, data and policy experts, students and government agencies.\\xa0\\xa0\\xa0\\n“It’s based on a way of thinking and how to organize work,” Lane said. “We have to scale the model of delivery, but five years from now, we will have enough proof of concept to know what works and what does not.”\\xa0\\nLearn more at\\xa0AI World Government, at the\\xa0Software Engineering Institute, at\\xa0DATA XD\\xa0and at\\xa0The Opportunity Project.\\xa0\\nHow Accountability Practices\\xa0Are\\xa0Pursued by AI Engineers in the Federal Government\\nBy\\xa0John P. Desmond,\\xa0AI Trends\\xa0Editor\\xa0\\xa0\\xa0\\nTwo experiences of how AI developers within the federal government are pursuing AI accountability practices were outlined at the\\xa0AI World Government\\xa0event held virtually and in-person this week in Alexandria, Va.\\xa0\\nTaka Ariga, chief data scientist and director, US\\xa0Government Accountability Office\\nTaka Ariga, chief data scientist and director at the US\\xa0Government Accountability Office,\\xa0described an AI accountability framework he uses within his agency and plans to make available to others.\\xa0\\xa0\\nAnd Bryce\\xa0Goodman, chief strategist for AI and machine learning at the\\xa0Defense Innovation Unit\\xa0(DIU), a unit of the Department of Defense founded to help the US military make faster use of emerging commercial technologies, described work in his unit to apply principles of AI development to terminology that an engineer can apply.\\xa0\\xa0\\nAriga, the first chief data scientist appointed to the US Government Accountability Office and director of the GAO’s Innovation Lab, discussed an\\xa0AI Accountability Framework\\xa0he helped to develop by convening a forum of experts in the government, industry, nonprofits, as well as federal inspector general officials and AI experts.\\xa0\\xa0\\xa0\\n“We are adopting an auditor’s perspective on the AI accountability framework,” Ariga said. “GAO is in the business of verification.”\\xa0\\xa0\\nThe effort to produce a formal framework began in September 2020 and included 60%\\xa0women, 40%\\xa0of whom were underrepresented minorities, to discuss over two days. The effort was spurred by a desire to ground the AI accountability framework in the reality of an engineer’s day-to-day work. The resulting framework was first published in June as what Ariga described as “version 1.0.”\\xa0\\xa0\\nSeeking to Bring a “High-Altitude Posture” Down to Earth\\xa0\\xa0\\n“We found the AI accountability framework had a very high-altitude posture,” Ariga said. “These are laudable ideals and aspirations, but what do they mean to the day-to-day AI practitioner? There is a gap, while we see AI proliferating across the government.”\\xa0\\xa0\\n“We landed on a lifecycle approach,” which steps through stages of design, development, deployment and continuous monitoring. The development effort stands on four “pillars” of Governance, Data, Monitoring and Performance.\\xa0\\xa0\\nGovernance reviews what the organization has put in place to oversee the AI efforts. “The chief AI officer might be in place, but what does it mean? Can the person make changes? Is it multidisciplinary?”\\xa0 At a system level within this pillar, the team will review individual AI models to see if they were “purposely deliberated.”\\xa0\\xa0\\nFor the Data pillar,\\xa0his team will examine how the training data was evaluated, how representative it is, and is it functioning as intended.\\xa0\\xa0\\nFor the Performance pillar, the team will consider the “societal impact” the AI system will have in deployment, including whether it risks a violation of the Civil Rights Act. “Auditors have a long-standing track record of evaluating equity. We grounded the evaluation of AI to a proven system,” Ariga said.\\xa0\\xa0\\xa0\\nEmphasizing the importance of continuous monitoring, he said, “AI is not a technology you deploy and forget.” he said. “We are preparing to continually monitor for model drift and the fragility of algorithms, and we are scaling the AI appropriately.” The evaluations will determine whether the AI system continues to meet the need “or whether a sunset is more appropriate,” Ariga said.\\xa0\\xa0\\nHe is part of the discussion with NIST on an overall government AI accountability framework. “We don’t want an ecosystem of confusion,” Ariga said. “We want a whole-government approach. We feel that this is a useful first step in pushing high-level ideas down to an altitude meaningful to the practitioners of AI.”\\xa0\\xa0\\nDIU Assesses Whether Proposed Projects Meet Ethical AI Guidelines\\xa0\\xa0\\nBryce Goodman, chief strategist for AI and machine learning, the\\xa0Defense Innovation Unit\\nAt the DIU, Goodman is involved in a similar effort to develop guidelines for developers of AI projects within the government.\\xa0\\xa0\\xa0\\nProjects Goodman has been involved with implementation of AI for humanitarian assistance and disaster response, predictive maintenance,\\xa0to\\xa0counter-disinformation,\\xa0and predictive health. He heads the Responsible AI Working Group. He is a faculty member of Singularity University, has a wide range of consulting clients from inside and outside the government, and holds a PhD in AI and Philosophy from the University of Oxford.\\xa0\\xa0\\nThe DOD in February 2020 adopted five areas of\\xa0Ethical Principles for AI\\xa0after 15 months of consulting with AI experts in commercial industry, government academia and the American public.\\xa0 These areas are: Responsible, Equitable, Traceable, Reliable and Governable.\\xa0\\xa0\\xa0\\n“Those are well-conceived, but it’s not obvious to an engineer how to translate them into a specific project requirement,” Good said in a presentation on Responsible AI Guidelines at the AI World Government event. “That’s the gap we are trying to fill.”\\xa0\\nBefore the DIU even considers a project, they run through the ethical principles to see if it passes muster. Not all projects do. “There needs to be an option to say the technology is not there or the problem is not compatible with AI,” he said.\\xa0\\xa0\\xa0\\nAll project stakeholders, including from commercial vendors and within the government, need to be able to test and validate and go beyond minimum legal requirements to meet the principles. “The law is not moving as fast as AI, which is why these principles are important,” he said.\\xa0\\xa0\\nAlso, collaboration is going on across the government to ensure values are being preserved and maintained. “Our intention with these guidelines is not to try to achieve perfection, but to avoid catastrophic consequences,” Goodman said. “It can be difficult to get a group to agree on what the best outcome is, but it’s easier to get the group to agree on what the worst-case outcome is.”\\xa0\\xa0\\nThe DIU guidelines along with case studies and supplemental materials will be published on the DIU website “soon,” Goodman said, to help others leverage the experience.\\xa0\\xa0\\nHere are Questions DIU Asks Before Development Starts\\xa0\\xa0\\nThe first step in the guidelines is to define the task.\\xa0 “That’s the single most important question,” he said. “Only if there is an advantage, should you use AI.”\\xa0\\nNext is a benchmark, which needs to be set up front to know if the project\\xa0has delivered.\\xa0\\xa0\\xa0\\nNext, he evaluates ownership of the candidate data. “Data is critical to the AI system and is the place where a lot of problems can exist.” Goodman said. “We need a certain contract on who owns the data. If ambiguous, this can lead to problems.”\\xa0\\xa0\\nNext, Goodman’s team wants a sample of data to evaluate. Then, they need to know how and why the information was collected. “If consent was given for one purpose, we cannot use it for another purpose without re-obtaining consent,” he said.\\xa0\\xa0\\nNext, the team asks if the responsible stakeholders are identified, such as pilots who could be affected if a component fails.\\xa0\\xa0\\xa0\\nNext, the responsible mission-holders must be identified. “We need a single individual for this,” Goodman said. “Often we have a tradeoff between the performance of an algorithm and its\\xa0explainability. We might have to decide between the two. Those kinds of decisions have an ethical component and an operational component. So we need to have someone who is accountable for those decisions, which is consistent with the chain of command in the DOD.”\\xa0\\xa0\\xa0\\nFinally, the DIU team requires a process for rolling back if things go wrong. “We need to be cautious about abandoning the previous system,” he said.\\xa0\\xa0\\xa0\\nOnce all these questions are answered in a satisfactory way, the team moves on to the development phase.\\xa0\\xa0\\nIn lessons learned, Goodman said, “Metrics are key. And simply measuring accuracy might not be adequate. We need to be able to measure success.”\\xa0\\nAlso, fit the technology to the task. “High risk applications require low-risk technology. And when potential harm is significant, we need to have high confidence in the technology,” he said.\\xa0\\xa0\\nAnother lesson learned is to set expectations with commercial vendors. “We need vendors to be transparent,” he said. ”When someone says they have a proprietary algorithm they cannot tell us about, we are very wary. We view the relationship as a collaboration. It’s the only way we can ensure\\xa0that the AI\\xa0is developed responsibly.”\\xa0\\xa0\\nLastly, “AI is not magic. It will not solve everything. It should only be used when necessary and only when we can prove it will provide an advantage.”\\xa0\\xa0\\nLearn more at\\xa0AI World Government, at the\\xa0Government Accountability Office,\\xa0at the\\xa0AI Accountability Framework\\xa0and at the\\xa0Defense Innovation Unit\\xa0site.\\xa0\\nStartup:\\xa0AssemblyAI\\xa0Represents New Generation Speech Recognition\\nBy AI Trends Staff\\xa0\\xa0\\nAdvances in the AI behind speech recognition are driving growth in the market, attracting venture capital and funding startups, posing challenges to established players.\\xa0\\xa0\\nThe growing acceptance and use of speech recognition devices are driving the market, which according to an estimate by Meticulous Research is expected to reach $26.8 billion globally by 2025, according to a recent account in\\xa0Analytics Insight. Better speed and accuracy are among the benefits of the evolving technology.\\xa0\\nDylan Fox, CEO and Founder,\\xa0AssemblyAI\\nOne company in the throes of this new growth,\\xa0AssemblyAI\\xa0of San Francisco, is offering an API for speech recognition capable of transcribing videos, podcasts, phone calls,\\xa0and remote meetings. The company was founded by CEO Dylan Fox in 2017 and has received backing from Y Combinator, a startup accelerator, as well as NVIDIA.\\xa0\\xa0\\xa0\\nFox has an unusual background for a high tech entrepreneur. He is a graduate of George Washington University with a degree in business administration, business economics,\\xa0and public policy. He got a job as a software engineer for machine learning in the emerging product lab of Cisco in San Francisco, working on deep neural networks and machine learning. He got the idea for\\xa0AssemblyAi\\xa0and attracted capital from Y Combinator, which enabled him to hire data scientists and data engineers to get the technology off the ground.\\xa0\\xa0\\xa0\\nAsked in an interview with\\xa0AI Trends\\xa0how he made this transition from undergrad in business administration and economics to high-tech entrepreneur, Fox said, “I taught myself how to program, which led me to a path of machine learning. I was looking for a harder software challenge, which led to natural language processing, which took me to Cisco.” They were working on Siri for the Enterprise for Apple at the time,\\xa0\\nTo speed up the work, Cisco was looking to acquire speech recognition software; Fox was in the catbird’s seat for the search. “We looked at Nuance,” for example, acknowledged as a market leader and owner of more speech recognition software than its competitors. (The acquisition of Nuance by Microsoft for $19.6 billion is expected to be finalized by year-end.) The young, budding entrepreneur was not impressed. “It was crazy how bad all the options were from an accuracy and a developer point of view,” he stated.\\xa0\\xa0\\nHe was impressed by Twilio, a San Francisco-based company founded in 2008, which that year released the Twilio Voice API to make and receive phone calls hosted in the cloud. The company has since raised $103 million in venture capital. “They were setting new standards for a good API for developers,” Fox said.\\xa0\\xa0\\nFox’s idea was to use AI and machine learning to achieve “super accurate results, and make it easy for developers to incorporate the API into their products. One customer is\\xa0CallRail, offering call tracking and marketing analytics software, which plans to incorporate\\xa0AssembyAI’s\\xa0API to gain insight into why people are calling. Other customers include NBC and the Wall Street\\xa0Journal, using the product to transcribe content and interviews, and provide closed captioning.\\xa0\\xa0\\n“We’ve been working on building as close to human speech recognition quality as possible. It’s been a lot of work” Fox said. He expects to reach that plateau in 2022.\\xa0\\xa0\\nHe targets companies incorporating speech recognition into their products and makes it easy to buy. Customers pay on a usage basis; for every second of audio transcribed,\\xa0AssemblyAI\\xa0charges a fraction of a penny. Clients get billed monthly. If a customer uses 10 hours a month, it costs about nine dollars. If a customer uses a million hours a month, it costs about $900,000.\\xa0\\xa0\\xa0\\xa0\\nVoice recognition is a hot market. “Many new startups are being launched,” Fox said, providing opportunity. “Many interesting new businesses are being built on voice data.”\\xa0\\xa0\\xa0\\nAssemblyAI’s\\xa0product can detect sensitive topics such as hate speech and profanity, so customers can save on human content moderation.\\xa0\\nAsked to describe what differentiates his technology, Fox said, “We are an experienced team of deep learning researchers,” with experience from companies including BMW, Apple,\\xa0and Facebook. “We build very large, very accurate deep learning models that have recognition results far more accurate than a traditional machine learning approach. We build really large models using advanced neural network technologies.” He compared the approach to what\\xa0OpenAI\\xa0uses to develop its GPT-3 large language model.\\xa0\\xa0\\nIn addition, they build AI features on top of the transcriptions, to provide summaries of audio and video content, which can be searched and indexed. “It goes beyond just transcription,” Fox said.\\xa0\\xa0\\xa0\\nThe company currently has 25 employees and expects to double in about four months. Business has been good. “There is an explosion of audio and video data online and customers want to be able to take advantage of it, so we see a lot of demand,” Fox said.\\xa0\\nLearn more at\\xa0AssemblyAI.\\xa0\\nPursuit of Autonomous Cars May Pose Risk of AI Tapping Forbidden Knowledge\\nBy Lance Eliot, the AI Trends Insider\\xa0\\xa0\\xa0\\xa0\\nAre there things that we must not know?\\xa0\\xa0\\xa0\\nThis is an age-old question.\\xa0Some assert that there is the potential for knowledge that ought to not be known. In other words, there are ideas, concepts, or mental formulations that should we become aware of that knowledge it could be our downfall. The discovery or invention of some new innovation or way of thinking could be unduly dangerous. It would be best to not go there, as it were, and avoid ever landing on such knowledge: forbidden knowledge.\\xa0\\xa0\\xa0\\nThe typical basis for wanting to forbid the discovery or emergence of forbidden knowledge is that the adverse consequences are overwhelming.\\xa0The end result is so devastating and undercutting that the bad side outweighs the good that could be derived from the knowledge.\\xa0\\xa0\\xa0\\nIt is conceivable that there might be knowledge that is so bad that it has no good possibilities at all. Thus, rather than trying to balance or weigh the good versus the bad, the knowledge has no counterbalancing effects. It is just plain bad.\\xa0\\xa0\\xa0\\xa0\\nWe are usually faced with the matter of knowledge that has both the good and the bad as to how it might be utilized or employed. This then leads to a dogged debate about whether the bad is so bad that it outweighs the good. On top of this, there is the unrealized bad and the unrealized good, which could be differentiated from the realized bad and the realized good (in essence, the knowledge might be said to be either good or bad, though this is purely conceptual and not put into real-world conditions to attest or become realized as such).\\xa0\\xa0\\xa0\\nThe most familiar reference to forbidden knowledge is likely evoked via the Garden of Eden and the essence of forbidden fruit.\\xa0\\nA contemporary down-to-earth example often discussed about forbidden knowledge consists of the atomic bomb. Some suggest that the knowledge devised or invented to ultimately produce a nuclear bomb provides a quite visible and overt\\xa0exemplar\\xa0of the problems associated with knowledge. Had the knowledge about being able to attain an atomic bomb never been achieved, there presumably would not be any such device. In debates about the topic, it is feasible to take a resolute position favoring the attainment of an atomic bomb and there are equally counterbalancing contentions sternly disfavoring this attainment.\\xa0\\xa0\\xa0\\nOne perplexing problem about forbidden knowledge encompasses knowing beforehand the kind of knowledge that might end up in the forbidden category. This is a bit of a Catch-22 or circular type of puzzle. You might discover knowledge and then ascertain it ought to be forbidden, but the cat is kind of out of the bag due to the knowledge having been already uncovered or rendered. Oopsie, you should have in advance decided to not go there and therefore have avoided falling into the forbidden knowledge zone.\\xa0\\xa0\\xa0\\nOn a related twist, suppose that we could beforehand declare what type of knowledge is to be averted because it is predetermined as forbidden. Some people might accidentally discover the knowledge, doing so by happenstance, and now they’ve again potentially opened Pandora’s box. Meanwhile, there might be others that, regardless of being instructed to not derive any such stated forbidden knowledge, do so anyway.\\xa0\\xa0\\xa0\\nThis then takes us to a frequently used retort about forbidden knowledge, namely, if you don’t seek the forbidden knowledge there is a chance that someone else will, and you’ll be left in the dust because they got there first. In that preemptive viewpoint, the claim is that it is better to go ahead and forage for the forbidden knowledge and not get caught behind the eight-ball when someone else beats you to the punch.\\xa0\\xa0\\xa0\\nRound and round we can go.\\xa0\\xa0\\xa0\\nThe main thing that most would agree to is that knowledge is power.\\xa0\\nThe alluded to power could be devastating and destroy others, possibly even leading to the self-destruction of the wielder of the knowledge. Yet there is also the potential for knowledge to be advantageous and save humanity from other ills.\\xa0\\xa0\\xa0\\nMaybe we ought to say that knowledge is powerful. Despite that perhaps obvious proclamation, we might also add that knowledge can decay and gradually become outdated or less potent. Furthermore, since we are immersing ourselves herein into the cauldron of the love-it or hate-it knowledge conundrum, knowledge can be known and yet undervalued, perhaps only becoming valuable at a later time and in a different light.\\xa0\\xa0\\xa0\\nThere is a case to be made that humankind has a seemingly irresistible allure toward more and more knowledge. Some philosophers suggest you are unlikely to be able to bottle up or stop this quest for knowledge. If that’s the manner of how humanity will be, this implies that you must find ways to control or contain knowledge and give up on the belief that we can altogether avoid landing into forbidden knowledge.\\xa0\\xa0\\xa0\\nThere is a relatively new venue prompting a lot of anxious hand wringing pertaining to forbidden knowledge, namely the advent of Artificial Intelligence (AI).\\xa0\\xa0\\xa0\\nHere’s the rub.\\xa0\\xa0\\xa0\\nSuppose that we are able to craft AI systems that make use of knowledge about how humans can think. There are two major potential gotchas.\\xa0\\xa0\\xa0\\nFirst, the AI systems themselves might end up doing good things, and they also might end up doing bad things. If the bad outweighs the good, maybe we are shooting our own foot by allowing AI to be put into use.\\xa0\\nSecondly, perhaps this could be averted entirely by deciding that there is forbidden knowledge about how humans think, and we ought to not discover or reveal those mental mechanisms. It is the classic stepwise logic that step A axiomatically leads to step B. We won’t need to worry about AI systems (step B), if we never allow the achievement of step A (figuring out how humans think and then imparting that into computers), since the attainment of AI would presumably not arise.\\xa0\\xa0\\xa0\\nIn any case, there is inarguably a growing concern about AI.\\xa0\\xa0\\xa0\\nPlenty of efforts are underway to promulgate a semblance of AI Ethics, meaning that those developers and indeed all stakeholders that are conceiving of, building, and putting into use an AI system needs to consider the ethical aspects of their efforts. AI systems have been unveiled and placed into use replete with all sorts of notable concerns, including incorporating unsavory biases and other problems.\\xa0\\xa0\\xa0\\nAll told, one bold and somewhat stark argument is that the pursuit of AI is being underpinned or stoked by the discovery and then exploitation of forbidden knowledge.\\xa0\\xa0\\xa0\\nBe aware that many would scoff at this allegation.\\xa0\\xa0\\xa0\\nThere are those deeply immersed in the field of AI who would laugh that there is anything in the entirety of AI to date that constitutes potential forbidden knowledge. The technology and technological elements are relatively ho-hum, they would argue. You would be hard-pressed to pinpoint what AI-related knowledge that is already known comes anywhere near the ballpark of forbidden knowledge.\\xa0\\nFor those that concur with that posture, there is the reply that it might be future knowledge that we have not yet attained that is the upcoming forbidden kind, and for which we are heading pell-mell down that path. Thus, they would concede that we haven’t arrived at forbidden knowledge at this juncture, but this is an insidious distractor due to the aspect that it masks or belies our qualms entailing the possibility that it lays in wait at the next turn.\\xa0\\nOne area where AI is being actively used is to create Autonomous Vehicles (AVs).\\xa0\\nWe are gradually seeing the emergence of self-driving cars and can expect self-driving trucks, self-driving motorcycles, self-driving drones, self-driving planes, self-driving ships, self-driving submersibles, etc.\\xa0\\xa0\\xa0\\nToday’s conventional cars are eventually going to give way to the advent of AI-based, true self-driving cars. Self-driving cars are driven via an AI driving system. There isn’t a need for a human driver at the wheel, and nor is there a provision for a human to drive the vehicle.\\xa0\\xa0\\xa0\\nHere’s an intriguing question that has arisen:\\xa0Might the crafting of AI-based true self-driving cars take us into the realm of discovering forbidden knowledge, and if so, what should be done about this?\\xa0\\xa0\\xa0\\nBefore jumping into the details, I’d like to clarify what is meant when referring to true self-driving cars.\\xa0\\xa0\\xa0\\nFor my framework about AI autonomous cars, see the link here:\\xa0https://aitrends.com/ai-insider/framework-ai-self-driving-driverless-cars-big-picture/\\xa0\\xa0\\xa0\\nWhy this is a moonshot effort, see my explanation here:\\xa0https://aitrends.com/ai-insider/self-driving-car-mother-ai-projects-moonshot/\\xa0\\xa0\\xa0\\nFor more about the levels as a type of Richter scale, see my discussion here:\\xa0https://aitrends.com/ai-insider/richter-scale-levels-self-driving-cars/\\xa0\\xa0\\xa0\\nFor the argument about bifurcating the levels, see my explanation here:\\xa0https://aitrends.com/ai-insider/reframing-ai-levels-for-self-driving-cars-bifurcation-of-autonomy/\\xa0\\xa0\\xa0\\nUnderstanding The Levels Of Self-Driving Cars\\xa0\\xa0\\xa0\\nAs a clarification, true self-driving cars are ones where the AI drives the car entirely on its own and there isn’t any human assistance during the driving task.\\xa0\\xa0\\xa0\\nThese driverless vehicles are considered Level 4 and Level 5, while a car that requires a human driver to co-share the driving effort is usually considered at Level 2 or Level 3. The cars that co-share the driving task are described as being semi-autonomous, and typically contain a variety of automated add-on’s that are referred to as ADAS (Advanced Driver-Assistance Systems).\\xa0\\nThere is not yet a true self-driving car at Level 5, which we don’t yet even know if this will be possible to achieve, and nor how long it will take to get there.\\xa0\\nMeanwhile, the Level 4 efforts are gradually trying to get some traction by undergoing very narrow and selective public roadway trials, though there is controversy over whether this testing should be allowed per se (we are all life-or-death guinea pigs in an experiment taking place on our highways and byways, some contend).\\xa0\\xa0\\xa0\\nSince semi-autonomous cars require a human driver, the adoption of those types of cars won’t be markedly different from driving conventional vehicles, so there’s not much new per se to cover about them on this topic (though, as you’ll see in a moment, the points next made are generally applicable).\\xa0\\xa0\\nFor semi-autonomous cars, it is important that the public needs to be forewarned about a disturbing aspect that’s been arising lately, namely that despite those human drivers that keep posting videos of themselves falling asleep at the wheel of a Level 2 or Level 3 car, we all need to avoid being misled into believing that the driver can take away their attention from the driving task while driving a semi-autonomous car.\\xa0\\xa0\\xa0\\nYou are the responsible party for the driving actions of the vehicle, regardless of how much automation might be tossed into a Level 2 or Level 3.\\xa0\\xa0\\xa0\\nFor why remote piloting or operating of self-driving cars is generally eschewed, see my explanation here:\\xa0https://aitrends.com/ai-insider/remote-piloting-is-a-self-driving-car-crutch/\\xa0\\xa0\\xa0\\nTo be wary of fake news about self-driving cars, see my tips here:\\xa0https://aitrends.com/ai-insider/ai-fake-news-about-self-driving-cars/\\xa0\\nThe ethical implications of AI driving systems are significant, see my indication here:\\xa0http://aitrends.com/selfdrivingcars/ethically-ambiguous-self-driving-cars/\\xa0\\xa0\\xa0\\nBe aware of the pitfalls of normalization of deviance when it comes to self-driving cars, here’s my call to arms:\\xa0https://aitrends.com/ai-insider/normalization-of-deviance-endangers-ai-self-driving-cars/\\xa0\\xa0\\xa0\\nSelf-Driving Cars And Forbidden Knowledge\\xa0\\xa0\\xa0\\nFor Level 4 and Level 5 true self-driving vehicles, there won’t be a human driver involved in the driving task.\\xa0\\xa0\\xa0\\nAll occupants will be passengers.\\xa0\\xa0\\xa0\\nThe AI is doing the driving.\\xa0\\xa0\\xa0\\nOne aspect to immediately discuss entails the fact that the AI involved in today’s AI driving systems is not sentient. In other words, the AI is altogether a collective of computer-based programming and algorithms, and most assuredly not able to reason in the same manner that humans can.\\xa0\\xa0\\xa0\\nWhy this added emphasis about the AI not being sentient?\\xa0\\xa0\\xa0\\nBecause I want to underscore that when discussing the role of the AI driving system, I am not ascribing human qualities to the AI. Please be aware that there is an ongoing and dangerous tendency these days to anthropomorphize AI. In essence, people are assigning human-like sentience to today’s AI, despite the undeniable and inarguable fact that no such AI exists as yet.\\xa0\\nWith that clarification, you can envision that the AI driving system won’t natively somehow “know” about the facets of driving. Driving and all that it entails will need to be programmed as part of the hardware and software of the self-driving car.\\xa0\\xa0\\xa0\\nLet’s dive into the myriad of aspects that come to play on this topic.\\xa0\\xa0\\xa0\\nThe crux here is whether there is forbidden knowledge lurking within the existing and ongoing efforts to achieve AI-based true self-driving cars. We’ll begin by considering the status of the existent efforts and then shift into speculation about the future of such efforts.\\xa0\\xa0\\xa0\\nPer the earlier discussion about whether there is forbidden knowledge that has already perchance been revealed or discovered via the efforts toward today’s AI systems all told, the odds seem stacked against such a notion at this time, and likewise the same could be said about the pursuit of self-driving cars. Essentially, there doesn’t seem to be any forbidden knowledge per se that has been discovered or revealed during the self-driving cars development journey so far, at least with respect to the conventional wisdom about what forbidden knowledge might entail.\\xa0\\xa0\\xa0\\nOne could try to argue that it is premature to reach such a conclusion and that we might, later on, realize that forbidden knowledge was indeed uncovered or invented, and we just didn’t realize it. That is a rabbit hole that we’ll not go down for now, though you are welcome to keep that presumption at hand if so desired.\\xa0\\xa0\\xa0\\nThat covers the present, and ergo we can turn our attention to the future.\\xa0\\xa0\\xa0\\nGenerally, the efforts underway today have been primarily aimed at achieving Level 4, and the hope is that someday we will go beyond Level 4 and attain Level 5. To get to a robust Level 4, most would likely say that we can continue the existing approaches.\\xa0\\xa0\\xa0\\nNot everyone would agree with that assumption. Some believe that we will get stymied within Level 4. Furthermore, the inability to produce a robust Level 4 will ostensibly preclude us from being able to attain Level 5. There is a contingent that suggests we need to start over and set aside the existing AI approaches, which otherwise are taking us down a dead-end or blind alley. An entirely new way of devising AI for autonomous vehicles is needed, they would vehemently argue.\\xa0\\xa0\\xa0\\nThere is also a contingent that asserts the Level 4 itself is a type of dead-end. In brief, those proponents would say that we will achieve a robust Level 4, though this will do little good towards attaining Level 5. Once again, their view is similar to the preceding remark that we will need to come up with some radically new understandings about AI and the nature of cognitive acumen in order to get self-driving cars into the Level 5 realm.\\xa0\\xa0\\xa0\\nAha, it is within that scope of having to dramatically revisit and revamp what AI is and how we can advance significantly in the pursuit of AI that the forbidden knowledge question can reside. In theory, perhaps the only means of attaining Level 5 will be to strike upon some knowledge that we do not yet know and that for which bodes for falling within the realm of forbidden knowledge.\\xa0\\nTo some, this seems\\xa0farfetched.\\xa0\\nThey would emphatically ask; just what kind of knowledge are you even talking about?\\xa0\\xa0\\xa0\\nHere’s their logic. Humans are able to drive cars. Humans do not seem to need or possess forbidden knowledge as it relates to the act of driving a car. Therefore, it seems ridiculous on the face of things to claim or contend that the only means to get AI-based true self-driving cars, for which they would be driven on an equal basis as human drivers can drive, would require the discovery or invention of whatever might be construed as forbidden knowledge.\\xa0\\xa0\\xa0\\nSeems like pretty ironclad logic.\\xa0\\xa0\\xa0\\nThe retort is that humans have common-sense reasoning. With common-sense reasoning, we seem to know all sorts of things about the world around us. When we drive a car, we intrinsically make use of our common-sense reasoning. We take for granted that we do have a common-sense reasoning capacity, and similarly, we take for granted that it integrally comes to the fore when driving a car.\\xa0\\xa0\\xa0\\nAttempts to create AI that can exhibit the equivalent of human common-sense reasoning have made ostensibly modest or some would say minimal progress (to clarify, those pursuing this line of inquiry are to be lauded, it’s just that no earth-shattering breakthroughs seem to have been reached and none seem on the immediate horizon). Yes, there are some quite fascinating and exciting efforts underway, but when you measure those against the everyday common-sense reasoning of humans, there is no comparison. They are night and day. If this were a contest, the humans win hands down, no doubt about it, and the AI experimental efforts encompassing common-sense reasoning are mere playthings in contrast.\\xa0\\xa0\\xa0\\nYou might have gleaned where this line of thought is headed.\\xa0\\xa0\\xa0\\nThe belief by some is that until we crack open the enigma of common-sense reasoning, there is little chance of achieving a Level 5, and perhaps also this will hold back the Level 4 too. It could be that a secret ingredient of sorts for autonomous vehicles is the need to figure out and include common-sense reasoning into AI-based driving and piloting systems.\\xa0\\xa0\\xa0\\nIf you buy into that logic, the added assertion is that maybe within the confines of how common-sense reasoning takes place is a semblance of forbidden knowledge. On the surface, you would certainly assume that if we knew entirely how common-sense reasoning works, there would not appear to be any cause for alarm or concern. The act of employing common-sense reasoning does not seem to necessarily embody forbidden knowledge.\\xa0\\xa0\\xa0\\nThe twist is that perhaps the underlying cognitive means that gives rise to the advent of common-sense reasoning is where there is forbidden knowledge. Some deep-rooted elements in the nature of human thought and how we form common sense and undertake common-sense reasoning are possibly a type of knowledge that will be shown as crucial and a forbidden knowledge formulation.\\xa0\\xa0\\xa0\\nFor more details about ODDs, see my indication at this link here:\\xa0https://www.aitrends.com/ai-insider/amalgamating-of-operational-design-domains-odds-for-ai-self-driving-cars/\\xa0\\nOn the topic of off-road self-driving cars, here’s my details elicitation:\\xa0https://www.aitrends.com/ai-insider/off-roading-as-a-challenging-use-case-for-ai-autonomous-cars/\\xa0\\nI’ve urged that there must be a Chief Safety Officer at self-driving car makers, here’s the scoop:\\xa0https://www.aitrends.com/ai-insider/chief-safety-officers-needed-in-ai-the-case-of-ai-self-driving-cars/\\xa0\\nExpect that lawsuits are going to gradually become a significant part of the self-driving car industry, see my explanatory details here:\\xa0http://aitrends.com/selfdrivingcars/self-driving-car-lawsuits-bonanza-ahead/\\xa0\\xa0\\xa0\\nConclusion\\xa0\\xa0\\xa0\\nWow, that’s quite a bit of pondering, contemplation, and (some would say) wild thinking.\\xa0\\xa0\\xa0\\nMaybe so, but it is a consideration that some would wish that we gave at least some credence toward and devoted attention to. There is the angst that we might find ourselves by happenstance stumbling into forbidden knowledge on these voracious self-driving cars quests.\\xa0\\xa0\\xa0\\nFor however you might emphasize that having AI-based true self-driving cars will be a potential blessing, proffering mobility-for-all and leading to reducing the number of car crash-related fatalities, there is a sneaking suspicion that it will not be all-good. The catch or trap could be that there is some kind of forbidden knowledge that will get brought to the eye and we will inevitably kick ourselves that we didn’t see it coming.\\xa0\\xa0\\xa0\\nThe next time you are munching on a delicious apple, give some thought to whether self-driving cars might be forbidden fruit.\\xa0\\xa0\\xa0\\nWe are on the path to taking a big bite, and we’ll have to see where that takes us.\\xa0\\nCopyright 2021 Dr. Lance Eliot\\xa0\\xa0\\nhttp://ai-selfdriving-cars.libsyn.com/website\\xa0\\n'"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feed_content = \"\"\n",
    "for article in blog_articles:\n",
    "    feed_content += article[\"title\"] + \"\\n\"\n",
    "    feed_content += article[\"content\"] + \"\\n\"\n",
    "feed_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d91a46c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'best practices building ai development platform government john p. desmond ai trends editor ai stack defined carnegie mellon university fundamental approach taken us army ai development platform efforts according isaac faber chief data scientist us army ai integration center speaking ai world government event held in-person virtually alexandria va. last week isaac faber chief data scientist us army ai integration center “ want move army legacy systems digital modernization one biggest issues found difficulty abstracting away differences applications ” said “ important part digital transformation middle layer platform makes easier cloud local computer. ” desire able move software platform another platform ease new smartphone carries user ’ contacts histories ethics cuts across layers ai application stack positions planning stage top followed decision support modeling machine learning massive data management device layer platform bottom “ advocating think stack core infrastructure way applications deployed siloed approach ” said “ need create development environment globally-distributed workforce. ” army working common operating environment software coes platform first announced 2017 design dod work scalable agile modular portable open “ suitable broad range ai projects ” faber said executing effort “ devil details ” said army working cmu private companies prototype platform including visimo coraopolis pa. offers ai development services faber said prefers collaborate coordinate private industry rather buying products shelf “ problem stuck value provided one vendor usually designed challenges dod networks ” said army trains range tech teams ai army engages ai workforce development efforts several teams including leadership professionals graduate degrees technical staff put training get certified ai users tech teams army different areas focus include general purpose software development operational data science deployment includes analytics machine learning operations team large team required build computer vision system “ folks come workforce need place collaborate build share ” faber said types projects include diagnostic might combining streams historical data predictive prescriptive recommends course action based prediction “ far end ai ’ start ” said faber developer solve three problems data engineering ai development platform called “ green bubble ” deployment platform called “ red bubble. ” “ mutually exclusive interconnected teams different people need programmatically coordinate usually good project team people bubble areas ” said “ done yet try solve green bubble problem makes sense pursue ai operational need. ” asked participant group difficult reach train faber said without hesitation “ hardest reach executives need learn value provided ai ecosystem biggest challenge communicate value ” said panel discusses ai use cases potential panel foundations emerging ai moderator curt savoie program director global smart cities strategies idc market research firm asked emerging ai use case potential jean-charles lede autonomy tech advisor us air force office scientific research said ” would point decision advantages edge supporting pilots operators decisions back mission resource planning. ” krista kinnard chief emerging technology department labor krista kinnard chief emerging technology department labor said “ natural language processing opportunity open doors ai department labor ” said “ ultimately dealing data people programs organizations. ” savoie asked big risks dangers panelists see implementing ai anil chaudhry director federal ai implementations general services administration gsa said typical organization using traditional software development impact decision developer goes far ai “ consider impact whole class people constituents stakeholders simple change algorithms could delaying benefits millions people making incorrect inferences scale ’ important risk ” said said asks contract partners “ humans loop humans loop. ” kinnard seconded saying “ intention removing humans loop ’ really empowering people make better decisions. ” emphasized importance monitoring ai models deployed “ models drift data underlying changes ” said “ need level critical thinking task assess whether ai model acceptable. ” added “ built use cases partnerships across government make sure ’ implementing responsible ai never replace people algorithms. ” lede air force said “ often use cases data exist explore 50 years war data use simulation risk teaching algorithm ‘ simulation real gap ’ real risk sure algorithms map real world. ” chaudhry emphasized importance testing strategy ai systems warned developers “ get enamored tool forget purpose exercise. ” recommended development manager design independent verification validation strategy “ testing focus energy leader leader needs idea mind committing resources justify whether investment success. ” lede air force talked importance explainability “ technologist ’ laws ability ai function explain way human interact important ai partner dialogue instead ai coming conclusion way verifying ” said learn ai world government advance trustworthy ai ml identify best practices scaling ai john p. desmond ai trends editor advancing trustworthy ai machine learning mitigate agency risk priority us department energy doe identifying best practices implementing ai scale priority us general services administration gsa ’ attendees learned two sessions ai world government live virtual event held alexandria va. last week pamela isom director ai technology office doe pamela isom director ai technology office doe spoke advancing trustworthy ai ml techniques mitigating agency risks involved proliferating use ai across agency several years emphasis applied ai data science oversees risk mitigation policies standards involved applying ai save lives fight fraud strengthen cybersecurity infrastructure emphasized need ai project effort part strategic portfolio “ office drive holistic view ai mitigate risk bringing us together address challenges ” said effort assisted doe ’ ai technology office focused transforming doe world-leading ai enterprise accelerating research development delivery adoption ai “ telling organization mindful fact tons tons data might representative ” said team looks examples international partners industry academia agencies outcomes “ trust ” systems incorporating ai “ know ai disruptive trying humans better ” said “ beyond human capability goes beyond data spreadsheets tell ’ going next contemplate ’ powerful ” said result close attention must paid data sources “ ai vital economy national security need precision need algorithms trust need accuracy ’ need biases ” isom said adding “ ’ forget need monitor output models long deployed. ” executive orders guide gsa ai work executive order 14028 detailed set actions address cybersecurity government agencies issued may year executive order 13960 promoting use trustworthy ai federal government issued december 2020 provide valuable guides work help manage risk ai development deployment isom produced ai risk management playbook provides guidance around system features mitigation techniques also filter ethical trustworthy principles considered throughout ai lifecycle stages risk types plus playbook ties relevant executive orders provides examples results came 80 accuracy wanted 90 “ something wrong ” isom said adding “ playbook helps look types problems mitigate risk factors weigh design build project. ” internal doe present agency looking next steps external version “ share federal agencies soon ” said gsa best practices scaling ai projects outlined anil chaudhry director federal ai implementations ai center excellence coe gsa anil chaudhry director federal ai implementations ai center excellence coe gsa spoke best practices implementing ai scale 20 years experience technology delivery operations program management defense intelligence national security sectors mission coe accelerate technology modernization across government improve public experience increase operational efficiency “ business model partner industry subject matter experts solve problems ” chaudhry said adding “ business recreating industry solutions duplicating them. ” coe providing recommendations partner agencies working implement ai systems federal government engages heavily ai development “ ai government landscape vast every federal agency sort ai project going right ” said maturity ai experience varies widely across agencies typical use cases seeing include ai focus increasing speed efficiency cost savings cost avoidance improved response time increased quality compliance one best practice recommended agencies vet commercial experience large datasets encounter government “ ’ talking petabytes exabytes structured unstructured data ” chaudhry said. ed note petabyte 1,000 terabytes “ also ask industry partners strategies processes macro micro trend analysis experience deployment bots robotic process automation demonstrate sustainability result drift data. ” also asks potential industry partners describe ai talent team talent access company weak ai talent chaudhry would ask “ buy something know got wanted way evaluating ” added “ best practice implementing ai defining train workforce leverage ai tools techniques practices define grow mature workforce access talent leads either success failure ai projects especially comes scaling pilot fully deployed system. ” another best practice chaudhry recommended examining industry partner ’ access financial capital “ ai field flow capital highly volatile “ predict project spend x amount dollars year get want ” said ai development team may need explore another hypothesis clean data may transparent potentially biased “ ’ access funding risk project fail ” said another best practice access logistical capital data sensors collect ai iot system “ ai requires enormous amount data authoritative timely direct access data critical ” chaudhry said recommended data sharing agreements place organizations relevant ai system “ might need right away access data could immediately use thought privacy issues need data good practice scaling ai programs ” said final best practice planning physical infrastructure data center space “ pilot need know much capacity need reserve data center many end points need manage ” application scales chaudhry said adding “ ties back access capital best practices. “ learn ai world government promise perils using ai hiring guard data bias ai trends staff ai hiring widely used writing job descriptions screening candidates automating interviews poses risk wide discrimination implemented carefully keith sonderling commissioner us equal opportunity commission message keith sonderling commissioner us equal opportunity commision speaking ai world government event held live virtually alexandria va. last week sonderling responsible enforcing federal laws prohibit discrimination job applicants race color religion sex national origin age disability “ thought ai would become mainstream hr departments closer science fiction two year ago pandemic accelerated rate ai used employers ” said “ virtual recruiting stay. ” ’ busy time hr professionals “ great resignation leading great rehiring ai play role like seen ” sonderling said ai employed years hiring— “ happen overnight. ” —for tasks including chatting applications predicting whether candidate would take job projecting type employee would mapping upskilling reskilling opportunities “ short ai making decisions made hr personnel ” characterize good bad “ carefully designed properly used ai potential make workplace fair ” sonderling said “ carelessly implemented ai could discriminate scale never seen hr professional. ” training datasets ai models used hiring need reflect diversity ai models rely training data company ’ current workforce used basis training “ replicate status quo ’ one gender one race primarily replicate ” said conversely ai help mitigate risks hiring bias race ethnic background disability status “ want see ai improve workplace discrimination ” said amazon began building hiring application 2014 found time discriminated women recommendations ai model trained dataset company ’ hiring record previous 10 years primarily males amazon developers tried correct ultimately scrapped system 2017. facebook recently agreed pay 14.25 million settle civil claims us government social media company discriminated american workers violated federal recruitment rules according account reuters case centered facebook ’ use called perm program labor certification government found facebook refused hire american workers jobs reserved temporary visa holders perm program “ excluding people hiring pool violation ” sonderling said ai program “ withholds existence job opportunity class exercise rights downgrades protected class within domain ” said employment assessments became common world war ii provided high value hr managers help ai potential minimize bias hiring “ time vulnerable claims discrimination employers need careful take hands-off approach ” sonderling said “ inaccurate data amplify bias decision-making employers must vigilant discriminatory outcomes. ” recommended researching solutions vendors vet data risks bias basis race sex factors one example hirevue south jordan utah built hiring platform predicated us equal opportunity commission ’ uniform guidelines designed specifically mitigate unfair hiring practices according account allwork post ai ethical principles website states part “ hirevue uses ai technology products actively work prevent introduction propagation bias group individual continue carefully review datasets use work ensure accurate diverse possible also continue advance abilities monitor detect mitigate bias strive build teams diverse backgrounds diverse knowledge experiences perspectives best represent people systems serve. ” also “ data scientists io psychologists build hirevue assessment algorithms way removes data consideration algorithm contributes adverse impact without significantly impacting assessment ’ predictive accuracy result highly valid bias-mitigated assessment helps enhance human decision making actively promoting diversity equal opportunity regardless gender ethnicity age disability status. ” dr. ed ikeguchi ceo aicure issue bias datasets used train ai models confined hiring dr. ed ikeguchi ceo aicure ai analytics company working life sciences industry stated recent account healthcareitnews “ ai strong data ’ fed lately data backbone ’ credibility increasingly called question today ’ ai developers lack access large diverse data sets train validate new tools. ” added “ often need leverage open-source datasets many trained using computer programmer volunteers predominantly white population algorithms often trained single-origin data samples limited diversity applied real-world scenarios broader population different races genders ages tech appeared highly accurate research may prove unreliable. ” also “ needs element governance peer review algorithms even solid tested algorithm bound unexpected results arise algorithm never done learning—it must constantly developed fed data improve. ” “ industry need become skeptical ai ’ conclusions encourage transparency industry companies readily answer basic questions ‘ algorithm trained basis draw conclusion ” read source articles information ai world government reuters healthcareitnews predictive maintenance proving successful ai use case john p. desmond ai trends editor companies successfully exploiting predictive maintenance systems combine ai iot sensors collect data anticipates breakdowns recommends preventive action break machines fail demonstration ai use case proven value growth reflected optimistic market forecasts predictive maintenance market sized 6.9 billion today projected grow 28.2 billion 2026 according report iot analytics hamburg germany firm counts 280 vendors offering solutions market today projected grow 500 2026. fernando bruegge analyst iot analytics hamburg germany “ research wake-up call claim iot failing ” stated analyst fernando bruegge author report adding “ companies industrial assets sell equipment time invest predictive maintenance-type solutions. ” “ enterprise technology firms need prepare integrate predictive maintenance solutions offerings ” bruegge suggested review specific experience predictive maintenance systems combine ai iot sensors aircraft engine manufacturer rolls-royce deploying predictive analytics help reduce amount carbon engines produce also optimizing maintenance help customers keep planes air longer according recent account cio rolls-royce built intelligent engine platform monitor engine flight gathering data weather conditions pilots flying machine learning applied data customize maintenance regimes individual engines stuart hughes chief information digital officer rolls-royce “ ’ tailoring maintenance regimes make sure ’ optimizing life engine life manual says ” stated stuart hughes chief information digital officer rolls-royce “ ’ truly variable service looking engine individual engine. ” customers seeing less service interruption “ rolls-royce monitoring engines charging per hour least 20 years ” hughes stated “ part business ’ new ’ evolved ’ begun treat engine singular engine ’ much personalization engine. ” predictive analytics applied healthcare well manufacturing industry kaiser permanente integrated managed care consortium based oakland calif. using predictive analytics identify non-intensive care unit icu patients risk rapid deterioration non-icu patients require unexpected transfers icu constitute less 4 total hospital population account 20 hospital deaths according dr. gabriel escobar research scientist division research regional director hospital operations research kaiser permanente northern california kaiser permanente practicing predictive maintenance healthcare kaiser permanente developed advanced alert monitor aam system leveraging three predictive analytic models analyze 70 factors given patient ’ electronic health record generate composite risk score “ aam system synthesizes analyzes vital statistics lab results variables generate hourly deterioration risk scores adult hospital patients medical-surgical transitional care units ” stated dick daniels executive vice president cio kaiser permanente cio account “ remote hospital teams evaluate risk scores every hour notify rapid response teams hospital potential deterioration detected rapid response team conducts bedside evaluation patient calibrates course treatment hospitalist. ” advice practitioners daniels recommended focus tool fit workflow health care teams “ took us five years perform initial mapping electronic medical record backend develop predictive models ” daniels stated “ took us another two three years transition models live web services application could used operationally. ” example food industry pepsico frito-lay plant fayetteville tenn. using predictive maintenance successfully year-to-date equipment downtime 0.75 unplanned downtime 2.88 according carlos calloway site ’ reliability engineering manager account plantservices examples monitoring include vibration readings confirmed ultrasound helped prevent pc combustion blower motor failing shutting whole potato chip department infrared analysis main pole plant ’ ges automated warehouse detected hot fuse holder helped avoid shutdown entire warehouse increased acid levels detected oil samples baked extruder gearbox indicating oil degradation enabled prevention shutdown cheetos puffs production frito-lay plant produces 150 million pounds product per year including lays ruffles cheetos doritos fritos tostitos types monitoring include vibration analysis used mechanical applications processed help third-party company sends alerts plant investigation resolution another service partner performs quarterly vibration monitoring selected equipment motor control center rooms electrical panels monitored quarterly infrared analysis also used electrical equipment rotating equipment heat exchangers addition plant done ultrasonic monitoring 15 years “ kind like pride joy site predictive standpoint ” stated calloway plan number products place ue systems elmsford ny supplier ultrasonic instruments hardware software training predictive maintenance louisiana alumina plant automating bearing maintenance bearings wear time varying conditions weather temperature case automobiles leading candidate iot monitoring predictive maintenance ai noranda alumina plant gramercy la finding big payoff investment system improve lubrication bearings production equipment system resulted 60 decline bearing changes second year using new lubrication system translating 900,000 savings bearings need replaced avoided downtime “ four hours downtime 1 million dollars ’ worth lost production ” stated russell goodwin reliability engineer millwright instructor noranda alumina plantservices account based presentations leading reliability 2021 event noranda alumina plant alumina plant operating us “ shut ’ need import ” stated goodwin plant experiences pervasive dust dirt caustic substances complicate efforts improved reliability maintenance practices noranda alumina tracks motors gearboxes 1,500 rpm higher vibration readings 1,500 ultrasound ultrasonic monitoring sound ranges beyond human hearing introduced plant goodwin joined company 2019. time grease monitoring room improvement “ grease visibly coming seal mechanical supervisor count round complete ” stated goodwin introducing automation greasing system improved dramatically stated system also able detect bearings belt whose bearings wearing quickly due contamination “ tool-enabled tracking helped prove ’ improper greasing rather bearing made improperly ” stated goodwin read source articles information iot analytics cio plantservices novelty game go provides bright insights ai autonomous vehicles lance eliot ai trends insider already expect humans exhibit flashes brilliance might happen time act welcomed altogether disturbing occurs artificial intelligence ai seems display act novelty instance bound get attention questions arise right away ai come apparent out-of-the-blue insight novel indication mistake fit within parameters ai expected produce also immediate consideration whether ai somehow slipping toward precipice becoming sentient please aware ai system existence anywhere close reaching sentience despite claims falsehoods tossed around media today ’ ai seems something appears novel act leap conclusion sign human insight within technology emergence human ingenuity among ai ’ anthropomorphic bridge far reality ai “ insightful ” novelties based various concrete computational algorithms tangible data-based pattern matching today ’ column ’ taking close look example ai-powered novel act illustrated via game go relate facets advent ai-based true self-driving cars means understanding ai-versus-human related ramifications realize capacity spot suggest novelty done methodically ai system contrast one say sure humans devise novel thoughts intuitions perhaps bound internal mechanistic-like facets maybe something else going someday hopefully crack open secret inner workings mind finally know think suppose might undercut mystery magical aura oftentimes goes along us moments outside-the-box visions though ’ trade enigma know cups-and-balls trickery truly functions going behind curtain speaking novelty famous game match involving playing go provide useful illumination overall topic go popular board game complexity category chess arguments made tougher chess go ’ going get mired morass sake civil discussion key point go highly complex requires intense mental concentration especially tournament level generally go consists trying capture territory standard go board consisting 19 19 grid intersecting lines never tried playing go closest similar kind game might connect-the-dots played childhood involves grabbing territory though go magnitudes involved need know anything particular go get gist discussed next regarding act human novelty act ai novelty famous go competition took place four years ago pitted one world ’ top professional go players lee sedol ai program crafted play go coined alphago riveting documentary contest plenty write-ups online videos detail covered match including post-game analysis put back time 2016 relive happened ai developers anticipate ai time would proficient enough beat top go player sure ai already able best top chess players thus offered glimmer expectation go would eventually equally undertaken ’ go programs able compete pinnacle levels human go players expected would probably around year 2020 capabilities ai would sufficient compete world-class go tournaments deepmind created alphago using deep learning machine learning small-sized tech company named deepmind technologies devised alphago ai playing system firm later acquired google using techniques machine learning deep learning alphago program revamped adjusted right actual tournament typical kind last-ditch developer contortions many us done trying get last bit added edge something demonstrated monumental competition garnered global interest human players go doubtful alphago program would win many ai techies doubtful alphago would win even alphago developers unsure well program would including stay-awake-at-night fears alphago program would hit bug go kind delusional mode make outright mistakes play foolishly million dollars prize money put pot competition would five go games played one per day along associated rules taking breaks etc predicted sedol would handily win five games without cracking sweat ai pundits clinging hope alphago would win least one five games otherwise present respectable level go player throughout contest first match alphago pretty much worldwide shocker sedol taken aback lots go players surprised computer program could compete beat someone sedol ’ level play everyone began give street cred alphago program efforts ai developers tension grew next match second game anticipated sedol might significantly change approach contest perhaps overconfident coming competition harshly asserted loss first game would awaken importance putting concentration tournament possibly played though competing lesser capable player thus pulling stops try win match happened second game turns alphago prevailed also something seemingly remarkable avidly play go 37th move match alphago program opted make placement onto go board spot nobody especially anticipated surprise move coming partway match otherwise relatively conventional nature moves made sedol alphago time real-time rampant speculation move utter gaffe part alphago program instead became famous novel move known “ move 37 ” heralded go used colloquially overall suggest instance ai something novel unexpected manner third match alphago successfully beaten sedol 3-out-of-5 winner competition continued though play fourth fifth game fourth game things tight usual match play going head-to-head well head versus ai put shoes sedol one sense ’ go player somehow representing humanity unfair misguided viewpoint pervasive anyway pressure win least one game even one game would something hang hat bolster faith mankind nonsensical way look seventy-eighth move fourth game sedol made so-called “ wedge ” play conventional surprised onlookers next move alphago rotten diminished likelihood win ai system additional play ultimately alphago tossed towel resigned match thus sedol finally win ai belt ended-up losing fifth game alphago four games sedol one move also became famous generally known “ move 78 ” lore go playing something else worthwhile know involves overarching strategy alphago crafted utilize play game let ’ say connect-the-dots aim grab many squares moment play belief inevitably win accumulation tactically-oriented successes human players go often apt play way said chess players nearly kind game playing altogether another approach involves playing win even thinnest margins long win case might motivated tactical move gain near-term territory score immediate points willing instead play larger scope game per se proverbial mantra shortsighted might win battles could eventually lose war therefore might better strategy keep eye prize winning war albeit means battles skirmishes lost along way ai developers devised alphago kind macro-perspective underlying ai system functioned humans especially hard time choosing moment make move might look bad ill-advised giving territory finding unable grit teeth taking lump two play embarrassment instant difficult offset betting going ultimately okay prevail end ai system semblance kind sentiment involved calculated odds probabilities ’ covered legendary go match let ’ consider lessons learned novelty “ move 38 ” made ai system magical interesting move sure ai developers later indicated move one ai calculated would rarely undertaken human player interpreted two ways least one interpretation human player would make move humans right know would lousy move another interpretation humans would make move due belief move unwise could result humans insufficiently assessing ultimate value move long-run getting caught shorter time frame semblance play instance turned good move—maybe brilliant move—and turned course game advantage ai thus looked like brilliance fact calculated move humans would imagined valuable jostled humans rethink think matters useful recap lessons showcasing human self-limited insight ai something seemingly novel might viewed novel simply humans already predetermined customary anything beyond blunted assumption unworthy mistaken could say mentally trapped drawing lines considered inside versus outside box humans exploiting ai added insight humans gainfully assess ai-powered novelty potentially re-calibrate human thinking given topic enlarging understanding via leveraging something ai via vast calculative capacity might detect spot yet ascertained thus besides admiring novelty ought seek improve mental prowess whatever source shines brightly including ai system ai novelty dual-edged sword need mindful ai systems possibility acting novel way could good could bad go game worked well circumstances ai exploiting novelty route might go tracks let ’ see made tangible via exploring advent ai-based true self-driving cars framework ai autonomous cars see link https //aitrends.com/ai-insider/framework-ai-self-driving-driverless-cars-big-picture/ moonshot effort see explanation https //aitrends.com/ai-insider/self-driving-car-mother-ai-projects-moonshot/ levels type richter scale see discussion https //aitrends.com/ai-insider/richter-scale-levels-self-driving-cars/ argument bifurcating levels see explanation https //aitrends.com/ai-insider/reframing-ai-levels-for-self-driving-cars-bifurcation-of-autonomy/ understanding levels self-driving cars clarification true self-driving cars ones ai drives car entirely ’ human assistance driving task driverless vehicles considered level 4 level 5 car requires human driver co-share driving effort usually considered level 2 level 3. cars co-share driving task described semi-autonomous typically contain variety automated add-on ’ referred adas advanced driver-assistance systems yet true self-driving car level 5 ’ yet even know possible achieve long take get meanwhile level 4 efforts gradually trying get traction undergoing narrow selective public roadway trials though controversy whether testing allowed per se life-or-death guinea pigs experiment taking place highways byways contend remote piloting operating self-driving cars generally eschewed see explanation https //aitrends.com/ai-insider/remote-piloting-is-a-self-driving-car-crutch/ wary fake news self-driving cars see tips https //aitrends.com/ai-insider/ai-fake-news-about-self-driving-cars/ ethical implications ai driving systems significant see indication http //aitrends.com/selfdrivingcars/ethically-ambiguous-self-driving-cars/ aware pitfalls normalization deviance comes self-driving cars ’ call arms https //aitrends.com/ai-insider/normalization-of-deviance-endangers-ai-self-driving-cars/ self-driving cars acts novelty level 4 level 5 true self-driving vehicles ’ human driver involved driving task occupants passengers ai driving could say ai playing game driving game requiring tactical decision-making strategic planning akin playing go chess though case involving life-or-death matters driving multi-ton car public roadways base assumption ai driving system going always take tried-and-true approach driving decisions assumption somewhat shaped around notion ai type robot automata bereft human biases human foibles reality reason make kind assumption yes generally rule aspect ai going display emotion human ilk also know ai drunk dui driving efforts nonetheless ai trained using machine learning ml deep learning dl pick subtleties human behavioral patterns data human driving likewise utilize mimic choosing driving actions example see column postings involving analysis potential racial biases ai possibility gender biases turning back topic novelty let ’ ponder specific use case years ago driving open highway going prevailing speed around 65 miles per hour something nearly unimaginable occurred car coming toward opposing lane likely traveling around 60 70 miles per hour suddenly unexpectedly veered lane one moments anticipate appear reason driver headed toward lane traffic coming imminent bone-chillingly terrifying head-on collision debris lane might clue perhaps driver simply trying swing around obstruction debris slower moving car driver might wanted fast end-around get past nope absolutely discernible basis radical life-threatening maneuver would come hurry clock ticking handful split seconds make life-or-death driving decision could stay lane hope driver realizes error ways opting veer back lane last moment could proactively go opposing lane giving driver clear path lane could chancy game chicken whereby driver chooses go back lane plus traffic behind driver going opposing lane quite dicey okay stay lane veer away opposing lane dare say people would torn two options neither one palatable suppose ai self-driving car faced circumstance would ai odds even ai fed thousands upon thousands miles driving via database human driving undergoing ml/dl training might instances head-to-head nature thus prior pattern utilize making onerous decision anyway ’ twist imagine ai calculated probabilities involving way go computational manner came conclusion self-driving car go ditch right roadway intended avoid entirely collision car ai estimated head-on collision would near-certain death occupants ai estimated going ditch high speed would indisputably wreck car cause great bodily injury occupants odds assured death let ’ say calculated lower head-on option possibilities variant infamous trolley problem covered columns ’ betting would concede humans would relatively unwilling aim purposely ditch know sure going wreck potential death instead willing reluctantly take hoped-for chance either veering lane staying course wishing best sense ai might seem made novel choice one ’ assume humans would given explicit thought toward returning earlier recap points ai novelty could suggest example ai exceeded human self-imposed limitation ai considered otherwise “ unthinkable ” options perhaps learn broaden view options otherwise ’ seem apparent recap element ai novelty dual-edged sword ai react driving ditch inside self-driving car got badly injured would later believe ai acted novel manner acted mistakenly adversely might say lived ask question apparently ai made right choice counter-argument ai gone one choices perhaps would sailed right past car gotten single scratch details odds see indication link https //www.aitrends.com/ai-insider/amalgamating-of-operational-design-domains-odds-for-ai-self-driving-cars/ topic off-road self-driving cars ’ details elicitation https //www.aitrends.com/ai-insider/off-roading-as-a-challenging-use-case-for-ai-autonomous-cars/ ’ urged must chief safety officer self-driving car makers ’ scoop https //www.aitrends.com/ai-insider/chief-safety-officers-needed-in-ai-the-case-of-ai-self-driving-cars/ expect lawsuits going gradually become significant part self-driving car industry see explanatory details http //aitrends.com/selfdrivingcars/self-driving-car-lawsuits-bonanza-ahead/ conclusion wondering actually happen lucky stars looking day survived nothing close call decided remain lane though tempting veer opposing lane miracle driver suddenly went back opposing lane tell story heart still gets pumping begin sweat overall ai appears engage novel approaches problems advantageous circumstances playing board game right wrong wrong especially put human lives stake ai-based true self-driving cars lives stake ’ need proceed mindfully eyes wide open want ai driving systems operate including calculating odds deriving choices wheel vehicle copyright 2021 dr. lance eliot http //ai-selfdriving-cars.libsyn.com/website getting government ai engineers tune ai ethics seen challenge john p. desmond ai trends editor engineers tend see things unambiguous terms may call black white terms choice right wrong good bad consideration ethics ai highly nuanced vast gray areas making challenging ai software engineers apply work takeaway session future standards ethical ai ai world government conference held in-person virtually alexandria va. week overall impression conference discussion ai ethics happening virtually every quarter ai vast enterprise federal government consistency points made across different independent efforts stood beth-ann schuelke-leech associate professor engineering management university windsor “ engineers often think ethics fuzzy thing one really explained ” stated beth-anne schuelke-leech associate professor engineering management entrepreneurship university windsor ontario canada speaking future ethical ai session “ difficult engineers looking solid constraints told ethical becomes really complicated ’ know really means. ” schuelke-leech started career engineer decided pursue phd public policy background enables see things engineer social scientist “ got phd social science pulled back engineering world involved ai projects based mechanical engineering faculty ” said engineering project goal describes purpose set needed features functions set constraints budget timeline “ standards regulations become part constraints ” said “ know comply tell ’ good thing may may adopt that. ” schuelke-leech also serves chair ieee society ’ committee social implications technology standards commented “ voluntary compliance standards ieee essential people industry getting together say think industry. ” standards around interoperability force law engineers comply systems work standards described good practices required followed “ whether helps achieve goal hinders getting objective engineer looks ” said pursuit ai ethics described “ messy difficult ” sara jordan senior counsel future privacy forum sara jordan senior counsel future privacy forum session schuelke-leech works ethical challenges ai machine learning active member ieee global initiative ethics autonomous intelligent systems “ ethics messy difficult context-laden proliferation theories frameworks constructs ” said adding “ practice ethical ai require repeatable rigorous thinking context. ” schuelke-leech offered “ ethics end outcome process followed ’ also looking someone tell need job tell ethical rules ’ supposed follow take away ambiguity. ” “ engineers shut get funny words ’ understand like ‘ ontological ’ ’ taking math science since 13-years-old ” said found difficult get engineers involved attempts draft standards ethical ai “ engineers missing table ” said “ debates whether get 100 ethical conversations engineers have. ” concluded “ managers tell figure need help engineers cross bridge halfway essential social scientists engineers ’ give this. ” leader ’ panel described integration ethics ai development practices topic ethics ai coming curriculum us naval war college newport r.i. established provide advanced study us navy officers educates leaders services ross coffey military professor national security affairs institution participated leader ’ panel ai ethics smart policy ai world government “ ethical literacy students increases time working ethical issues urgent matter take long time ” coffey said panel member carole smith senior research scientist carnegie mellon university studies human-machine interaction involved integrating ethics ai systems development since 2015. cited importance “ demystifying ” ai “ interest understanding kind interactions create human appropriately trusting system working over- under-trusting ” said adding “ general people higher expectations systems. ” example cited tesla autopilot features implement self-driving car capability degree completely “ people assume system much broader set activities designed helping people understand limitations system important everyone needs understand expected outcomes system mitigating circumstances might ” said panel member taka ariga first chief data scientist appointed us government accountability office director gao ’ innovation lab sees gap ai literacy young workforce coming federal government “ data scientist training always include ethics accountable ai laudable construct ’ sure everyone buys need responsibility go beyond technical aspects accountable end user trying serve ” said panel moderator alison brooks phd research vp smart cities communities idc market research firm asked whether principles ethical ai shared across boundaries nations “ limited ability every nation align exact approach align ways allow ai people also responsible ” stated smith cmu panelists credited european commission front issues ethics especially enforcement realm ross naval war colleges acknowledged importance finding common ground around ai ethics “ military perspective interoperability needs go whole new level need find common ground partners allies allow ai allow ai do. ” unfortunately “ ’ know discussion happening ” said discussion ai ethics could perhaps pursued part certain existing treaties smith suggested many ai ethics principles frameworks road maps offered many federal agencies challenging follow made consistent take said “ hopeful next year two see coalescing. ” information access recorded sessions go ai world government digital natives seen advantages part government ai engineering teams john p. desmond ai trends editor ai accessible young people workforce grew ‘ digital natives ’ alexa self-driving cars part landscape giving expectations grounded experience possible idea set foundation panel discussion ai world government mindset needs skill set myths ai engineering teams held week virtually in-person alexandria va. dorothy aronson cio chief data officer national science foundation “ people feel ai within grasp technology available technology ahead cultural maturity ” said panel member dorothy aronson cio chief data officer national science foundation “ ’ like giving sharp object child might access big data might right thing ” work cases things accelerating raising expectations panel member vivek rao lecturer researcher university california berkeley working phd paper natural language processing might master ’ thesis “ assign homework assignment two-day turnaround enormous amount compute power available even two years ago ” said students described “ digital natives ” high expectations ai makes possible rachel dzombak digital transformation lead software engineering institute carnegie mellon university panel moderator rachel dzombak digital transformation lead software engineering institute carnegie mellon university asked panelists unique working ai government aronson said government get far ahead technology users know interact “ ’ building iphones ” said “ experimentation going always looking ahead anticipating future make cost-effective decisions government right seeing convergence emerging generation close-to-retiring generation also serve. ” early career aronson want work government “ thought meant either armed services peace corps ” said “ learned motivates federal employees service larger problem-solving institutions trying solve really big problems equity diversity getting food people keeping people safe people work government dedicated missions. ” referred two children 20s like idea service “ tiny chunks ” meaning “ ’ look government place freedom whatever want see lockdown situation ’ really not. ” berkeley students learn role government disaster response rao berkeley said students seeing wildfires california asking working challenge something tells almost always local state federal government entities “ students generally surprised find out. ” one example developed course innovation disaster response collaboration cmu department defense army futures lab coast guard search rescue “ eye-opening students ” said outset two 35 students expressed interest federal government career end course 10 35 students expressing interest one hired naval surface warfare center outside corona calif. software engineer rao said aronson described process bringing new federal employees “ heavy lift ” suggesting “ could prepare advance would move lot faster. ” bryan lane director data ai general services administration asked dzombak skill sets mindsets seen essential ai engineering teams panel member bryan lane director data ai general services administration announced session taking new role fdic said resiliency necessary quality lane technology executive within gsa modernization centers excellence coe 15 years experience leading advanced analytics technology initiatives led gsa partnership dod joint artificial intelligence center jaic ed note known “ jake. ” lane also founder data xd also experience industry managing acquisition portfolios “ important thing resilient teams going ai journey need ready unexpected mission persists ” said “ aligned importance mission team held together. ” good sign team members acknowledge “ never done ” regarding mindset said team members coming saying “ ’ never done before. ” sees good sign offers opportunity talk risk alternative solutions “ team psychological safety say ’ know something ” lane sees positive “ focus always done delivered rarely focus done want grow ” said aronson found challenging get ai projects ground “ ’ hard tell management use case problem solve want go 50-50 chance get done ’ know much ’ going cost ” said “ comes articulating rationale convincing others ’ right thing move forward. ” rao said talks students experimentation experimental mindset “ ai tools easily accessible mask challenges encounter apply vision api example context challenges business government agency things may smooth ” said moderator dzombak asked panelists build teams arson said “ need mix people. ” tried “ communities practice ” around solving specific problems people come go “ bring people together around problem tool ” said lane seconded “ really stopped focusing tools general ” said ran experiments jaic accounting finance areas “ found ’ really tools ’ getting right people together understand problems looking tools available ” said lane said sets “ cross-functional teams ” “ little formal community interest. ” found effective working together problem maybe 45 days also likes working customers needed services inside organization seen customers learn data management ai result “ pick one two along way become advocates accelerating ai throughout organization ” lane said lane sees taking five years work proven methods thinking working best practices developing ai systems serve government mentioned opportunity project top us census bureau begun 2016 work challenges ocean plastic pollution covid-19 economic recovery disaster response top engaged 135 public-facing projects time 1,300 alumni including developers designers community leaders data policy experts students government agencies “ ’ based way thinking organize work ” lane said “ scale model delivery five years enough proof concept know works not. ” learn ai world government software engineering institute data xd opportunity project accountability practices pursued ai engineers federal government john p. desmond ai trends editor two experiences ai developers within federal government pursuing ai accountability practices outlined ai world government event held virtually in-person week alexandria va. taka ariga chief data scientist director us government accountability office taka ariga chief data scientist director us government accountability office described ai accountability framework uses within agency plans make available others bryce goodman chief strategist ai machine learning defense innovation unit diu unit department defense founded help us military make faster use emerging commercial technologies described work unit apply principles ai development terminology engineer apply ariga first chief data scientist appointed us government accountability office director gao ’ innovation lab discussed ai accountability framework helped develop convening forum experts government industry nonprofits well federal inspector general officials ai experts “ adopting auditor ’ perspective ai accountability framework ” ariga said “ gao business verification. ” effort produce formal framework began september 2020 included 60 women 40 underrepresented minorities discuss two days effort spurred desire ground ai accountability framework reality engineer ’ day-to-day work resulting framework first published june ariga described “ version 1.0. ” seeking bring “ high-altitude posture ” earth “ found ai accountability framework high-altitude posture ” ariga said “ laudable ideals aspirations mean day-to-day ai practitioner gap see ai proliferating across government. ” “ landed lifecycle approach ” steps stages design development deployment continuous monitoring development effort stands four “ pillars ” governance data monitoring performance governance reviews organization put place oversee ai efforts “ chief ai officer might place mean person make changes multidisciplinary ” system level within pillar team review individual ai models see “ purposely deliberated. ” data pillar team examine training data evaluated representative functioning intended performance pillar team consider “ societal impact ” ai system deployment including whether risks violation civil rights act “ auditors long-standing track record evaluating equity grounded evaluation ai proven system ” ariga said emphasizing importance continuous monitoring said “ ai technology deploy forget. ” said “ preparing continually monitor model drift fragility algorithms scaling ai appropriately. ” evaluations determine whether ai system continues meet need “ whether sunset appropriate ” ariga said part discussion nist overall government ai accountability framework “ ’ want ecosystem confusion ” ariga said “ want whole-government approach feel useful first step pushing high-level ideas altitude meaningful practitioners ai. ” diu assesses whether proposed projects meet ethical ai guidelines bryce goodman chief strategist ai machine learning defense innovation unit diu goodman involved similar effort develop guidelines developers ai projects within government projects goodman involved implementation ai humanitarian assistance disaster response predictive maintenance counter-disinformation predictive health heads responsible ai working group faculty member singularity university wide range consulting clients inside outside government holds phd ai philosophy university oxford dod february 2020 adopted five areas ethical principles ai 15 months consulting ai experts commercial industry government academia american public areas responsible equitable traceable reliable governable “ well-conceived ’ obvious engineer translate specific project requirement ” good said presentation responsible ai guidelines ai world government event “ ’ gap trying fill. ” diu even considers project run ethical principles see passes muster projects “ needs option say technology problem compatible ai ” said project stakeholders including commercial vendors within government need able test validate go beyond minimum legal requirements meet principles “ law moving fast ai principles important ” said also collaboration going across government ensure values preserved maintained “ intention guidelines try achieve perfection avoid catastrophic consequences ” goodman said “ difficult get group agree best outcome ’ easier get group agree worst-case outcome is. ” diu guidelines along case studies supplemental materials published diu website “ soon ” goodman said help others leverage experience questions diu asks development starts first step guidelines define task “ ’ single important question ” said “ advantage use ai. ” next benchmark needs set front know project delivered next evaluates ownership candidate data “ data critical ai system place lot problems exist. ” goodman said “ need certain contract owns data ambiguous lead problems. ” next goodman ’ team wants sample data evaluate need know information collected “ consent given one purpose use another purpose without re-obtaining consent ” said next team asks responsible stakeholders identified pilots could affected component fails next responsible mission-holders must identified “ need single individual ” goodman said “ often tradeoff performance algorithm explainability might decide two kinds decisions ethical component operational component need someone accountable decisions consistent chain command dod. ” finally diu team requires process rolling back things go wrong “ need cautious abandoning previous system ” said questions answered satisfactory way team moves development phase lessons learned goodman said “ metrics key simply measuring accuracy might adequate need able measure success. ” also fit technology task “ high risk applications require low-risk technology potential harm significant need high confidence technology ” said another lesson learned set expectations commercial vendors “ need vendors transparent ” said ” someone says proprietary algorithm tell us wary view relationship collaboration ’ way ensure ai developed responsibly. ” lastly “ ai magic solve everything used necessary prove provide advantage. ” learn ai world government government accountability office ai accountability framework defense innovation unit site startup assemblyai represents new generation speech recognition ai trends staff advances ai behind speech recognition driving growth market attracting venture capital funding startups posing challenges established players growing acceptance use speech recognition devices driving market according estimate meticulous research expected reach 26.8 billion globally 2025 according recent account analytics insight better speed accuracy among benefits evolving technology dylan fox ceo founder assemblyai one company throes new growth assemblyai san francisco offering api speech recognition capable transcribing videos podcasts phone calls remote meetings company founded ceo dylan fox 2017 received backing combinator startup accelerator well nvidia fox unusual background high tech entrepreneur graduate george washington university degree business administration business economics public policy got job software engineer machine learning emerging product lab cisco san francisco working deep neural networks machine learning got idea assemblyai attracted capital combinator enabled hire data scientists data engineers get technology ground asked interview ai trends made transition undergrad business administration economics high-tech entrepreneur fox said “ taught program led path machine learning looking harder software challenge led natural language processing took cisco. ” working siri enterprise apple time speed work cisco looking acquire speech recognition software fox catbird ’ seat search “ looked nuance ” example acknowledged market leader owner speech recognition software competitors acquisition nuance microsoft 19.6 billion expected finalized year-end young budding entrepreneur impressed “ crazy bad options accuracy developer point view ” stated impressed twilio san francisco-based company founded 2008 year released twilio voice api make receive phone calls hosted cloud company since raised 103 million venture capital “ setting new standards good api developers ” fox said fox ’ idea use ai machine learning achieve “ super accurate results make easy developers incorporate api products one customer callrail offering call tracking marketing analytics software plans incorporate assembyai ’ api gain insight people calling customers include nbc wall street journal using product transcribe content interviews provide closed captioning “ ’ working building close human speech recognition quality possible ’ lot work ” fox said expects reach plateau 2022. targets companies incorporating speech recognition products makes easy buy customers pay usage basis every second audio transcribed assemblyai charges fraction penny clients get billed monthly customer uses 10 hours month costs nine dollars customer uses million hours month costs 900,000 voice recognition hot market “ many new startups launched ” fox said providing opportunity “ many interesting new businesses built voice data. ” assemblyai ’ product detect sensitive topics hate speech profanity customers save human content moderation asked describe differentiates technology fox said “ experienced team deep learning researchers ” experience companies including bmw apple facebook “ build large accurate deep learning models recognition results far accurate traditional machine learning approach build really large models using advanced neural network technologies. ” compared approach openai uses develop gpt-3 large language model addition build ai features top transcriptions provide summaries audio video content searched indexed “ goes beyond transcription ” fox said company currently 25 employees expects double four months business good “ explosion audio video data online customers want able take advantage see lot demand ” fox said learn assemblyai pursuit autonomous cars may pose risk ai tapping forbidden knowledge lance eliot ai trends insider things must know age-old question assert potential knowledge ought known words ideas concepts mental formulations become aware knowledge could downfall discovery invention new innovation way thinking could unduly dangerous would best go avoid ever landing knowledge forbidden knowledge typical basis wanting forbid discovery emergence forbidden knowledge adverse consequences overwhelming end result devastating undercutting bad side outweighs good could derived knowledge conceivable might knowledge bad good possibilities thus rather trying balance weigh good versus bad knowledge counterbalancing effects plain bad usually faced matter knowledge good bad might utilized employed leads dogged debate whether bad bad outweighs good top unrealized bad unrealized good could differentiated realized bad realized good essence knowledge might said either good bad though purely conceptual put real-world conditions attest become realized familiar reference forbidden knowledge likely evoked via garden eden essence forbidden fruit contemporary down-to-earth example often discussed forbidden knowledge consists atomic bomb suggest knowledge devised invented ultimately produce nuclear bomb provides quite visible overt exemplar problems associated knowledge knowledge able attain atomic bomb never achieved presumably would device debates topic feasible take resolute position favoring attainment atomic bomb equally counterbalancing contentions sternly disfavoring attainment one perplexing problem forbidden knowledge encompasses knowing beforehand kind knowledge might end forbidden category bit catch-22 circular type puzzle might discover knowledge ascertain ought forbidden cat kind bag due knowledge already uncovered rendered oopsie advance decided go therefore avoided falling forbidden knowledge zone related twist suppose could beforehand declare type knowledge averted predetermined forbidden people might accidentally discover knowledge happenstance ’ potentially opened pandora ’ box meanwhile might others regardless instructed derive stated forbidden knowledge anyway takes us frequently used retort forbidden knowledge namely ’ seek forbidden knowledge chance someone else ’ left dust got first preemptive viewpoint claim better go ahead forage forbidden knowledge get caught behind eight-ball someone else beats punch round round go main thing would agree knowledge power alluded power could devastating destroy others possibly even leading self-destruction wielder knowledge yet also potential knowledge advantageous save humanity ills maybe ought say knowledge powerful despite perhaps obvious proclamation might also add knowledge decay gradually become outdated less potent furthermore since immersing herein cauldron love-it hate-it knowledge conundrum knowledge known yet undervalued perhaps becoming valuable later time different light case made humankind seemingly irresistible allure toward knowledge philosophers suggest unlikely able bottle stop quest knowledge ’ manner humanity implies must find ways control contain knowledge give belief altogether avoid landing forbidden knowledge relatively new venue prompting lot anxious hand wringing pertaining forbidden knowledge namely advent artificial intelligence ai ’ rub suppose able craft ai systems make use knowledge humans think two major potential gotchas first ai systems might end good things also might end bad things bad outweighs good maybe shooting foot allowing ai put use secondly perhaps could averted entirely deciding forbidden knowledge humans think ought discover reveal mental mechanisms classic stepwise logic step axiomatically leads step b. ’ need worry ai systems step b never allow achievement step figuring humans think imparting computers since attainment ai would presumably arise case inarguably growing concern ai plenty efforts underway promulgate semblance ai ethics meaning developers indeed stakeholders conceiving building putting use ai system needs consider ethical aspects efforts ai systems unveiled placed use replete sorts notable concerns including incorporating unsavory biases problems told one bold somewhat stark argument pursuit ai underpinned stoked discovery exploitation forbidden knowledge aware many would scoff allegation deeply immersed field ai would laugh anything entirety ai date constitutes potential forbidden knowledge technology technological elements relatively ho-hum would argue would hard-pressed pinpoint ai-related knowledge already known comes anywhere near ballpark forbidden knowledge concur posture reply might future knowledge yet attained upcoming forbidden kind heading pell-mell path thus would concede ’ arrived forbidden knowledge juncture insidious distractor due aspect masks belies qualms entailing possibility lays wait next turn one area ai actively used create autonomous vehicles avs gradually seeing emergence self-driving cars expect self-driving trucks self-driving motorcycles self-driving drones self-driving planes self-driving ships self-driving submersibles etc today ’ conventional cars eventually going give way advent ai-based true self-driving cars self-driving cars driven via ai driving system ’ need human driver wheel provision human drive vehicle ’ intriguing question arisen might crafting ai-based true self-driving cars take us realm discovering forbidden knowledge done jumping details ’ like clarify meant referring true self-driving cars framework ai autonomous cars see link https //aitrends.com/ai-insider/framework-ai-self-driving-driverless-cars-big-picture/ moonshot effort see explanation https //aitrends.com/ai-insider/self-driving-car-mother-ai-projects-moonshot/ levels type richter scale see discussion https //aitrends.com/ai-insider/richter-scale-levels-self-driving-cars/ argument bifurcating levels see explanation https //aitrends.com/ai-insider/reframing-ai-levels-for-self-driving-cars-bifurcation-of-autonomy/ understanding levels self-driving cars clarification true self-driving cars ones ai drives car entirely ’ human assistance driving task driverless vehicles considered level 4 level 5 car requires human driver co-share driving effort usually considered level 2 level 3. cars co-share driving task described semi-autonomous typically contain variety automated add-on ’ referred adas advanced driver-assistance systems yet true self-driving car level 5 ’ yet even know possible achieve long take get meanwhile level 4 efforts gradually trying get traction undergoing narrow selective public roadway trials though controversy whether testing allowed per se life-or-death guinea pigs experiment taking place highways byways contend since semi-autonomous cars require human driver adoption types cars ’ markedly different driving conventional vehicles ’ much new per se cover topic though ’ see moment points next made generally applicable semi-autonomous cars important public needs forewarned disturbing aspect ’ arising lately namely despite human drivers keep posting videos falling asleep wheel level 2 level 3 car need avoid misled believing driver take away attention driving task driving semi-autonomous car responsible party driving actions vehicle regardless much automation might tossed level 2 level 3. remote piloting operating self-driving cars generally eschewed see explanation https //aitrends.com/ai-insider/remote-piloting-is-a-self-driving-car-crutch/ wary fake news self-driving cars see tips https //aitrends.com/ai-insider/ai-fake-news-about-self-driving-cars/ ethical implications ai driving systems significant see indication http //aitrends.com/selfdrivingcars/ethically-ambiguous-self-driving-cars/ aware pitfalls normalization deviance comes self-driving cars ’ call arms https //aitrends.com/ai-insider/normalization-of-deviance-endangers-ai-self-driving-cars/ self-driving cars forbidden knowledge level 4 level 5 true self-driving vehicles ’ human driver involved driving task occupants passengers ai driving one aspect immediately discuss entails fact ai involved today ’ ai driving systems sentient words ai altogether collective computer-based programming algorithms assuredly able reason manner humans added emphasis ai sentient want underscore discussing role ai driving system ascribing human qualities ai please aware ongoing dangerous tendency days anthropomorphize ai essence people assigning human-like sentience today ’ ai despite undeniable inarguable fact ai exists yet clarification envision ai driving system ’ natively somehow “ know ” facets driving driving entails need programmed part hardware software self-driving car let ’ dive myriad aspects come play topic crux whether forbidden knowledge lurking within existing ongoing efforts achieve ai-based true self-driving cars ’ begin considering status existent efforts shift speculation future efforts per earlier discussion whether forbidden knowledge already perchance revealed discovered via efforts toward today ’ ai systems told odds seem stacked notion time likewise could said pursuit self-driving cars essentially ’ seem forbidden knowledge per se discovered revealed self-driving cars development journey far least respect conventional wisdom forbidden knowledge might entail one could try argue premature reach conclusion might later realize forbidden knowledge indeed uncovered invented ’ realize rabbit hole ’ go though welcome keep presumption hand desired covers present ergo turn attention future generally efforts underway today primarily aimed achieving level 4 hope someday go beyond level 4 attain level 5. get robust level 4 would likely say continue existing approaches everyone would agree assumption believe get stymied within level 4. furthermore inability produce robust level 4 ostensibly preclude us able attain level 5. contingent suggests need start set aside existing ai approaches otherwise taking us dead-end blind alley entirely new way devising ai autonomous vehicles needed would vehemently argue also contingent asserts level 4 type dead-end brief proponents would say achieve robust level 4 though little good towards attaining level 5. view similar preceding remark need come radically new understandings ai nature cognitive acumen order get self-driving cars level 5 realm aha within scope dramatically revisit revamp ai advance significantly pursuit ai forbidden knowledge question reside theory perhaps means attaining level 5 strike upon knowledge yet know bodes falling within realm forbidden knowledge seems farfetched would emphatically ask kind knowledge even talking ’ logic humans able drive cars humans seem need possess forbidden knowledge relates act driving car therefore seems ridiculous face things claim contend means get ai-based true self-driving cars would driven equal basis human drivers drive would require discovery invention whatever might construed forbidden knowledge seems like pretty ironclad logic retort humans common-sense reasoning common-sense reasoning seem know sorts things world around us drive car intrinsically make use common-sense reasoning take granted common-sense reasoning capacity similarly take granted integrally comes fore driving car attempts create ai exhibit equivalent human common-sense reasoning made ostensibly modest would say minimal progress clarify pursuing line inquiry lauded ’ earth-shattering breakthroughs seem reached none seem immediate horizon yes quite fascinating exciting efforts underway measure everyday common-sense reasoning humans comparison night day contest humans win hands doubt ai experimental efforts encompassing common-sense reasoning mere playthings contrast might gleaned line thought headed belief crack open enigma common-sense reasoning little chance achieving level 5 perhaps also hold back level 4 could secret ingredient sorts autonomous vehicles need figure include common-sense reasoning ai-based driving piloting systems buy logic added assertion maybe within confines common-sense reasoning takes place semblance forbidden knowledge surface would certainly assume knew entirely common-sense reasoning works would appear cause alarm concern act employing common-sense reasoning seem necessarily embody forbidden knowledge twist perhaps underlying cognitive means gives rise advent common-sense reasoning forbidden knowledge deep-rooted elements nature human thought form common sense undertake common-sense reasoning possibly type knowledge shown crucial forbidden knowledge formulation details odds see indication link https //www.aitrends.com/ai-insider/amalgamating-of-operational-design-domains-odds-for-ai-self-driving-cars/ topic off-road self-driving cars ’ details elicitation https //www.aitrends.com/ai-insider/off-roading-as-a-challenging-use-case-for-ai-autonomous-cars/ ’ urged must chief safety officer self-driving car makers ’ scoop https //www.aitrends.com/ai-insider/chief-safety-officers-needed-in-ai-the-case-of-ai-self-driving-cars/ expect lawsuits going gradually become significant part self-driving car industry see explanatory details http //aitrends.com/selfdrivingcars/self-driving-car-lawsuits-bonanza-ahead/ conclusion wow ’ quite bit pondering contemplation would say wild thinking maybe consideration would wish gave least credence toward devoted attention angst might find happenstance stumbling forbidden knowledge voracious self-driving cars quests however might emphasize ai-based true self-driving cars potential blessing proffering mobility-for-all leading reducing number car crash-related fatalities sneaking suspicion all-good catch trap could kind forbidden knowledge get brought eye inevitably kick ’ see coming next time munching delicious apple give thought whether self-driving cars might forbidden fruit path taking big bite ’ see takes us copyright 2021 dr. lance eliot http //ai-selfdriving-cars.libsyn.com/website'"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "formatted_feed_content = preprocess(feed_content) # remove stopwords and punctuation\n",
    "formatted_feed_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "4438f558",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(103131, 72256)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(feed_content), len(formatted_feed_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bbfc974",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 2000x2000 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(20,20)) # seeing word count in this article feed\n",
    "plt.imshow(WordCloud().generate(formatted_feed_content)) \n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57e1fbcf",
   "metadata": {},
   "source": [
    "**9. Extracting named entities**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c374ae2",
   "metadata": {},
   "source": [
    "Acronyms: https://spacy.io/api/annotation/#named-entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "36caf1d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<spacy.lang.en.English at 0x115796660>"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "nlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "b098d421",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">By \n",
       "<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    John P. Desmond\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n",
       "</mark>\n",
       ", \n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    AI\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       " Trends Editor <br>The \n",
       "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    AI\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
       "</mark>\n",
       " stack defined by \n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Carnegie Mellon University\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       " is fundamental to the approach being taken by \n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    the US Army\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       " for its \n",
       "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    AI\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
       "</mark>\n",
       " development platform efforts, according to \n",
       "<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Isaac Faber\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n",
       "</mark>\n",
       ", Chief Data Scientist at \n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    the US Army AI Integration Center\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       ", speaking at the \n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    AI World Government\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       " event held in-person and virtually from \n",
       "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Alexandria\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
       "</mark>\n",
       ", \n",
       "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Va.\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
       "</mark>\n",
       ", \n",
       "<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    last week\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">DATE</span>\n",
       "</mark>\n",
       ".  <br>\n",
       "<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Isaac Faber\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n",
       "</mark>\n",
       ", Chief \n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Data Scientist\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       ", \n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    US Army AI Integration Center\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       "<br>“If we want to move the \n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Army\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       " from legacy systems through digital modernization, one of the biggest issues I have found is the difficulty in abstracting away the differences in applications,” he said. “The most important part of digital transformation is the middle layer, the platform that makes it easier to be on the cloud or on a local computer.” The desire is to be able to move your software platform to another platform, with the same ease with which a new smartphone carries over the user’s contacts and histories.  <br>Ethics cuts across all layers of the \n",
       "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    AI\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
       "</mark>\n",
       " application stack, which positions the planning stage at the top, followed by decision support, modeling, machine learning, massive data management and the device layer or platform at the bottom.  <br>“I am advocating that we think of the stack as a core infrastructure and a way for applications to be deployed and not to be siloed in our approach,” he said. “We need to create a development environment for a globally-distributed workforce.”   <br>The \n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Army\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       " has been working on a Common Operating Environment Software (\n",
       "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Coes\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
       "</mark>\n",
       ") platform, \n",
       "<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    first\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORDINAL</span>\n",
       "</mark>\n",
       " announced in \n",
       "<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    2017\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">DATE</span>\n",
       "</mark>\n",
       ", a design for DOD work that is scalable, agile, modular, portable and open. “It is suitable for a broad range of \n",
       "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    AI\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
       "</mark>\n",
       " projects,” \n",
       "<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Faber\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n",
       "</mark>\n",
       " said. For executing the effort, “The devil is in the details,” he said.   <br>The \n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Army\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       " is working with \n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    CMU\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       " and private companies on a prototype platform, including with \n",
       "<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Visimo\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n",
       "</mark>\n",
       " of \n",
       "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Coraopolis\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
       "</mark>\n",
       ", \n",
       "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Pa.\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
       "</mark>\n",
       ", which offers \n",
       "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    AI\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
       "</mark>\n",
       " development services. \n",
       "<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Faber\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n",
       "</mark>\n",
       " said he prefers to collaborate and coordinate with private industry rather than buying products off the shelf. “The problem with that is, you are stuck with the value you are being provided by that \n",
       "<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    one\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">CARDINAL</span>\n",
       "</mark>\n",
       " vendor, which is usually not designed for the challenges of DOD networks,” he said.  <br>Army Trains a Range of Tech Teams in \n",
       "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    AI\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
       "</mark>\n",
       " <br>The \n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Army\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       " engages in \n",
       "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    AI\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
       "</mark>\n",
       " workforce development efforts for several teams, including:  leadership, professionals with graduate degrees; technical staff, which is put through training to get certified; and \n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    AI\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       " users.   <br>Tech teams in the \n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Army\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       " have different areas of focus include: general purpose software development, operational data science, deployment which includes analytics, and a machine learning operations team, such as a large team required to build a computer vision system. “As folks come through the workforce, they need a place to collaborate, build and share,” \n",
       "<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Faber\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n",
       "</mark>\n",
       " said.   <br>Types of projects include diagnostic, which might be combining streams of historical data, predictive and prescriptive, which recommends a course of action based on a prediction. “At the far end is \n",
       "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    AI\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
       "</mark>\n",
       "; you don’t start with that,” said \n",
       "<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Faber\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n",
       "</mark>\n",
       ". The developer has to solve \n",
       "<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    three\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">CARDINAL</span>\n",
       "</mark>\n",
       " problems: data engineering, the \n",
       "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    AI\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
       "</mark>\n",
       " development platform, which he called “the green bubble,” and the deployment platform, which he called “the red bubble.”   <br>“These are mutually exclusive and all interconnected. Those teams of different people need to programmatically coordinate. Usually a good project team will have people from each of those bubble areas,” he said. “If you have not done this yet, do not try to solve the green bubble problem. It makes no sense to pursue \n",
       "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    AI\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
       "</mark>\n",
       " until you have an operational need.”   <br>Asked by a participant which group is the most difficult to reach and train, \n",
       "<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Faber\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n",
       "</mark>\n",
       " said without hesitation, “The hardest to reach are the executives. They need to learn what the value is to be provided by the \n",
       "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    AI\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
       "</mark>\n",
       " ecosystem. The biggest challenge is how to communicate that value,” he said.   <br>Panel Discusses AI Use Cases with the Most Potential  <br>In a panel on \n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Foundations of Emerging AI\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       ", moderator \n",
       "<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Curt Savoie\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n",
       "</mark>\n",
       ", program director, \n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Global Smart Cities Strategies\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       " for IDC, the market research firm, asked what emerging AI use case has the most potential.  <br>\n",
       "<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Jean-Charles Lede\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n",
       "</mark>\n",
       ", autonomy tech advisor for \n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    the US Air Force, Office of Scientific Research\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       ", said,” I would point to decision advantages at the edge, supporting pilots and operators, and decisions at the back, for mission and resource planning.”   <br>\n",
       "<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Krista Kinnard\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n",
       "</mark>\n",
       ", Chief of Emerging Technology for \n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    the Department of Labor\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       "<br>\n",
       "<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Krista Kinnard\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n",
       "</mark>\n",
       ", Chief of Emerging Technology for \n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    the Department of Labor\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       ", said, “Natural language processing is an opportunity to open the doors to \n",
       "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    AI\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
       "</mark>\n",
       " in \n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    the Department of Labor\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       ",” she said. “Ultimately, we are dealing with data on people, programs, and organizations.”    <br>\n",
       "<mark class=\"entity\" style=\"background: #c887fb; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Savoie\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">NORP</span>\n",
       "</mark>\n",
       " asked what are the big risks and dangers the panelists see when implementing \n",
       "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    AI\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
       "</mark>\n",
       ".   <br>\n",
       "<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Anil Chaudhry\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n",
       "</mark>\n",
       ", Director of \n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Federal AI Implementations\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       " for \n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    the General Services Administration\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       " (\n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    GSA\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       "), said in a typical IT organization using traditional software development, the impact of a decision by a developer only goes so far. With \n",
       "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    AI\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
       "</mark>\n",
       ", “You have to consider the impact on a whole class of people, constituents, and stakeholders. With a simple change in algorithms, you could be delaying benefits to \n",
       "<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    millions\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">CARDINAL</span>\n",
       "</mark>\n",
       " of people or making incorrect inferences at scale. That’s the most important risk,” he said.  <br>He said he asks his contract partners to have “humans in the loop and humans on the loop.”   <br>Kinnard seconded this, saying, “We have no intention of removing humans from the loop. It’s really about empowering people to make better decisions.”   <br>She emphasized the importance of monitoring the \n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    AI\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       " models after they are deployed. “Models can drift as the data underlying the changes,” she said. “So you need a level of critical thinking to not only do the task, but to assess whether what the \n",
       "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    AI\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
       "</mark>\n",
       " model is doing is acceptable.”   <br>She added, “We have built out use cases and partnerships across the government to make sure we’re implementing responsible \n",
       "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    AI\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
       "</mark>\n",
       ". We will never replace people with algorithms.”  <br>Lede of \n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    the Air Force\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       " said, “We often have use cases where the data does not exist. We cannot explore \n",
       "<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    50 years\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">DATE</span>\n",
       "</mark>\n",
       " of war data, so we use simulation. The risk is in teaching an algorithm that you have a ‘simulation to real gap’ that is a real risk. You are not sure how the algorithms will map to the real world.”  <br>\n",
       "<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Chaudhry\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n",
       "</mark>\n",
       " emphasized the importance of a testing strategy for \n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    AI\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       " systems. He warned of developers “who get enamored with a tool and forget the purpose of the exercise.” He recommended the development manager design in independent verification and validation strategy. “Your testing, that is where you have to focus your energy as a leader. The leader needs an idea in mind, before committing resources, on how they will justify whether the investment was a success.”   <br>Lede of \n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    the Air Force\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       " talked about the importance of explainability. “I am a technologist. I don’t do laws. The ability for the AI function to explain in a way a human can interact with, is important. The \n",
       "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    AI\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
       "</mark>\n",
       " is a partner that we have a dialogue with, instead of the \n",
       "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    AI\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
       "</mark>\n",
       " coming up with a conclusion that we have no way of verifying,” he said.  <br>Learn more at \n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    AI World Government\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       ". </div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "displacy.render(nlp(blog_articles[0][\"content\"]), style=\"ent\", jupyter=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "371caa0d",
   "metadata": {},
   "source": [
    "**10. Summarizing the articles**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c033351",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize(title, best_sentences, original_sentences):\n",
    "    \"\"\"\n",
    "    Display the article title and highlight the best sentences in the summary.\n",
    "    - title: str, the article title\n",
    "    - best_sentences: list of str, the selected summary sentences\n",
    "    - original_sentences: list of str, all sentences in the article (in order)\n",
    "    \"\"\"\n",
    "    text = \"\"\n",
    "    for sentence in original_sentences:\n",
    "        if sentence in best_sentences:\n",
    "            text += f\"<mark>{sentence}</mark> \"\n",
    "        else:\n",
    "            text += f\"{sentence} \"\n",
    "    html = f\"<h2>{title}</h2><p>{text}</p>\"\n",
    "    display(HTML(html))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "14dd4287",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To summarize each article in the feed using Luhn Algorithm\n",
    "for article in blog_articles:\n",
    "    original_setences, best_sentences, sentences_score = summarize(article[\"content\"], 150, 10, number_of_sentences=5, percantage=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2df1a5a8",
   "metadata": {},
   "source": [
    "**11. Saving the summary**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "d1cf4a9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_summary(title, original_sentences, best_sentences): \n",
    "    HTML_TEMPLATE = \"\"\"\n",
    "    <html>\n",
    "    <head><title>Summary</title></head>\n",
    "    <meta http-equiv=\"Content-Type\" content=\"text/html; charset=utf-8\"/>\n",
    "    <body>\n",
    "    {1}\n",
    "    </body>\n",
    "    </html>\n",
    "    \"\"\"\n",
    "    text = ''\n",
    "    for sentence in original_sentences:\n",
    "        if sentence in best_sentences:\n",
    "            text += str(sentence).replace(sentence, f\"<mark>{sentence}</mark>\")\n",
    "        else:\n",
    "            text += sentence\n",
    "\n",
    "    save_file = open(os.path.join(title + '.html'), 'wb')\n",
    "    html_file = HTML_TEMPLATE.format(title, text)\n",
    "    save_file.write(html_file.encode('utf-8'))\n",
    "    save_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "9a1ab6d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<h2>Pursuit of Autonomous Cars May Pose Risk of AI Tapping Forbidden Knowledge</h2><p><mark>By Lance Eliot, the AI Trends Insider    \n",
       "Are there things that we must not know?</mark> This is an age-old question. <mark>Some assert that there is the potential for knowledge that ought to not be known.</mark> <mark>In other words, there are ideas, concepts, or mental formulations that should we become aware of that knowledge it could be our downfall.</mark> <mark>The discovery or invention of some new innovation or way of thinking could be unduly dangerous.</mark> <mark>It would be best to not go there, as it were, and avoid ever landing on such knowledge: forbidden knowledge.</mark> <mark>The typical basis for wanting to forbid the discovery or emergence of forbidden knowledge is that the adverse consequences are overwhelming.</mark> <mark>The end result is so devastating and undercutting that the bad side outweighs the good that could be derived from the knowledge.</mark> <mark>It is conceivable that there might be knowledge that is so bad that it has no good possibilities at all.</mark> <mark>Thus, rather than trying to balance or weigh the good versus the bad, the knowledge has no counterbalancing effects.</mark> It is just plain bad. <mark>We are usually faced with the matter of knowledge that has both the good and the bad as to how it might be utilized or employed.</mark> This then leads to a dogged debate about whether the bad is so bad that it outweighs the good. <mark>On top of this, there is the unrealized bad and the unrealized good, which could be differentiated from the realized bad and the realized good (in essence, the knowledge might be said to be either good or bad, though this is purely conceptual and not put into real-world conditions to attest or become realized as such).</mark> <mark>The most familiar reference to forbidden knowledge is likely evoked via the Garden of Eden and the essence of forbidden fruit.</mark> A contemporary down-to-earth example often discussed about forbidden knowledge consists of the atomic bomb. <mark>Some suggest that the knowledge devised or invented to ultimately produce a nuclear bomb provides a quite visible and overt exemplar of the problems associated with knowledge.</mark> <mark>Had the knowledge about being able to attain an atomic bomb never been achieved, there presumably would not be any such device.</mark> <mark>In debates about the topic, it is feasible to take a resolute position favoring the attainment of an atomic bomb and there are equally counterbalancing contentions sternly disfavoring this attainment.</mark> <mark>One perplexing problem about forbidden knowledge encompasses knowing beforehand the kind of knowledge that might end up in the forbidden category.</mark> This is a bit of a Catch-22 or circular type of puzzle. <mark>You might discover knowledge and then ascertain it ought to be forbidden, but the cat is kind of out of the bag due to the knowledge having been already uncovered or rendered.</mark> Oopsie, you should have in advance decided to not go there and therefore have avoided falling into the forbidden knowledge zone. On a related twist, suppose that we could beforehand declare what type of knowledge is to be averted because it is predetermined as forbidden. Some people might accidentally discover the knowledge, doing so by happenstance, and now they’ve again potentially opened Pandora’s box. Meanwhile, there might be others that, regardless of being instructed to not derive any such stated forbidden knowledge, do so anyway. <mark>This then takes us to a frequently used retort about forbidden knowledge, namely, if you don’t seek the forbidden knowledge there is a chance that someone else will, and you’ll be left in the dust because they got there first.</mark> In that preemptive viewpoint, the claim is that it is better to go ahead and forage for the forbidden knowledge and not get caught behind the eight-ball when someone else beats you to the punch. Round and round we can go. The main thing that most would agree to is that knowledge is power. The alluded to power could be devastating and destroy others, possibly even leading to the self-destruction of the wielder of the knowledge. <mark>Yet there is also the potential for knowledge to be advantageous and save humanity from other ills.</mark> <mark>Maybe we ought to say that knowledge is powerful.</mark> <mark>Despite that perhaps obvious proclamation, we might also add that knowledge can decay and gradually become outdated or less potent.</mark> Furthermore, since we are immersing ourselves herein into the cauldron of the love-it or hate-it knowledge conundrum, knowledge can be known and yet undervalued, perhaps only becoming valuable at a later time and in a different light. There is a case to be made that humankind has a seemingly irresistible allure toward more and more knowledge. Some philosophers suggest you are unlikely to be able to bottle up or stop this quest for knowledge. If that’s the manner of how humanity will be, this implies that you must find ways to control or contain knowledge and give up on the belief that we can altogether avoid landing into forbidden knowledge. There is a relatively new venue prompting a lot of anxious hand wringing pertaining to forbidden knowledge, namely the advent of Artificial Intelligence (AI). Here’s the rub. <mark>Suppose that we are able to craft AI systems that make use of knowledge about how humans can think.</mark> There are two major potential gotchas. <mark>First, the AI systems themselves might end up doing good things, and they also might end up doing bad things.</mark> <mark>If the bad outweighs the good, maybe we are shooting our own foot by allowing AI to be put into use.</mark> <mark>Secondly, perhaps this could be averted entirely by deciding that there is forbidden knowledge about how humans think, and we ought to not discover or reveal those mental mechanisms.</mark> It is the classic stepwise logic that step A axiomatically leads to step B. <mark>We won’t need to worry about AI systems (step B), if we never allow the achievement of step A (figuring out how humans think and then imparting that into computers), since the attainment of AI would presumably not arise.</mark> In any case, there is inarguably a growing concern about AI. Plenty of efforts are underway to promulgate a semblance of AI Ethics, meaning that those developers and indeed all stakeholders that are conceiving of, building, and putting into use an AI system needs to consider the ethical aspects of their efforts. AI systems have been unveiled and placed into use replete with all sorts of notable concerns, including incorporating unsavory biases and other problems. All told, one bold and somewhat stark argument is that the pursuit of AI is being underpinned or stoked by the discovery and then exploitation of forbidden knowledge. Be aware that many would scoff at this allegation. There are those deeply immersed in the field of AI who would laugh that there is anything in the entirety of AI to date that constitutes potential forbidden knowledge. The technology and technological elements are relatively ho-hum, they would argue. You would be hard-pressed to pinpoint what AI-related knowledge that is already known comes anywhere near the ballpark of forbidden knowledge. <mark>For those that concur with that posture, there is the reply that it might be future knowledge that we have not yet attained that is the upcoming forbidden kind, and for which we are heading pell-mell down that path.</mark> Thus, they would concede that we haven’t arrived at forbidden knowledge at this juncture, but this is an insidious distractor due to the aspect that it masks or belies our qualms entailing the possibility that it lays in wait at the next turn. One area where AI is being actively used is to create Autonomous Vehicles (AVs). We are gradually seeing the emergence of self-driving cars and can expect self-driving trucks, self-driving motorcycles, self-driving drones, self-driving planes, self-driving ships, self-driving submersibles, etc. <mark>Today’s conventional cars are eventually going to give way to the advent of AI-based, true self-driving cars.</mark> <mark>Self-driving cars are driven via an AI driving system.</mark> There isn’t a need for a human driver at the wheel, and nor is there a provision for a human to drive the vehicle. <mark>Here’s an intriguing question that has arisen: Might the crafting of AI-based true self-driving cars take us into the realm of discovering forbidden knowledge, and if so, what should be done about this?</mark> Before jumping into the details, I’d like to clarify what is meant when referring to true self-driving cars. <mark>For my framework about AI autonomous cars, see the link here: https://aitrends.com/ai-insider/framework-ai-self-driving-driverless-cars-big-picture/   \n",
       "Why this is a moonshot effort, see my explanation here: https://aitrends.com/ai-insider/self-driving-car-mother-ai-projects-moonshot/   \n",
       "For more about the levels as a type of Richter scale, see my discussion here: https://aitrends.com/ai-insider/richter-scale-levels-self-driving-cars/   \n",
       "For the argument about bifurcating the levels, see my explanation here: https://aitrends.com/ai-insider/reframing-ai-levels-for-self-driving-cars-bifurcation-of-autonomy/   \n",
       "Understanding The Levels Of Self-Driving Cars   \n",
       "As a clarification, true self-driving cars are ones where the AI drives the car entirely on its own and there isn’t any human assistance during the driving task.</mark> <mark>These driverless vehicles are considered Level 4 and Level 5, while a car that requires a human driver to co-share the driving effort is usually considered at Level 2 or Level 3.</mark> The cars that co-share the driving task are described as being semi-autonomous, and typically contain a variety of automated add-on’s that are referred to as ADAS (Advanced Driver-Assistance Systems). <mark>There is not yet a true self-driving car at Level 5, which we don’t yet even know if this will be possible to achieve, and nor how long it will take to get there.</mark> <mark>Meanwhile, the Level 4 efforts are gradually trying to get some traction by undergoing very narrow and selective public roadway trials, though there is controversy over whether this testing should be allowed per se (we are all life-or-death guinea pigs in an experiment taking place on our highways and byways, some contend).</mark> <mark>Since semi-autonomous cars require a human driver, the adoption of those types of cars won’t be markedly different from driving conventional vehicles, so there’s not much new per se to cover about them on this topic (though, as you’ll see in a moment, the points next made are generally applicable).</mark> <mark>For semi-autonomous cars, it is important that the public needs to be forewarned about a disturbing aspect that’s been arising lately, namely that despite those human drivers that keep posting videos of themselves falling asleep at the wheel of a Level 2 or Level 3 car, we all need to avoid being misled into believing that the driver can take away their attention from the driving task while driving a semi-autonomous car.</mark> You are the responsible party for the driving actions of the vehicle, regardless of how much automation might be tossed into a Level 2 or Level 3. <mark>For why remote piloting or operating of self-driving cars is generally eschewed, see my explanation here: https://aitrends.com/ai-insider/remote-piloting-is-a-self-driving-car-crutch/   \n",
       "To be wary of fake news about self-driving cars, see my tips here: https://aitrends.com/ai-insider/ai-fake-news-about-self-driving-cars/ \n",
       "The ethical implications of AI driving systems are significant, see my indication here: http://aitrends.com/selfdrivingcars/ethically-ambiguous-self-driving-cars/   \n",
       "Be aware of the pitfalls of normalization of deviance when it comes to self-driving cars, here’s my call to arms: https://aitrends.com/ai-insider/normalization-of-deviance-endangers-ai-self-driving-cars/   \n",
       "Self-Driving Cars And Forbidden Knowledge   \n",
       "For Level 4 and Level 5 true self-driving vehicles, there won’t be a human driver involved in the driving task.</mark> All occupants will be passengers. <mark>The AI is doing the driving.</mark> One aspect to immediately discuss entails the fact that the AI involved in today’s AI driving systems is not sentient. In other words, the AI is altogether a collective of computer-based programming and algorithms, and most assuredly not able to reason in the same manner that humans can. Why this added emphasis about the AI not being sentient? Because I want to underscore that when discussing the role of the AI driving system, I am not ascribing human qualities to the AI. Please be aware that there is an ongoing and dangerous tendency these days to anthropomorphize AI. In essence, people are assigning human-like sentience to today’s AI, despite the undeniable and inarguable fact that no such AI exists as yet. With that clarification, you can envision that the AI driving system won’t natively somehow “know” about the facets of driving. Driving and all that it entails will need to be programmed as part of the hardware and software of the self-driving car. <mark>Let’s dive into the myriad of aspects that come to play on this topic.</mark> The crux here is whether there is forbidden knowledge lurking within the existing and ongoing efforts to achieve AI-based true self-driving cars. <mark>We’ll begin by considering the status of the existent efforts and then shift into speculation about the future of such efforts.</mark> <mark>Per the earlier discussion about whether there is forbidden knowledge that has already perchance been revealed or discovered via the efforts toward today’s AI systems all told, the odds seem stacked against such a notion at this time, and likewise the same could be said about the pursuit of self-driving cars.</mark> <mark>Essentially, there doesn’t seem to be any forbidden knowledge per se that has been discovered or revealed during the self-driving cars development journey so far, at least with respect to the conventional wisdom about what forbidden knowledge might entail.</mark> One could try to argue that it is premature to reach such a conclusion and that we might, later on, realize that forbidden knowledge was indeed uncovered or invented, and we just didn’t realize it. That is a rabbit hole that we’ll not go down for now, though you are welcome to keep that presumption at hand if so desired. <mark>That covers the present, and ergo we can turn our attention to the future.</mark> <mark>Generally, the efforts underway today have been primarily aimed at achieving Level 4, and the hope is that someday we will go beyond Level 4 and attain Level 5.</mark> To get to a robust Level 4, most would likely say that we can continue the existing approaches. Not everyone would agree with that assumption. <mark>Some believe that we will get stymied within Level 4.</mark> Furthermore, the inability to produce a robust Level 4 will ostensibly preclude us from being able to attain Level 5. <mark>There is a contingent that suggests we need to start over and set aside the existing AI approaches, which otherwise are taking us down a dead-end or blind alley.</mark> An entirely new way of devising AI for autonomous vehicles is needed, they would vehemently argue. <mark>There is also a contingent that asserts the Level 4 itself is a type of dead-end.</mark> <mark>In brief, those proponents would say that we will achieve a robust Level 4, though this will do little good towards attaining Level 5.</mark> Once again, their view is similar to the preceding remark that we will need to come up with some radically new understandings about AI and the nature of cognitive acumen in order to get self-driving cars into the Level 5 realm. <mark>Aha, it is within that scope of having to dramatically revisit and revamp what AI is and how we can advance significantly in the pursuit of AI that the forbidden knowledge question can reside.</mark> In theory, perhaps the only means of attaining Level 5 will be to strike upon some knowledge that we do not yet know and that for which bodes for falling within the realm of forbidden knowledge. To some, this seems farfetched. They would emphatically ask; just what kind of knowledge are you even talking about? <mark>Here’s their logic.</mark> <mark>Humans are able to drive cars.</mark> <mark>Humans do not seem to need or possess forbidden knowledge as it relates to the act of driving a car.</mark> Therefore, it seems ridiculous on the face of things to claim or contend that the only means to get AI-based true self-driving cars, for which they would be driven on an equal basis as human drivers can drive, would require the discovery or invention of whatever might be construed as forbidden knowledge. Seems like pretty ironclad logic. <mark>The retort is that humans have common-sense reasoning.</mark> <mark>With common-sense reasoning, we seem to know all sorts of things about the world around us.</mark> When we drive a car, we intrinsically make use of our common-sense reasoning. <mark>We take for granted that we do have a common-sense reasoning capacity, and similarly, we take for granted that it integrally comes to the fore when driving a car.</mark> <mark>Attempts to create AI that can exhibit the equivalent of human common-sense reasoning have made ostensibly modest or some would say minimal progress (to clarify, those pursuing this line of inquiry are to be lauded, it’s just that no earth-shattering breakthroughs seem to have been reached and none seem on the immediate horizon).</mark> Yes, there are some quite fascinating and exciting efforts underway, but when you measure those against the everyday common-sense reasoning of humans, there is no comparison. They are night and day. <mark>If this were a contest, the humans win hands down, no doubt about it, and the AI experimental efforts encompassing common-sense reasoning are mere playthings in contrast.</mark> <mark>You might have gleaned where this line of thought is headed.</mark> <mark>The belief by some is that until we crack open the enigma of common-sense reasoning, there is little chance of achieving a Level 5, and perhaps also this will hold back the Level 4 too.</mark> It could be that a secret ingredient of sorts for autonomous vehicles is the need to figure out and include common-sense reasoning into AI-based driving and piloting systems. <mark>If you buy into that logic, the added assertion is that maybe within the confines of how common-sense reasoning takes place is a semblance of forbidden knowledge.</mark> <mark>On the surface, you would certainly assume that if we knew entirely how common-sense reasoning works, there would not appear to be any cause for alarm or concern.</mark> <mark>The act of employing common-sense reasoning does not seem to necessarily embody forbidden knowledge.</mark> <mark>The twist is that perhaps the underlying cognitive means that gives rise to the advent of common-sense reasoning is where there is forbidden knowledge.</mark> Some deep-rooted elements in the nature of human thought and how we form common sense and undertake common-sense reasoning are possibly a type of knowledge that will be shown as crucial and a forbidden knowledge formulation. For more details about ODDs, see my indication at this link here: https://www.aitrends.com/ai-insider/amalgamating-of-operational-design-domains-odds-for-ai-self-driving-cars/ \n",
       "On the topic of off-road self-driving cars, here’s my details elicitation: https://www.aitrends.com/ai-insider/off-roading-as-a-challenging-use-case-for-ai-autonomous-cars/ \n",
       "I’ve urged that there must be a Chief Safety Officer at self-driving car makers, here’s the scoop: https://www.aitrends.com/ai-insider/chief-safety-officers-needed-in-ai-the-case-of-ai-self-driving-cars/ \n",
       "Expect that lawsuits are going to gradually become a significant part of the self-driving car industry, see my explanatory details here: http://aitrends.com/selfdrivingcars/self-driving-car-lawsuits-bonanza-ahead/   \n",
       "Conclusion   \n",
       "Wow, that’s quite a bit of pondering, contemplation, and (some would say) wild thinking. <mark>Maybe so, but it is a consideration that some would wish that we gave at least some credence toward and devoted attention to.</mark> <mark>There is the angst that we might find ourselves by happenstance stumbling into forbidden knowledge on these voracious self-driving cars quests.</mark> <mark>For however you might emphasize that having AI-based true self-driving cars will be a potential blessing, proffering mobility-for-all and leading to reducing the number of car crash-related fatalities, there is a sneaking suspicion that it will not be all-good.</mark> <mark>The catch or trap could be that there is some kind of forbidden knowledge that will get brought to the eye and we will inevitably kick ourselves that we didn’t see it coming.</mark> The next time you are munching on a delicious apple, give some thought to whether self-driving cars might be forbidden fruit. We are on the path to taking a big bite, and we’ll have to see where that takes us. Copyright 2021 Dr. Lance Eliot  \n",
       "http://ai-selfdriving-cars.libsyn.com/website </p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "visualize(article[\"title\"], best_sentences, original_setences)\n",
    "save_summary(article[\"title\"], original_setences, best_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1f2ea82",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
